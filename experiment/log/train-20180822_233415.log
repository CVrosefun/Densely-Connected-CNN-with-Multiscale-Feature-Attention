I0822 23:34:16.013797  2111 caffe.cpp:185] Using GPUs 0, 1, 2, 3
I0822 23:34:16.196475  2111 caffe.cpp:190] GPU 0: TITAN X (Pascal)
I0822 23:34:16.197434  2111 caffe.cpp:190] GPU 1: TITAN X (Pascal)
I0822 23:34:16.198335  2111 caffe.cpp:190] GPU 2: TITAN X (Pascal)
I0822 23:34:16.199131  2111 caffe.cpp:190] GPU 3: TITAN X (Pascal)
I0822 23:34:16.667970  2111 solver.cpp:48] Initializing solver from parameters: 
test_iter: 95
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
snapshot: 1000
snapshot_prefix: "model"
solver_mode: GPU
device_id: 0
net: "train_test_net.prototxt"
regularization_type: "L1"
test_initialization: false
stepvalue: 25000
stepvalue: 28000
I0822 23:34:16.668090  2111 solver.cpp:91] Creating training net from net file: train_test_net.prototxt
I0822 23:34:16.669915  2111 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0822 23:34:16.670663  2111 net.cpp:49] Initializing net from parameters: 
name: "DenseAttentionNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "TextData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  text_data_param {
    label_source: "/ssd/ijcai18/data/agnews/train_label.txt"
    data_source: "/ssd/ijcai18/data/agnews/train_data.txt"
    dict_source: "/ssd/ijcai18/data/agnews/local_dic_index.txt"
    batch_size: 64
    channel: 1
    num_words: 200
    crop_height: 100
    crop_width: 300
  }
}
layer {
  name: "trans_layer_conv0"
  type: "Convolution"
  bottom: "data"
  top: "trans_layer_conv0"
  convolution_param {
    num_output: 64
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 300
  }
}
layer {
  name: "trans_layer_bn0"
  type: "BatchNorm"
  bottom: "trans_layer_conv0"
  top: "trans_layer_bn0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "trans_layer_relu0"
  type: "ReLU"
  bottom: "trans_layer_bn0"
  top: "trans_layer_bn0"
}
layer {
  name: "trans_layer_conv1"
  type: "Convolution"
  bottom: "trans_layer_bn0"
  top: "trans_layer_conv1"
  convolution_param {
    num_output: 64
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
  }
}
layer {
  name: "trans_layer_bn1"
  type: "BatchNorm"
  bottom: "trans_layer_conv1"
  top: "trans_layer_bn1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "trans_layer_relu1"
  type: "ReLU"
  bottom: "trans_layer_bn1"
  top: "trans_layer_bn1"
}
layer {
  name: "trans_layer_conv2"
  type: "Convolution"
  bottom: "trans_layer_bn1"
  top: "trans_layer_conv2"
  convolution_param {
    num_output: 64
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
  }
}
layer {
  name: "trans_layer_bn2"
  type: "BatchNorm"
  bottom: "trans_layer_conv2"
  top: "trans_layer_bn2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "trans_layer_relu2"
  type: "ReLU"
  bottom: "trans_layer_bn2"
  top: "trans_layer_bn2"
}
layer {
  name: "dense_layer_conv0"
  type: "Convolution"
  bottom: "trans_layer_bn2"
  top: "dense_layer_conv0"
  convolution_param {
    num_output: 64
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "dense_layer_concat0"
  type: "Concat"
  bottom: "dense_layer_conv0"
  bottom: "trans_layer_conv0"
  bottom: "trans_layer_conv1"
  bottom: "trans_layer_conv2"
  top: "dense_layer_concat0"
  concat_param {
    axis: 1
  }
}
layer {
  name: "dense_layer_bn0"
  type: "BatchNorm"
  bottom: "dense_layer_concat0"
  top: "dense_layer_bn0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "dense_layer_relu0"
  type: "ReLU"
  bottom: "dense_layer_bn0"
  top: "dense_layer_bn0"
}
layer {
  name: "dense_layer_pool0"
  type: "Pooling"
  bottom: "dense_layer_bn0"
  top: "dense_layer_pool0"
  pooling_param {
    pool: AVE
    kernel_h: 3
    kernel_w: 1
    stride_h: 2
    stride_w: 1
    pad_h: 1
    pad_w: 0
  }
}
layer {
  name: "dense_layer_conv1"
  type: "Convolution"
  bottom: "dense_layer_pool0"
  top: "dense_layer_conv1"
  convolution_param {
    num_output: 64
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "dense_layer_concat1"
  type: "Concat"
  bottom: "dense_layer_conv1"
  bottom: "dense_layer_pool0"
  top: "dense_layer_concat1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "dense_layer_bn1"
  type: "BatchNorm"
  bottom: "dense_layer_concat1"
  top: "dense_layer_bn1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "dense_layer_relu1"
  type: "ReLU"
  bottom: "dense_layer_bn1"
  top: "dense_layer_bn1"
}
layer {
  name: "dense_layer_pool1"
  type: "Pooling"
  bottom: "dense_layer_bn1"
  top: "dense_layer_pool1"
  pooling_param {
    pool: AVE
    kernel_h: 3
    kernel_w: 1
    stride_h: 2
    stride_w: 1
    pad_h: 1
    pad_w: 0
  }
}
layer {
  name: "dense_layer_conv2"
  type: "Convolution"
  bottom: "dense_layer_pool1"
  top: "dense_layer_conv2"
  convolution_param {
    num_output: 64
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "dense_layer_concat2"
  type: "Concat"
  bottom: "dense_layer_conv2"
  bottom: "dense_layer_pool1"
  top: "dense_layer_concat2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "dense_layer_bn2"
  type: "BatchNorm"
  bottom: "dense_layer_concat2"
  top: "dense_layer_bn2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "dense_layer_relu2"
  type: "ReLU"
  bottom: "dense_layer_bn2"
  top: "dense_layer_bn2"
}
layer {
  name: "dense_layer_pool2"
  type: "Pooling"
  bottom: "dense_layer_bn2"
  top: "dense_layer_pool2"
  pooling_param {
    pool: AVE
    kernel_h: 3
    kernel_w: 1
    stride_h: 2
    stride_w: 1
    pad_h: 1
    pad_w: 0
  }
}
layer {
  name: "dense_layer_conv3"
  type: "Convolution"
  bottom: "dense_layer_pool2"
  top: "dense_layer_conv3"
  convolution_param {
    num_output: 64
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "dense_layer_concat3"
  type: "Concat"
  bottom: "dense_layer_conv3"
  bottom: "dense_layer_pool2"
  top: "dense_layer_concat3"
  concat_param {
    axis: 1
  }
}
layer {
  name: "dense_layer_bn3"
  type: "BatchNorm"
  bottom: "dense_layer_concat3"
  top: "dense_layer_bn3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "dense_layer_relu3"
  type: "ReLU"
  bottom: "dense_layer_bn3"
  top: "dense_layer_bn3"
}
layer {
  name: "dense_layer_pool3"
  type: "Pooling"
  bottom: "dense_layer_bn3"
  top: "dense_layer_pool3"
  pooling_param {
    pool: AVE
    kernel_h: 3
    kernel_w: 1
    stride_h: 2
    stride_w: 1
    pad_h: 1
    pad_w: 0
  }
}
layer {
  name: "dense_layer_conv4"
  type: "Convolution"
  bottom: "dense_layer_pool3"
  top: "dense_layer_conv4"
  convolution_param {
    num_output: 64
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "dense_layer_concat4"
  type: "Concat"
  bottom: "dense_layer_conv4"
  bottom: "dense_layer_pool3"
  top: "dense_layer_concat4"
  concat_param {
    axis: 1
  }
}
layer {
  name: "dense_layer_bn4"
  type: "BatchNorm"
  bottom: "dense_layer_concat4"
  top: "dense_layer_bn4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "dense_layer_relu4"
  type: "ReLU"
  bottom: "dense_layer_bn4"
  top: "dense_layer_bn4"
}
layer {
  name: "attention_layer_conv0"
  type: "Convolution"
  bottom: "dense_layer_bn4"
  top: "attention_layer_conv0"
  convolution_param {
    num_output: 512
    group: 8
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
  }
}
layer {
  name: "attention_layer_bn0"
  type: "BatchNorm"
  bottom: "attention_layer_conv0"
  top: "attention_layer_bn0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "attention_layer_relu0"
  type: "ReLU"
  bottom: "attention_layer_bn0"
  top: "attention_layer_bn0"
}
layer {
  name: "attention_layer_conv1"
  type: "Convolution"
  bottom: "attention_layer_bn0"
  top: "attention_layer_conv1"
  convolution_param {
    num_output: 512
    group: 8
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
  }
}
layer {
  name: "attention_layer_bn1"
  type: "BatchNorm"
  bottom: "attention_layer_conv1"
  top: "attention_layer_bn1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "attention_layer_relu1"
  type: "ReLU"
  bottom: "attention_layer_bn1"
  top: "attention_layer_bn1"
}
layer {
  name: "attention_scale_slice"
  type: "Slice"
  bottom: "attention_layer_bn1"
  top: "attention_scale0"
  top: "attention_scale1"
  top: "attention_scale2"
  top: "attention_scale3"
  top: "attention_scale4"
  top: "attention_scale5"
  top: "attention_scale6"
  top: "attention_scale7"
  slice_param {
    slice_point: 64
    slice_point: 128
    slice_point: 192
    slice_point: 256
    slice_point: 320
    slice_point: 384
    slice_point: 448
    axis: 1
  }
}
layer {
  name: "attention_permute0"
  type: "Permute"
  bottom: "attention_scale0"
  top: "attention_permute0"
  permute_param {
    order: 0
    order: 3
    order: 2
    order: 1
  }
}
layer {
  name: "attention_reduction0"
  type: "Reduction"
  bottom: "attention_permute0"
  top: "attention_reduction0"
  reduction_param {
    axis: 3
  }
}
layer {
  name: "attention_permute1"
  type: "Permute"
  bottom: "attention_scale1"
  top: "attention_permute1"
  permute_param {
    order: 0
    order: 3
    order: 2
    order: 1
  }
}
layer {
  name: "attention_reduction1"
  type: "Reduction"
  bottom: "attention_permute1"
  top: "attention_reduction1"
  reduction_param {
    axis: 3
  }
}
layer {
  name: "attention_permute2"
  type: "Permute"
  bottom: "attention_scale2"
  top: "attention_permute2"
  permute_param {
    order: 0
    order: 3
    order: 2
    order: 1
  }
}
layer {
  name: "attention_reduction2"
  type: "Reduction"
  bottom: "attention_permute2"
  top: "attention_reduction2"
  reduction_param {
    axis: 3
  }
}
layer {
  name: "attention_permute3"
  type: "Permute"
  bottom: "attention_scale3"
  top: "attention_permute3"
  permute_param {
    order: 0
    order: 3
    order: 2
    order: 1
  }
}
layer {
  name: "attention_reduction3"
  type: "Reduction"
  bottom: "attention_permute3"
  top: "attention_reduction3"
  reduction_param {
    axis: 3
  }
}
layer {
  name: "attention_permute4"
  type: "Permute"
  bottom: "attention_scale4"
  top: "attention_permute4"
  permute_param {
    order: 0
    order: 3
    order: 2
    order: 1
  }
}
layer {
  name: "attention_reduction4"
  type: "Reduction"
  bottom: "attention_permute4"
  top: "attention_reduction4"
  reduction_param {
    axis: 3
  }
}
layer {
  name: "attention_permute5"
  type: "Permute"
  bottom: "attention_scale5"
  top: "attention_permute5"
  permute_param {
    order: 0
    order: 3
    order: 2
    order: 1
  }
}
layer {
  name: "attention_reduction5"
  type: "Reduction"
  bottom: "attention_permute5"
  top: "attention_reduction5"
  reduction_param {
    axis: 3
  }
}
layer {
  name: "attention_permute6"
  type: "Permute"
  bottom: "attention_scale6"
  top: "attention_permute6"
  permute_param {
    order: 0
    order: 3
    order: 2
    order: 1
  }
}
layer {
  name: "attention_reduction6"
  type: "Reduction"
  bottom: "attention_permute6"
  top: "attention_reduction6"
  reduction_param {
    axis: 3
  }
}
layer {
  name: "attention_permute7"
  type: "Permute"
  bottom: "attention_scale7"
  top: "attention_permute7"
  permute_param {
    order: 0
    order: 3
    order: 2
    order: 1
  }
}
layer {
  name: "attention_reduction7"
  type: "Reduction"
  bottom: "attention_permute7"
  top: "attention_reduction7"
  reduction_param {
    axis: 3
  }
}
layer {
  name: "attention_scale_concat"
  type: "Concat"
  bottom: "attention_reduction0"
  bottom: "attention_reduction1"
  bottom: "attention_reduction2"
  bottom: "attention_reduction3"
  bottom: "attention_reduction4"
  bottom: "attention_reduction5"
  bottom: "attention_reduction6"
  bottom: "attention_reduction7"
  top: "attention_scale_concat"
  concat_param {
    axis: 1
  }
}
layer {
  name: "attention_height_slice"
  type: "Slice"
  bottom: "attention_scale_concat"
  top: "attention_height0"
  top: "attention_height1"
  top: "attention_height2"
  top: "attention_height3"
  top: "attention_height4"
  top: "attention_height5"
  top: "attention_height6"
  top: "attention_height7"
  slice_param {
    slice_point: 1
    slice_point: 2
    slice_point: 3
    slice_point: 4
    slice_point: 5
    slice_point: 6
    slice_point: 7
    axis: 2
  }
}
layer {
  name: "attention_height_fc1_0"
  type: "InnerProduct"
  bottom: "attention_height0"
  top: "attention_height_fc1_0"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc2_0"
  type: "InnerProduct"
  bottom: "attention_height_fc1_0"
  top: "attention_height_fc2_0"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc3_0"
  type: "InnerProduct"
  bottom: "attention_height_fc2_0"
  top: "attention_height_fc3_0"
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_reshape0"
  type: "Reshape"
  bottom: "attention_height_fc3_0"
  top: "attention_height_reshape0"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "attention_height_fc1_1"
  type: "InnerProduct"
  bottom: "attention_height1"
  top: "attention_height_fc1_1"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc2_1"
  type: "InnerProduct"
  bottom: "attention_height_fc1_1"
  top: "attention_height_fc2_1"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc3_1"
  type: "InnerProduct"
  bottom: "attention_height_fc2_1"
  top: "attention_height_fc3_1"
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_reshape1"
  type: "Reshape"
  bottom: "attention_height_fc3_1"
  top: "attention_height_reshape1"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "attention_height_fc1_2"
  type: "InnerProduct"
  bottom: "attention_height2"
  top: "attention_height_fc1_2"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc2_2"
  type: "InnerProduct"
  bottom: "attention_height_fc1_2"
  top: "attention_height_fc2_2"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc3_2"
  type: "InnerProduct"
  bottom: "attention_height_fc2_2"
  top: "attention_height_fc3_2"
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_reshape2"
  type: "Reshape"
  bottom: "attention_height_fc3_2"
  top: "attention_height_reshape2"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "attention_height_fc1_3"
  type: "InnerProduct"
  bottom: "attention_height3"
  top: "attention_height_fc1_3"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc2_3"
  type: "InnerProduct"
  bottom: "attention_height_fc1_3"
  top: "attention_height_fc2_3"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc3_3"
  type: "InnerProduct"
  bottom: "attention_height_fc2_3"
  top: "attention_height_fc3_3"
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_reshape3"
  type: "Reshape"
  bottom: "attention_height_fc3_3"
  top: "attention_height_reshape3"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "attention_height_fc1_4"
  type: "InnerProduct"
  bottom: "attention_height4"
  top: "attention_height_fc1_4"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc2_4"
  type: "InnerProduct"
  bottom: "attention_height_fc1_4"
  top: "attention_height_fc2_4"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc3_4"
  type: "InnerProduct"
  bottom: "attention_height_fc2_4"
  top: "attention_height_fc3_4"
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_reshape4"
  type: "Reshape"
  bottom: "attention_height_fc3_4"
  top: "attention_height_reshape4"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "attention_height_fc1_5"
  type: "InnerProduct"
  bottom: "attention_height5"
  top: "attention_height_fc1_5"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc2_5"
  type: "InnerProduct"
  bottom: "attention_height_fc1_5"
  top: "attention_height_fc2_5"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc3_5"
  type: "InnerProduct"
  bottom: "attention_height_fc2_5"
  top: "attention_height_fc3_5"
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_reshape5"
  type: "Reshape"
  bottom: "attention_height_fc3_5"
  top: "attention_height_reshape5"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "attention_height_fc1_6"
  type: "InnerProduct"
  bottom: "attention_height6"
  top: "attention_height_fc1_6"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc2_6"
  type: "InnerProduct"
  bottom: "attention_height_fc1_6"
  top: "attention_height_fc2_6"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc3_6"
  type: "InnerProduct"
  bottom: "attention_height_fc2_6"
  top: "attention_height_fc3_6"
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_reshape6"
  type: "Reshape"
  bottom: "attention_height_fc3_6"
  top: "attention_height_reshape6"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "attention_height_fc1_7"
  type: "InnerProduct"
  bottom: "attention_height7"
  top: "attention_height_fc1_7"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc2_7"
  type: "InnerProduct"
  bottom: "attention_height_fc1_7"
  top: "attention_height_fc2_7"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc3_7"
  type: "InnerProduct"
  bottom: "attention_height_fc2_7"
  top: "attention_height_fc3_7"
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_reshape7"
  type: "Reshape"
  bottom: "attention_height_fc3_7"
  top: "attention_height_reshape7"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "attention_height_concat"
  type: "Concat"
  bottom: "attention_height_reshape0"
  bottom: "attention_height_reshape1"
  bottom: "attention_height_reshape2"
  bottom: "attention_height_reshape3"
  bottom: "attention_height_reshape4"
  bottom: "attention_height_reshape5"
  bottom: "attention_height_reshape6"
  bottom: "attention_height_reshape7"
  top: "attention_height_concat"
  concat_param {
    axis: 2
  }
}
layer {
  name: "attention_weight"
  type: "Softmax"
  bottom: "attention_height_concat"
  top: "attention_weight"
}
layer {
  name: "attention_weight_slice"
  type: "Slice"
  bottom: "attention_weight"
  top: "attention_weight_slice0"
  top: "attention_weight_slice1"
  top: "attention_weight_slice2"
  top: "attention_weight_slice3"
  top: "attention_weight_slice4"
  top: "attention_weight_slice5"
  top: "attention_weight_slice6"
  top: "attention_weight_slice7"
  slice_param {
    slice_point: 1
    slice_point: 2
    slice_point: 3
    slice_point: 4
    slice_point: 5
    slice_point: 6
    slice_point: 7
    axis: 1
  }
}
layer {
  name: "attention_weight_tile0"
  type: "Tile"
  bottom: "attention_weight_slice0"
  top: "attention_weight_tile0"
  tile_param {
    axis: 1
    tiles: 64
  }
}
layer {
  name: "attention_reweight0"
  type: "Eltwise"
  bottom: "attention_scale0"
  bottom: "attention_weight_tile0"
  top: "attention_reweight0"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "attention_weight_tile1"
  type: "Tile"
  bottom: "attention_weight_slice1"
  top: "attention_weight_tile1"
  tile_param {
    axis: 1
    tiles: 64
  }
}
layer {
  name: "attention_reweight1"
  type: "Eltwise"
  bottom: "attention_scale1"
  bottom: "attention_weight_tile1"
  top: "attention_reweight1"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "attention_weight_tile2"
  type: "Tile"
  bottom: "attention_weight_slice2"
  top: "attention_weight_tile2"
  tile_param {
    axis: 1
    tiles: 64
  }
}
layer {
  name: "attention_reweight2"
  type: "Eltwise"
  bottom: "attention_scale2"
  bottom: "attention_weight_tile2"
  top: "attention_reweight2"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "attention_weight_tile3"
  type: "Tile"
  bottom: "attention_weight_slice3"
  top: "attention_weight_tile3"
  tile_param {
    axis: 1
    tiles: 64
  }
}
layer {
  name: "attention_reweight3"
  type: "Eltwise"
  bottom: "attention_scale3"
  bottom: "attention_weight_tile3"
  top: "attention_reweight3"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "attention_weight_tile4"
  type: "Tile"
  bottom: "attention_weight_slice4"
  top: "attention_weight_tile4"
  tile_param {
    axis: 1
    tiles: 64
  }
}
layer {
  name: "attention_reweight4"
  type: "Eltwise"
  bottom: "attention_scale4"
  bottom: "attention_weight_tile4"
  top: "attention_reweight4"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "attention_weight_tile5"
  type: "Tile"
  bottom: "attention_weight_slice5"
  top: "attention_weight_tile5"
  tile_param {
    axis: 1
    tiles: 64
  }
}
layer {
  name: "attention_reweight5"
  type: "Eltwise"
  bottom: "attention_scale5"
  bottom: "attention_weight_tile5"
  top: "attention_reweight5"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "attention_weight_tile6"
  type: "Tile"
  bottom: "attention_weight_slice6"
  top: "attention_weight_tile6"
  tile_param {
    axis: 1
    tiles: 64
  }
}
layer {
  name: "attention_reweight6"
  type: "Eltwise"
  bottom: "attention_scale6"
  bottom: "attention_weight_tile6"
  top: "attention_reweight6"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "attention_weight_tile7"
  type: "Tile"
  bottom: "attention_weight_slice7"
  top: "attention_weight_tile7"
  tile_param {
    axis: 1
    tiles: 64
  }
}
layer {
  name: "attention_reweight7"
  type: "Eltwise"
  bottom: "attention_scale7"
  bottom: "attention_weight_tile7"
  top: "attention_reweight7"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "attention_reweight_sum"
  type: "Eltwise"
  bottom: "attention_reweight0"
  bottom: "attention_reweight1"
  bottom: "attention_reweight2"
  bottom: "attention_reweight3"
  bottom: "attention_reweight4"
  bottom: "attention_reweight5"
  bottom: "attention_reweight6"
  bottom: "attention_reweight7"
  top: "attention_reweight_sum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "attention_reweight_flatten"
  type: "Flatten"
  bottom: "attention_reweight_sum"
  top: "attention_reweight_flatten"
}
layer {
  name: "attention_reweight_dropout"
  type: "Dropout"
  bottom: "attention_reweight_flatten"
  top: "attention_reweight_dropout"
  dropout_param {
    dropout_ratio: 0.7
  }
}
layer {
  name: "classification_fc1"
  type: "InnerProduct"
  bottom: "attention_reweight_dropout"
  top: "classification_fc1"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "classification_dropout"
  type: "Dropout"
  bottom: "classification_fc1"
  top: "classification_dropout"
  dropout_param {
    dropout_ratio: 0.7
  }
}
layer {
  name: "classification_fc2"
  type: "InnerProduct"
  bottom: "classification_dropout"
  top: "classification_fc2"
  param {
    lr_mult: 10
    decay_mult: 2
  }
  param {
    lr_mult: 10
    decay_mult: 2
  }
  inner_product_param {
    num_output: 4
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "classification_loss"
  type: "SoftmaxWithLoss"
  bottom: "classification_fc2"
  bottom: "label"
  top: "classification_loss"
  loss_weight: 1
}
layer {
  name: "classification_accuracy"
  type: "Accuracy"
  bottom: "classification_fc2"
  bottom: "label"
  top: "classification_accuracy"
}
I0822 23:34:16.671106  2111 layer_factory.hpp:77] Creating layer data
I0822 23:34:16.671139  2111 net.cpp:106] Creating Layer data
I0822 23:34:16.671144  2111 net.cpp:411] data -> data
I0822 23:34:16.671162  2111 net.cpp:411] data -> label
I0822 23:34:16.677191  2111 text_data_layer.cpp:83] read label done (120000 samples)
I0822 23:34:16.677199  2111 text_data_layer.cpp:86] Reading content
I0822 23:34:16.702595  2111 text_data_layer.cpp:102] read content done: #120000 samples.
I0822 23:34:16.702936  2111 text_data_layer.cpp:108] Shuffling data
I0822 23:34:16.734869  2111 text_data_layer.cpp:117] output data size: 64,1,100,300
I0822 23:34:16.734936  2111 text_data_layer.cpp:125] Reading vector dict...
I0822 23:34:21.295002  2111 text_data_layer.cpp:132] vec_dict size: 56249
I0822 23:34:21.304023  2111 net.cpp:150] Setting up data
I0822 23:34:21.304064  2111 net.cpp:157] Top shape: 64 1 100 300 (1920000)
I0822 23:34:21.304069  2111 net.cpp:157] Top shape: 64 (64)
I0822 23:34:21.304071  2111 net.cpp:165] Memory required for data: 7680256
I0822 23:34:21.304080  2111 layer_factory.hpp:77] Creating layer label_data_1_split
I0822 23:34:21.304091  2111 net.cpp:106] Creating Layer label_data_1_split
I0822 23:34:21.304095  2111 net.cpp:454] label_data_1_split <- label
I0822 23:34:21.304105  2111 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0822 23:34:21.304112  2111 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0822 23:34:21.304191  2111 net.cpp:150] Setting up label_data_1_split
I0822 23:34:21.304198  2111 net.cpp:157] Top shape: 64 (64)
I0822 23:34:21.304201  2111 net.cpp:157] Top shape: 64 (64)
I0822 23:34:21.304204  2111 net.cpp:165] Memory required for data: 7680768
I0822 23:34:21.304206  2111 layer_factory.hpp:77] Creating layer trans_layer_conv0
I0822 23:34:21.304224  2111 net.cpp:106] Creating Layer trans_layer_conv0
I0822 23:34:21.304226  2111 net.cpp:454] trans_layer_conv0 <- data
I0822 23:34:21.304230  2111 net.cpp:411] trans_layer_conv0 -> trans_layer_conv0
I0822 23:34:21.305570  2111 net.cpp:150] Setting up trans_layer_conv0
I0822 23:34:21.305593  2111 net.cpp:157] Top shape: 64 64 100 1 (409600)
I0822 23:34:21.305596  2111 net.cpp:165] Memory required for data: 9319168
I0822 23:34:21.305606  2111 layer_factory.hpp:77] Creating layer trans_layer_conv0_trans_layer_conv0_0_split
I0822 23:34:21.305611  2111 net.cpp:106] Creating Layer trans_layer_conv0_trans_layer_conv0_0_split
I0822 23:34:21.305614  2111 net.cpp:454] trans_layer_conv0_trans_layer_conv0_0_split <- trans_layer_conv0
I0822 23:34:21.305618  2111 net.cpp:411] trans_layer_conv0_trans_layer_conv0_0_split -> trans_layer_conv0_trans_layer_conv0_0_split_0
I0822 23:34:21.305622  2111 net.cpp:411] trans_layer_conv0_trans_layer_conv0_0_split -> trans_layer_conv0_trans_layer_conv0_0_split_1
I0822 23:34:21.305646  2111 net.cpp:150] Setting up trans_layer_conv0_trans_layer_conv0_0_split
I0822 23:34:21.305650  2111 net.cpp:157] Top shape: 64 64 100 1 (409600)
I0822 23:34:21.305654  2111 net.cpp:157] Top shape: 64 64 100 1 (409600)
I0822 23:34:21.305655  2111 net.cpp:165] Memory required for data: 12595968
I0822 23:34:21.305657  2111 layer_factory.hpp:77] Creating layer trans_layer_bn0
I0822 23:34:21.305667  2111 net.cpp:106] Creating Layer trans_layer_bn0
I0822 23:34:21.305670  2111 net.cpp:454] trans_layer_bn0 <- trans_layer_conv0_trans_layer_conv0_0_split_0
I0822 23:34:21.305673  2111 net.cpp:411] trans_layer_bn0 -> trans_layer_bn0
I0822 23:34:21.305796  2111 net.cpp:150] Setting up trans_layer_bn0
I0822 23:34:21.305801  2111 net.cpp:157] Top shape: 64 64 100 1 (409600)
I0822 23:34:21.305804  2111 net.cpp:165] Memory required for data: 14234368
I0822 23:34:21.305810  2111 layer_factory.hpp:77] Creating layer trans_layer_relu0
I0822 23:34:21.305815  2111 net.cpp:106] Creating Layer trans_layer_relu0
I0822 23:34:21.305819  2111 net.cpp:454] trans_layer_relu0 <- trans_layer_bn0
I0822 23:34:21.305821  2111 net.cpp:397] trans_layer_relu0 -> trans_layer_bn0 (in-place)
I0822 23:34:21.305827  2111 net.cpp:150] Setting up trans_layer_relu0
I0822 23:34:21.305830  2111 net.cpp:157] Top shape: 64 64 100 1 (409600)
I0822 23:34:21.305832  2111 net.cpp:165] Memory required for data: 15872768
I0822 23:34:21.305835  2111 layer_factory.hpp:77] Creating layer trans_layer_conv1
I0822 23:34:21.305840  2111 net.cpp:106] Creating Layer trans_layer_conv1
I0822 23:34:21.305842  2111 net.cpp:454] trans_layer_conv1 <- trans_layer_bn0
I0822 23:34:21.305846  2111 net.cpp:411] trans_layer_conv1 -> trans_layer_conv1
I0822 23:34:21.307360  2111 net.cpp:150] Setting up trans_layer_conv1
I0822 23:34:21.307374  2111 net.cpp:157] Top shape: 64 64 100 1 (409600)
I0822 23:34:21.307376  2111 net.cpp:165] Memory required for data: 17511168
I0822 23:34:21.307381  2111 layer_factory.hpp:77] Creating layer trans_layer_conv1_trans_layer_conv1_0_split
I0822 23:34:21.307386  2111 net.cpp:106] Creating Layer trans_layer_conv1_trans_layer_conv1_0_split
I0822 23:34:21.307389  2111 net.cpp:454] trans_layer_conv1_trans_layer_conv1_0_split <- trans_layer_conv1
I0822 23:34:21.307394  2111 net.cpp:411] trans_layer_conv1_trans_layer_conv1_0_split -> trans_layer_conv1_trans_layer_conv1_0_split_0
I0822 23:34:21.307399  2111 net.cpp:411] trans_layer_conv1_trans_layer_conv1_0_split -> trans_layer_conv1_trans_layer_conv1_0_split_1
I0822 23:34:21.307422  2111 net.cpp:150] Setting up trans_layer_conv1_trans_layer_conv1_0_split
I0822 23:34:21.307426  2111 net.cpp:157] Top shape: 64 64 100 1 (409600)
I0822 23:34:21.307430  2111 net.cpp:157] Top shape: 64 64 100 1 (409600)
I0822 23:34:21.307431  2111 net.cpp:165] Memory required for data: 20787968
I0822 23:34:21.307435  2111 layer_factory.hpp:77] Creating layer trans_layer_bn1
I0822 23:34:21.307440  2111 net.cpp:106] Creating Layer trans_layer_bn1
I0822 23:34:21.307442  2111 net.cpp:454] trans_layer_bn1 <- trans_layer_conv1_trans_layer_conv1_0_split_0
I0822 23:34:21.307446  2111 net.cpp:411] trans_layer_bn1 -> trans_layer_bn1
I0822 23:34:21.307564  2111 net.cpp:150] Setting up trans_layer_bn1
I0822 23:34:21.307574  2111 net.cpp:157] Top shape: 64 64 100 1 (409600)
I0822 23:34:21.307576  2111 net.cpp:165] Memory required for data: 22426368
I0822 23:34:21.307585  2111 layer_factory.hpp:77] Creating layer trans_layer_relu1
I0822 23:34:21.307588  2111 net.cpp:106] Creating Layer trans_layer_relu1
I0822 23:34:21.307591  2111 net.cpp:454] trans_layer_relu1 <- trans_layer_bn1
I0822 23:34:21.307595  2111 net.cpp:397] trans_layer_relu1 -> trans_layer_bn1 (in-place)
I0822 23:34:21.307598  2111 net.cpp:150] Setting up trans_layer_relu1
I0822 23:34:21.307601  2111 net.cpp:157] Top shape: 64 64 100 1 (409600)
I0822 23:34:21.307605  2111 net.cpp:165] Memory required for data: 24064768
I0822 23:34:21.307606  2111 layer_factory.hpp:77] Creating layer trans_layer_conv2
I0822 23:34:21.307612  2111 net.cpp:106] Creating Layer trans_layer_conv2
I0822 23:34:21.307615  2111 net.cpp:454] trans_layer_conv2 <- trans_layer_bn1
I0822 23:34:21.307618  2111 net.cpp:411] trans_layer_conv2 -> trans_layer_conv2
I0822 23:34:21.307853  2111 net.cpp:150] Setting up trans_layer_conv2
I0822 23:34:21.307860  2111 net.cpp:157] Top shape: 64 64 100 1 (409600)
I0822 23:34:21.307862  2111 net.cpp:165] Memory required for data: 25703168
I0822 23:34:21.307868  2111 layer_factory.hpp:77] Creating layer trans_layer_conv2_trans_layer_conv2_0_split
I0822 23:34:21.307871  2111 net.cpp:106] Creating Layer trans_layer_conv2_trans_layer_conv2_0_split
I0822 23:34:21.307873  2111 net.cpp:454] trans_layer_conv2_trans_layer_conv2_0_split <- trans_layer_conv2
I0822 23:34:21.307878  2111 net.cpp:411] trans_layer_conv2_trans_layer_conv2_0_split -> trans_layer_conv2_trans_layer_conv2_0_split_0
I0822 23:34:21.307881  2111 net.cpp:411] trans_layer_conv2_trans_layer_conv2_0_split -> trans_layer_conv2_trans_layer_conv2_0_split_1
I0822 23:34:21.307902  2111 net.cpp:150] Setting up trans_layer_conv2_trans_layer_conv2_0_split
I0822 23:34:21.307906  2111 net.cpp:157] Top shape: 64 64 100 1 (409600)
I0822 23:34:21.307909  2111 net.cpp:157] Top shape: 64 64 100 1 (409600)
I0822 23:34:21.307911  2111 net.cpp:165] Memory required for data: 28979968
I0822 23:34:21.307914  2111 layer_factory.hpp:77] Creating layer trans_layer_bn2
I0822 23:34:21.307919  2111 net.cpp:106] Creating Layer trans_layer_bn2
I0822 23:34:21.307921  2111 net.cpp:454] trans_layer_bn2 <- trans_layer_conv2_trans_layer_conv2_0_split_0
I0822 23:34:21.307924  2111 net.cpp:411] trans_layer_bn2 -> trans_layer_bn2
I0822 23:34:21.308043  2111 net.cpp:150] Setting up trans_layer_bn2
I0822 23:34:21.308054  2111 net.cpp:157] Top shape: 64 64 100 1 (409600)
I0822 23:34:21.308058  2111 net.cpp:165] Memory required for data: 30618368
I0822 23:34:21.308063  2111 layer_factory.hpp:77] Creating layer trans_layer_relu2
I0822 23:34:21.308066  2111 net.cpp:106] Creating Layer trans_layer_relu2
I0822 23:34:21.308069  2111 net.cpp:454] trans_layer_relu2 <- trans_layer_bn2
I0822 23:34:21.308074  2111 net.cpp:397] trans_layer_relu2 -> trans_layer_bn2 (in-place)
I0822 23:34:21.308079  2111 net.cpp:150] Setting up trans_layer_relu2
I0822 23:34:21.308082  2111 net.cpp:157] Top shape: 64 64 100 1 (409600)
I0822 23:34:21.308084  2111 net.cpp:165] Memory required for data: 32256768
I0822 23:34:21.308087  2111 layer_factory.hpp:77] Creating layer dense_layer_conv0
I0822 23:34:21.308092  2111 net.cpp:106] Creating Layer dense_layer_conv0
I0822 23:34:21.308095  2111 net.cpp:454] dense_layer_conv0 <- trans_layer_bn2
I0822 23:34:21.308099  2111 net.cpp:411] dense_layer_conv0 -> dense_layer_conv0
I0822 23:34:21.309258  2111 net.cpp:150] Setting up dense_layer_conv0
I0822 23:34:21.309270  2111 net.cpp:157] Top shape: 64 64 100 1 (409600)
I0822 23:34:21.309273  2111 net.cpp:165] Memory required for data: 33895168
I0822 23:34:21.309281  2111 layer_factory.hpp:77] Creating layer dense_layer_concat0
I0822 23:34:21.309286  2111 net.cpp:106] Creating Layer dense_layer_concat0
I0822 23:34:21.309289  2111 net.cpp:454] dense_layer_concat0 <- dense_layer_conv0
I0822 23:34:21.309293  2111 net.cpp:454] dense_layer_concat0 <- trans_layer_conv0_trans_layer_conv0_0_split_1
I0822 23:34:21.309296  2111 net.cpp:454] dense_layer_concat0 <- trans_layer_conv1_trans_layer_conv1_0_split_1
I0822 23:34:21.309298  2111 net.cpp:454] dense_layer_concat0 <- trans_layer_conv2_trans_layer_conv2_0_split_1
I0822 23:34:21.309303  2111 net.cpp:411] dense_layer_concat0 -> dense_layer_concat0
I0822 23:34:21.309329  2111 net.cpp:150] Setting up dense_layer_concat0
I0822 23:34:21.309334  2111 net.cpp:157] Top shape: 64 256 100 1 (1638400)
I0822 23:34:21.309335  2111 net.cpp:165] Memory required for data: 40448768
I0822 23:34:21.309339  2111 layer_factory.hpp:77] Creating layer dense_layer_bn0
I0822 23:34:21.309345  2111 net.cpp:106] Creating Layer dense_layer_bn0
I0822 23:34:21.309346  2111 net.cpp:454] dense_layer_bn0 <- dense_layer_concat0
I0822 23:34:21.309350  2111 net.cpp:411] dense_layer_bn0 -> dense_layer_bn0
I0822 23:34:21.309468  2111 net.cpp:150] Setting up dense_layer_bn0
I0822 23:34:21.309471  2111 net.cpp:157] Top shape: 64 256 100 1 (1638400)
I0822 23:34:21.309473  2111 net.cpp:165] Memory required for data: 47002368
I0822 23:34:21.309479  2111 layer_factory.hpp:77] Creating layer dense_layer_relu0
I0822 23:34:21.309484  2111 net.cpp:106] Creating Layer dense_layer_relu0
I0822 23:34:21.309485  2111 net.cpp:454] dense_layer_relu0 <- dense_layer_bn0
I0822 23:34:21.309489  2111 net.cpp:397] dense_layer_relu0 -> dense_layer_bn0 (in-place)
I0822 23:34:21.309494  2111 net.cpp:150] Setting up dense_layer_relu0
I0822 23:34:21.309496  2111 net.cpp:157] Top shape: 64 256 100 1 (1638400)
I0822 23:34:21.309499  2111 net.cpp:165] Memory required for data: 53555968
I0822 23:34:21.309501  2111 layer_factory.hpp:77] Creating layer dense_layer_pool0
I0822 23:34:21.309506  2111 net.cpp:106] Creating Layer dense_layer_pool0
I0822 23:34:21.309509  2111 net.cpp:454] dense_layer_pool0 <- dense_layer_bn0
I0822 23:34:21.309511  2111 net.cpp:411] dense_layer_pool0 -> dense_layer_pool0
I0822 23:34:21.309535  2111 net.cpp:150] Setting up dense_layer_pool0
I0822 23:34:21.309540  2111 net.cpp:157] Top shape: 64 256 51 1 (835584)
I0822 23:34:21.309541  2111 net.cpp:165] Memory required for data: 56898304
I0822 23:34:21.309543  2111 layer_factory.hpp:77] Creating layer dense_layer_pool0_dense_layer_pool0_0_split
I0822 23:34:21.309547  2111 net.cpp:106] Creating Layer dense_layer_pool0_dense_layer_pool0_0_split
I0822 23:34:21.309551  2111 net.cpp:454] dense_layer_pool0_dense_layer_pool0_0_split <- dense_layer_pool0
I0822 23:34:21.309553  2111 net.cpp:411] dense_layer_pool0_dense_layer_pool0_0_split -> dense_layer_pool0_dense_layer_pool0_0_split_0
I0822 23:34:21.309557  2111 net.cpp:411] dense_layer_pool0_dense_layer_pool0_0_split -> dense_layer_pool0_dense_layer_pool0_0_split_1
I0822 23:34:21.309581  2111 net.cpp:150] Setting up dense_layer_pool0_dense_layer_pool0_0_split
I0822 23:34:21.309584  2111 net.cpp:157] Top shape: 64 256 51 1 (835584)
I0822 23:34:21.309588  2111 net.cpp:157] Top shape: 64 256 51 1 (835584)
I0822 23:34:21.309590  2111 net.cpp:165] Memory required for data: 63582976
I0822 23:34:21.309592  2111 layer_factory.hpp:77] Creating layer dense_layer_conv1
I0822 23:34:21.309599  2111 net.cpp:106] Creating Layer dense_layer_conv1
I0822 23:34:21.309602  2111 net.cpp:454] dense_layer_conv1 <- dense_layer_pool0_dense_layer_pool0_0_split_0
I0822 23:34:21.309605  2111 net.cpp:411] dense_layer_conv1 -> dense_layer_conv1
I0822 23:34:21.310940  2111 net.cpp:150] Setting up dense_layer_conv1
I0822 23:34:21.310947  2111 net.cpp:157] Top shape: 64 64 51 1 (208896)
I0822 23:34:21.310950  2111 net.cpp:165] Memory required for data: 64418560
I0822 23:34:21.310955  2111 layer_factory.hpp:77] Creating layer dense_layer_concat1
I0822 23:34:21.310958  2111 net.cpp:106] Creating Layer dense_layer_concat1
I0822 23:34:21.310961  2111 net.cpp:454] dense_layer_concat1 <- dense_layer_conv1
I0822 23:34:21.310963  2111 net.cpp:454] dense_layer_concat1 <- dense_layer_pool0_dense_layer_pool0_0_split_1
I0822 23:34:21.310967  2111 net.cpp:411] dense_layer_concat1 -> dense_layer_concat1
I0822 23:34:21.310981  2111 net.cpp:150] Setting up dense_layer_concat1
I0822 23:34:21.310986  2111 net.cpp:157] Top shape: 64 320 51 1 (1044480)
I0822 23:34:21.310987  2111 net.cpp:165] Memory required for data: 68596480
I0822 23:34:21.310991  2111 layer_factory.hpp:77] Creating layer dense_layer_bn1
I0822 23:34:21.310994  2111 net.cpp:106] Creating Layer dense_layer_bn1
I0822 23:34:21.310997  2111 net.cpp:454] dense_layer_bn1 <- dense_layer_concat1
I0822 23:34:21.311000  2111 net.cpp:411] dense_layer_bn1 -> dense_layer_bn1
I0822 23:34:21.311842  2111 net.cpp:150] Setting up dense_layer_bn1
I0822 23:34:21.311854  2111 net.cpp:157] Top shape: 64 320 51 1 (1044480)
I0822 23:34:21.311857  2111 net.cpp:165] Memory required for data: 72774400
I0822 23:34:21.311863  2111 layer_factory.hpp:77] Creating layer dense_layer_relu1
I0822 23:34:21.311868  2111 net.cpp:106] Creating Layer dense_layer_relu1
I0822 23:34:21.311872  2111 net.cpp:454] dense_layer_relu1 <- dense_layer_bn1
I0822 23:34:21.311875  2111 net.cpp:397] dense_layer_relu1 -> dense_layer_bn1 (in-place)
I0822 23:34:21.311880  2111 net.cpp:150] Setting up dense_layer_relu1
I0822 23:34:21.311887  2111 net.cpp:157] Top shape: 64 320 51 1 (1044480)
I0822 23:34:21.311888  2111 net.cpp:165] Memory required for data: 76952320
I0822 23:34:21.311890  2111 layer_factory.hpp:77] Creating layer dense_layer_pool1
I0822 23:34:21.311894  2111 net.cpp:106] Creating Layer dense_layer_pool1
I0822 23:34:21.311897  2111 net.cpp:454] dense_layer_pool1 <- dense_layer_bn1
I0822 23:34:21.311900  2111 net.cpp:411] dense_layer_pool1 -> dense_layer_pool1
I0822 23:34:21.311915  2111 net.cpp:150] Setting up dense_layer_pool1
I0822 23:34:21.311919  2111 net.cpp:157] Top shape: 64 320 26 1 (532480)
I0822 23:34:21.311921  2111 net.cpp:165] Memory required for data: 79082240
I0822 23:34:21.311924  2111 layer_factory.hpp:77] Creating layer dense_layer_pool1_dense_layer_pool1_0_split
I0822 23:34:21.311928  2111 net.cpp:106] Creating Layer dense_layer_pool1_dense_layer_pool1_0_split
I0822 23:34:21.311930  2111 net.cpp:454] dense_layer_pool1_dense_layer_pool1_0_split <- dense_layer_pool1
I0822 23:34:21.311933  2111 net.cpp:411] dense_layer_pool1_dense_layer_pool1_0_split -> dense_layer_pool1_dense_layer_pool1_0_split_0
I0822 23:34:21.311938  2111 net.cpp:411] dense_layer_pool1_dense_layer_pool1_0_split -> dense_layer_pool1_dense_layer_pool1_0_split_1
I0822 23:34:21.311957  2111 net.cpp:150] Setting up dense_layer_pool1_dense_layer_pool1_0_split
I0822 23:34:21.311961  2111 net.cpp:157] Top shape: 64 320 26 1 (532480)
I0822 23:34:21.311964  2111 net.cpp:157] Top shape: 64 320 26 1 (532480)
I0822 23:34:21.311966  2111 net.cpp:165] Memory required for data: 83342080
I0822 23:34:21.311969  2111 layer_factory.hpp:77] Creating layer dense_layer_conv2
I0822 23:34:21.311975  2111 net.cpp:106] Creating Layer dense_layer_conv2
I0822 23:34:21.311976  2111 net.cpp:454] dense_layer_conv2 <- dense_layer_pool1_dense_layer_pool1_0_split_0
I0822 23:34:21.311980  2111 net.cpp:411] dense_layer_conv2 -> dense_layer_conv2
I0822 23:34:21.313617  2111 net.cpp:150] Setting up dense_layer_conv2
I0822 23:34:21.313627  2111 net.cpp:157] Top shape: 64 64 26 1 (106496)
I0822 23:34:21.313628  2111 net.cpp:165] Memory required for data: 83768064
I0822 23:34:21.313634  2111 layer_factory.hpp:77] Creating layer dense_layer_concat2
I0822 23:34:21.313638  2111 net.cpp:106] Creating Layer dense_layer_concat2
I0822 23:34:21.313642  2111 net.cpp:454] dense_layer_concat2 <- dense_layer_conv2
I0822 23:34:21.313644  2111 net.cpp:454] dense_layer_concat2 <- dense_layer_pool1_dense_layer_pool1_0_split_1
I0822 23:34:21.313648  2111 net.cpp:411] dense_layer_concat2 -> dense_layer_concat2
I0822 23:34:21.313663  2111 net.cpp:150] Setting up dense_layer_concat2
I0822 23:34:21.313668  2111 net.cpp:157] Top shape: 64 384 26 1 (638976)
I0822 23:34:21.313670  2111 net.cpp:165] Memory required for data: 86323968
I0822 23:34:21.313673  2111 layer_factory.hpp:77] Creating layer dense_layer_bn2
I0822 23:34:21.313676  2111 net.cpp:106] Creating Layer dense_layer_bn2
I0822 23:34:21.313678  2111 net.cpp:454] dense_layer_bn2 <- dense_layer_concat2
I0822 23:34:21.313683  2111 net.cpp:411] dense_layer_bn2 -> dense_layer_bn2
I0822 23:34:21.313804  2111 net.cpp:150] Setting up dense_layer_bn2
I0822 23:34:21.313819  2111 net.cpp:157] Top shape: 64 384 26 1 (638976)
I0822 23:34:21.313822  2111 net.cpp:165] Memory required for data: 88879872
I0822 23:34:21.313827  2111 layer_factory.hpp:77] Creating layer dense_layer_relu2
I0822 23:34:21.313832  2111 net.cpp:106] Creating Layer dense_layer_relu2
I0822 23:34:21.313834  2111 net.cpp:454] dense_layer_relu2 <- dense_layer_bn2
I0822 23:34:21.313838  2111 net.cpp:397] dense_layer_relu2 -> dense_layer_bn2 (in-place)
I0822 23:34:21.313841  2111 net.cpp:150] Setting up dense_layer_relu2
I0822 23:34:21.313844  2111 net.cpp:157] Top shape: 64 384 26 1 (638976)
I0822 23:34:21.313848  2111 net.cpp:165] Memory required for data: 91435776
I0822 23:34:21.313849  2111 layer_factory.hpp:77] Creating layer dense_layer_pool2
I0822 23:34:21.313853  2111 net.cpp:106] Creating Layer dense_layer_pool2
I0822 23:34:21.313856  2111 net.cpp:454] dense_layer_pool2 <- dense_layer_bn2
I0822 23:34:21.313859  2111 net.cpp:411] dense_layer_pool2 -> dense_layer_pool2
I0822 23:34:21.313876  2111 net.cpp:150] Setting up dense_layer_pool2
I0822 23:34:21.313880  2111 net.cpp:157] Top shape: 64 384 14 1 (344064)
I0822 23:34:21.313884  2111 net.cpp:165] Memory required for data: 92812032
I0822 23:34:21.313885  2111 layer_factory.hpp:77] Creating layer dense_layer_pool2_dense_layer_pool2_0_split
I0822 23:34:21.313889  2111 net.cpp:106] Creating Layer dense_layer_pool2_dense_layer_pool2_0_split
I0822 23:34:21.313891  2111 net.cpp:454] dense_layer_pool2_dense_layer_pool2_0_split <- dense_layer_pool2
I0822 23:34:21.313895  2111 net.cpp:411] dense_layer_pool2_dense_layer_pool2_0_split -> dense_layer_pool2_dense_layer_pool2_0_split_0
I0822 23:34:21.313901  2111 net.cpp:411] dense_layer_pool2_dense_layer_pool2_0_split -> dense_layer_pool2_dense_layer_pool2_0_split_1
I0822 23:34:21.313921  2111 net.cpp:150] Setting up dense_layer_pool2_dense_layer_pool2_0_split
I0822 23:34:21.313925  2111 net.cpp:157] Top shape: 64 384 14 1 (344064)
I0822 23:34:21.313928  2111 net.cpp:157] Top shape: 64 384 14 1 (344064)
I0822 23:34:21.313930  2111 net.cpp:165] Memory required for data: 95564544
I0822 23:34:21.313932  2111 layer_factory.hpp:77] Creating layer dense_layer_conv3
I0822 23:34:21.313941  2111 net.cpp:106] Creating Layer dense_layer_conv3
I0822 23:34:21.313942  2111 net.cpp:454] dense_layer_conv3 <- dense_layer_pool2_dense_layer_pool2_0_split_0
I0822 23:34:21.313946  2111 net.cpp:411] dense_layer_conv3 -> dense_layer_conv3
I0822 23:34:21.316542  2111 net.cpp:150] Setting up dense_layer_conv3
I0822 23:34:21.316555  2111 net.cpp:157] Top shape: 64 64 14 1 (57344)
I0822 23:34:21.316557  2111 net.cpp:165] Memory required for data: 95793920
I0822 23:34:21.316562  2111 layer_factory.hpp:77] Creating layer dense_layer_concat3
I0822 23:34:21.316567  2111 net.cpp:106] Creating Layer dense_layer_concat3
I0822 23:34:21.316570  2111 net.cpp:454] dense_layer_concat3 <- dense_layer_conv3
I0822 23:34:21.316573  2111 net.cpp:454] dense_layer_concat3 <- dense_layer_pool2_dense_layer_pool2_0_split_1
I0822 23:34:21.316577  2111 net.cpp:411] dense_layer_concat3 -> dense_layer_concat3
I0822 23:34:21.316594  2111 net.cpp:150] Setting up dense_layer_concat3
I0822 23:34:21.316598  2111 net.cpp:157] Top shape: 64 448 14 1 (401408)
I0822 23:34:21.316601  2111 net.cpp:165] Memory required for data: 97399552
I0822 23:34:21.316603  2111 layer_factory.hpp:77] Creating layer dense_layer_bn3
I0822 23:34:21.316607  2111 net.cpp:106] Creating Layer dense_layer_bn3
I0822 23:34:21.316610  2111 net.cpp:454] dense_layer_bn3 <- dense_layer_concat3
I0822 23:34:21.316613  2111 net.cpp:411] dense_layer_bn3 -> dense_layer_bn3
I0822 23:34:21.316759  2111 net.cpp:150] Setting up dense_layer_bn3
I0822 23:34:21.316766  2111 net.cpp:157] Top shape: 64 448 14 1 (401408)
I0822 23:34:21.316767  2111 net.cpp:165] Memory required for data: 99005184
I0822 23:34:21.316777  2111 layer_factory.hpp:77] Creating layer dense_layer_relu3
I0822 23:34:21.316781  2111 net.cpp:106] Creating Layer dense_layer_relu3
I0822 23:34:21.316783  2111 net.cpp:454] dense_layer_relu3 <- dense_layer_bn3
I0822 23:34:21.316787  2111 net.cpp:397] dense_layer_relu3 -> dense_layer_bn3 (in-place)
I0822 23:34:21.316792  2111 net.cpp:150] Setting up dense_layer_relu3
I0822 23:34:21.316794  2111 net.cpp:157] Top shape: 64 448 14 1 (401408)
I0822 23:34:21.316797  2111 net.cpp:165] Memory required for data: 100610816
I0822 23:34:21.316798  2111 layer_factory.hpp:77] Creating layer dense_layer_pool3
I0822 23:34:21.316803  2111 net.cpp:106] Creating Layer dense_layer_pool3
I0822 23:34:21.316805  2111 net.cpp:454] dense_layer_pool3 <- dense_layer_bn3
I0822 23:34:21.316808  2111 net.cpp:411] dense_layer_pool3 -> dense_layer_pool3
I0822 23:34:21.317392  2111 net.cpp:150] Setting up dense_layer_pool3
I0822 23:34:21.317400  2111 net.cpp:157] Top shape: 64 448 8 1 (229376)
I0822 23:34:21.317402  2111 net.cpp:165] Memory required for data: 101528320
I0822 23:34:21.317405  2111 layer_factory.hpp:77] Creating layer dense_layer_pool3_dense_layer_pool3_0_split
I0822 23:34:21.317409  2111 net.cpp:106] Creating Layer dense_layer_pool3_dense_layer_pool3_0_split
I0822 23:34:21.317411  2111 net.cpp:454] dense_layer_pool3_dense_layer_pool3_0_split <- dense_layer_pool3
I0822 23:34:21.317415  2111 net.cpp:411] dense_layer_pool3_dense_layer_pool3_0_split -> dense_layer_pool3_dense_layer_pool3_0_split_0
I0822 23:34:21.317420  2111 net.cpp:411] dense_layer_pool3_dense_layer_pool3_0_split -> dense_layer_pool3_dense_layer_pool3_0_split_1
I0822 23:34:21.317441  2111 net.cpp:150] Setting up dense_layer_pool3_dense_layer_pool3_0_split
I0822 23:34:21.317445  2111 net.cpp:157] Top shape: 64 448 8 1 (229376)
I0822 23:34:21.317448  2111 net.cpp:157] Top shape: 64 448 8 1 (229376)
I0822 23:34:21.317451  2111 net.cpp:165] Memory required for data: 103363328
I0822 23:34:21.317452  2111 layer_factory.hpp:77] Creating layer dense_layer_conv4
I0822 23:34:21.317458  2111 net.cpp:106] Creating Layer dense_layer_conv4
I0822 23:34:21.317461  2111 net.cpp:454] dense_layer_conv4 <- dense_layer_pool3_dense_layer_pool3_0_split_0
I0822 23:34:21.317466  2111 net.cpp:411] dense_layer_conv4 -> dense_layer_conv4
I0822 23:34:21.319692  2111 net.cpp:150] Setting up dense_layer_conv4
I0822 23:34:21.319700  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.319703  2111 net.cpp:165] Memory required for data: 103494400
I0822 23:34:21.319708  2111 layer_factory.hpp:77] Creating layer dense_layer_concat4
I0822 23:34:21.319712  2111 net.cpp:106] Creating Layer dense_layer_concat4
I0822 23:34:21.319715  2111 net.cpp:454] dense_layer_concat4 <- dense_layer_conv4
I0822 23:34:21.319718  2111 net.cpp:454] dense_layer_concat4 <- dense_layer_pool3_dense_layer_pool3_0_split_1
I0822 23:34:21.319722  2111 net.cpp:411] dense_layer_concat4 -> dense_layer_concat4
I0822 23:34:21.319737  2111 net.cpp:150] Setting up dense_layer_concat4
I0822 23:34:21.319741  2111 net.cpp:157] Top shape: 64 512 8 1 (262144)
I0822 23:34:21.319743  2111 net.cpp:165] Memory required for data: 104542976
I0822 23:34:21.319746  2111 layer_factory.hpp:77] Creating layer dense_layer_bn4
I0822 23:34:21.319751  2111 net.cpp:106] Creating Layer dense_layer_bn4
I0822 23:34:21.319753  2111 net.cpp:454] dense_layer_bn4 <- dense_layer_concat4
I0822 23:34:21.319756  2111 net.cpp:411] dense_layer_bn4 -> dense_layer_bn4
I0822 23:34:21.319880  2111 net.cpp:150] Setting up dense_layer_bn4
I0822 23:34:21.319885  2111 net.cpp:157] Top shape: 64 512 8 1 (262144)
I0822 23:34:21.319888  2111 net.cpp:165] Memory required for data: 105591552
I0822 23:34:21.319893  2111 layer_factory.hpp:77] Creating layer dense_layer_relu4
I0822 23:34:21.319897  2111 net.cpp:106] Creating Layer dense_layer_relu4
I0822 23:34:21.319900  2111 net.cpp:454] dense_layer_relu4 <- dense_layer_bn4
I0822 23:34:21.319903  2111 net.cpp:397] dense_layer_relu4 -> dense_layer_bn4 (in-place)
I0822 23:34:21.319907  2111 net.cpp:150] Setting up dense_layer_relu4
I0822 23:34:21.319911  2111 net.cpp:157] Top shape: 64 512 8 1 (262144)
I0822 23:34:21.319912  2111 net.cpp:165] Memory required for data: 106640128
I0822 23:34:21.319914  2111 layer_factory.hpp:77] Creating layer attention_layer_conv0
I0822 23:34:21.319922  2111 net.cpp:106] Creating Layer attention_layer_conv0
I0822 23:34:21.319926  2111 net.cpp:454] attention_layer_conv0 <- dense_layer_bn4
I0822 23:34:21.319928  2111 net.cpp:411] attention_layer_conv0 -> attention_layer_conv0
I0822 23:34:21.320864  2111 net.cpp:150] Setting up attention_layer_conv0
I0822 23:34:21.320870  2111 net.cpp:157] Top shape: 64 512 8 1 (262144)
I0822 23:34:21.320873  2111 net.cpp:165] Memory required for data: 107688704
I0822 23:34:21.320878  2111 layer_factory.hpp:77] Creating layer attention_layer_bn0
I0822 23:34:21.320883  2111 net.cpp:106] Creating Layer attention_layer_bn0
I0822 23:34:21.320884  2111 net.cpp:454] attention_layer_bn0 <- attention_layer_conv0
I0822 23:34:21.320888  2111 net.cpp:411] attention_layer_bn0 -> attention_layer_bn0
I0822 23:34:21.321009  2111 net.cpp:150] Setting up attention_layer_bn0
I0822 23:34:21.321015  2111 net.cpp:157] Top shape: 64 512 8 1 (262144)
I0822 23:34:21.321017  2111 net.cpp:165] Memory required for data: 108737280
I0822 23:34:21.321022  2111 layer_factory.hpp:77] Creating layer attention_layer_relu0
I0822 23:34:21.321027  2111 net.cpp:106] Creating Layer attention_layer_relu0
I0822 23:34:21.321029  2111 net.cpp:454] attention_layer_relu0 <- attention_layer_bn0
I0822 23:34:21.321034  2111 net.cpp:397] attention_layer_relu0 -> attention_layer_bn0 (in-place)
I0822 23:34:21.321038  2111 net.cpp:150] Setting up attention_layer_relu0
I0822 23:34:21.321043  2111 net.cpp:157] Top shape: 64 512 8 1 (262144)
I0822 23:34:21.321044  2111 net.cpp:165] Memory required for data: 109785856
I0822 23:34:21.321046  2111 layer_factory.hpp:77] Creating layer attention_layer_conv1
I0822 23:34:21.321051  2111 net.cpp:106] Creating Layer attention_layer_conv1
I0822 23:34:21.321054  2111 net.cpp:454] attention_layer_conv1 <- attention_layer_bn0
I0822 23:34:21.321058  2111 net.cpp:411] attention_layer_conv1 -> attention_layer_conv1
I0822 23:34:21.321997  2111 net.cpp:150] Setting up attention_layer_conv1
I0822 23:34:21.322005  2111 net.cpp:157] Top shape: 64 512 8 1 (262144)
I0822 23:34:21.322007  2111 net.cpp:165] Memory required for data: 110834432
I0822 23:34:21.322012  2111 layer_factory.hpp:77] Creating layer attention_layer_bn1
I0822 23:34:21.322016  2111 net.cpp:106] Creating Layer attention_layer_bn1
I0822 23:34:21.322019  2111 net.cpp:454] attention_layer_bn1 <- attention_layer_conv1
I0822 23:34:21.322026  2111 net.cpp:411] attention_layer_bn1 -> attention_layer_bn1
I0822 23:34:21.322147  2111 net.cpp:150] Setting up attention_layer_bn1
I0822 23:34:21.322154  2111 net.cpp:157] Top shape: 64 512 8 1 (262144)
I0822 23:34:21.322156  2111 net.cpp:165] Memory required for data: 111883008
I0822 23:34:21.322161  2111 layer_factory.hpp:77] Creating layer attention_layer_relu1
I0822 23:34:21.322165  2111 net.cpp:106] Creating Layer attention_layer_relu1
I0822 23:34:21.322167  2111 net.cpp:454] attention_layer_relu1 <- attention_layer_bn1
I0822 23:34:21.322171  2111 net.cpp:397] attention_layer_relu1 -> attention_layer_bn1 (in-place)
I0822 23:34:21.322175  2111 net.cpp:150] Setting up attention_layer_relu1
I0822 23:34:21.322178  2111 net.cpp:157] Top shape: 64 512 8 1 (262144)
I0822 23:34:21.322180  2111 net.cpp:165] Memory required for data: 112931584
I0822 23:34:21.322182  2111 layer_factory.hpp:77] Creating layer attention_scale_slice
I0822 23:34:21.322191  2111 net.cpp:106] Creating Layer attention_scale_slice
I0822 23:34:21.322194  2111 net.cpp:454] attention_scale_slice <- attention_layer_bn1
I0822 23:34:21.322197  2111 net.cpp:411] attention_scale_slice -> attention_scale0
I0822 23:34:21.322203  2111 net.cpp:411] attention_scale_slice -> attention_scale1
I0822 23:34:21.322208  2111 net.cpp:411] attention_scale_slice -> attention_scale2
I0822 23:34:21.322212  2111 net.cpp:411] attention_scale_slice -> attention_scale3
I0822 23:34:21.322216  2111 net.cpp:411] attention_scale_slice -> attention_scale4
I0822 23:34:21.322221  2111 net.cpp:411] attention_scale_slice -> attention_scale5
I0822 23:34:21.322223  2111 net.cpp:411] attention_scale_slice -> attention_scale6
I0822 23:34:21.322227  2111 net.cpp:411] attention_scale_slice -> attention_scale7
I0822 23:34:21.322294  2111 net.cpp:150] Setting up attention_scale_slice
I0822 23:34:21.322299  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322301  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322304  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322306  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322309  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322312  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322314  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322317  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322319  2111 net.cpp:165] Memory required for data: 113980160
I0822 23:34:21.322321  2111 layer_factory.hpp:77] Creating layer attention_scale0_attention_scale_slice_0_split
I0822 23:34:21.322325  2111 net.cpp:106] Creating Layer attention_scale0_attention_scale_slice_0_split
I0822 23:34:21.322327  2111 net.cpp:454] attention_scale0_attention_scale_slice_0_split <- attention_scale0
I0822 23:34:21.322331  2111 net.cpp:411] attention_scale0_attention_scale_slice_0_split -> attention_scale0_attention_scale_slice_0_split_0
I0822 23:34:21.322335  2111 net.cpp:411] attention_scale0_attention_scale_slice_0_split -> attention_scale0_attention_scale_slice_0_split_1
I0822 23:34:21.322355  2111 net.cpp:150] Setting up attention_scale0_attention_scale_slice_0_split
I0822 23:34:21.322360  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322362  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322365  2111 net.cpp:165] Memory required for data: 114242304
I0822 23:34:21.322366  2111 layer_factory.hpp:77] Creating layer attention_scale1_attention_scale_slice_1_split
I0822 23:34:21.322371  2111 net.cpp:106] Creating Layer attention_scale1_attention_scale_slice_1_split
I0822 23:34:21.322372  2111 net.cpp:454] attention_scale1_attention_scale_slice_1_split <- attention_scale1
I0822 23:34:21.322376  2111 net.cpp:411] attention_scale1_attention_scale_slice_1_split -> attention_scale1_attention_scale_slice_1_split_0
I0822 23:34:21.322382  2111 net.cpp:411] attention_scale1_attention_scale_slice_1_split -> attention_scale1_attention_scale_slice_1_split_1
I0822 23:34:21.322401  2111 net.cpp:150] Setting up attention_scale1_attention_scale_slice_1_split
I0822 23:34:21.322404  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322407  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322409  2111 net.cpp:165] Memory required for data: 114504448
I0822 23:34:21.322412  2111 layer_factory.hpp:77] Creating layer attention_scale2_attention_scale_slice_2_split
I0822 23:34:21.322414  2111 net.cpp:106] Creating Layer attention_scale2_attention_scale_slice_2_split
I0822 23:34:21.322417  2111 net.cpp:454] attention_scale2_attention_scale_slice_2_split <- attention_scale2
I0822 23:34:21.322420  2111 net.cpp:411] attention_scale2_attention_scale_slice_2_split -> attention_scale2_attention_scale_slice_2_split_0
I0822 23:34:21.322424  2111 net.cpp:411] attention_scale2_attention_scale_slice_2_split -> attention_scale2_attention_scale_slice_2_split_1
I0822 23:34:21.322443  2111 net.cpp:150] Setting up attention_scale2_attention_scale_slice_2_split
I0822 23:34:21.322446  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322448  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322450  2111 net.cpp:165] Memory required for data: 114766592
I0822 23:34:21.322453  2111 layer_factory.hpp:77] Creating layer attention_scale3_attention_scale_slice_3_split
I0822 23:34:21.322456  2111 net.cpp:106] Creating Layer attention_scale3_attention_scale_slice_3_split
I0822 23:34:21.322458  2111 net.cpp:454] attention_scale3_attention_scale_slice_3_split <- attention_scale3
I0822 23:34:21.322461  2111 net.cpp:411] attention_scale3_attention_scale_slice_3_split -> attention_scale3_attention_scale_slice_3_split_0
I0822 23:34:21.322465  2111 net.cpp:411] attention_scale3_attention_scale_slice_3_split -> attention_scale3_attention_scale_slice_3_split_1
I0822 23:34:21.322484  2111 net.cpp:150] Setting up attention_scale3_attention_scale_slice_3_split
I0822 23:34:21.322489  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322490  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322492  2111 net.cpp:165] Memory required for data: 115028736
I0822 23:34:21.322494  2111 layer_factory.hpp:77] Creating layer attention_scale4_attention_scale_slice_4_split
I0822 23:34:21.322497  2111 net.cpp:106] Creating Layer attention_scale4_attention_scale_slice_4_split
I0822 23:34:21.322500  2111 net.cpp:454] attention_scale4_attention_scale_slice_4_split <- attention_scale4
I0822 23:34:21.322505  2111 net.cpp:411] attention_scale4_attention_scale_slice_4_split -> attention_scale4_attention_scale_slice_4_split_0
I0822 23:34:21.322510  2111 net.cpp:411] attention_scale4_attention_scale_slice_4_split -> attention_scale4_attention_scale_slice_4_split_1
I0822 23:34:21.322532  2111 net.cpp:150] Setting up attention_scale4_attention_scale_slice_4_split
I0822 23:34:21.322536  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322540  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322541  2111 net.cpp:165] Memory required for data: 115290880
I0822 23:34:21.322543  2111 layer_factory.hpp:77] Creating layer attention_scale5_attention_scale_slice_5_split
I0822 23:34:21.322547  2111 net.cpp:106] Creating Layer attention_scale5_attention_scale_slice_5_split
I0822 23:34:21.322549  2111 net.cpp:454] attention_scale5_attention_scale_slice_5_split <- attention_scale5
I0822 23:34:21.322552  2111 net.cpp:411] attention_scale5_attention_scale_slice_5_split -> attention_scale5_attention_scale_slice_5_split_0
I0822 23:34:21.322556  2111 net.cpp:411] attention_scale5_attention_scale_slice_5_split -> attention_scale5_attention_scale_slice_5_split_1
I0822 23:34:21.322574  2111 net.cpp:150] Setting up attention_scale5_attention_scale_slice_5_split
I0822 23:34:21.322577  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322580  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322582  2111 net.cpp:165] Memory required for data: 115553024
I0822 23:34:21.322584  2111 layer_factory.hpp:77] Creating layer attention_scale6_attention_scale_slice_6_split
I0822 23:34:21.322587  2111 net.cpp:106] Creating Layer attention_scale6_attention_scale_slice_6_split
I0822 23:34:21.322589  2111 net.cpp:454] attention_scale6_attention_scale_slice_6_split <- attention_scale6
I0822 23:34:21.322593  2111 net.cpp:411] attention_scale6_attention_scale_slice_6_split -> attention_scale6_attention_scale_slice_6_split_0
I0822 23:34:21.322597  2111 net.cpp:411] attention_scale6_attention_scale_slice_6_split -> attention_scale6_attention_scale_slice_6_split_1
I0822 23:34:21.322615  2111 net.cpp:150] Setting up attention_scale6_attention_scale_slice_6_split
I0822 23:34:21.322618  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322621  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322623  2111 net.cpp:165] Memory required for data: 115815168
I0822 23:34:21.322628  2111 layer_factory.hpp:77] Creating layer attention_scale7_attention_scale_slice_7_split
I0822 23:34:21.322631  2111 net.cpp:106] Creating Layer attention_scale7_attention_scale_slice_7_split
I0822 23:34:21.322633  2111 net.cpp:454] attention_scale7_attention_scale_slice_7_split <- attention_scale7
I0822 23:34:21.322636  2111 net.cpp:411] attention_scale7_attention_scale_slice_7_split -> attention_scale7_attention_scale_slice_7_split_0
I0822 23:34:21.322640  2111 net.cpp:411] attention_scale7_attention_scale_slice_7_split -> attention_scale7_attention_scale_slice_7_split_1
I0822 23:34:21.322659  2111 net.cpp:150] Setting up attention_scale7_attention_scale_slice_7_split
I0822 23:34:21.322662  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322665  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.322667  2111 net.cpp:165] Memory required for data: 116077312
I0822 23:34:21.322669  2111 layer_factory.hpp:77] Creating layer attention_permute0
I0822 23:34:21.322674  2111 net.cpp:106] Creating Layer attention_permute0
I0822 23:34:21.322676  2111 net.cpp:454] attention_permute0 <- attention_scale0_attention_scale_slice_0_split_0
I0822 23:34:21.322680  2111 net.cpp:411] attention_permute0 -> attention_permute0
I0822 23:34:21.322739  2111 net.cpp:150] Setting up attention_permute0
I0822 23:34:21.322743  2111 net.cpp:157] Top shape: 64 1 8 64 (32768)
I0822 23:34:21.322746  2111 net.cpp:165] Memory required for data: 116208384
I0822 23:34:21.322747  2111 layer_factory.hpp:77] Creating layer attention_reduction0
I0822 23:34:21.322755  2111 net.cpp:106] Creating Layer attention_reduction0
I0822 23:34:21.322757  2111 net.cpp:454] attention_reduction0 <- attention_permute0
I0822 23:34:21.322762  2111 net.cpp:411] attention_reduction0 -> attention_reduction0
I0822 23:34:21.322793  2111 net.cpp:150] Setting up attention_reduction0
I0822 23:34:21.322798  2111 net.cpp:157] Top shape: 64 1 8 (512)
I0822 23:34:21.322800  2111 net.cpp:165] Memory required for data: 116210432
I0822 23:34:21.322803  2111 layer_factory.hpp:77] Creating layer attention_permute1
I0822 23:34:21.322806  2111 net.cpp:106] Creating Layer attention_permute1
I0822 23:34:21.322808  2111 net.cpp:454] attention_permute1 <- attention_scale1_attention_scale_slice_1_split_0
I0822 23:34:21.322811  2111 net.cpp:411] attention_permute1 -> attention_permute1
I0822 23:34:21.322866  2111 net.cpp:150] Setting up attention_permute1
I0822 23:34:21.322872  2111 net.cpp:157] Top shape: 64 1 8 64 (32768)
I0822 23:34:21.322875  2111 net.cpp:165] Memory required for data: 116341504
I0822 23:34:21.322877  2111 layer_factory.hpp:77] Creating layer attention_reduction1
I0822 23:34:21.322881  2111 net.cpp:106] Creating Layer attention_reduction1
I0822 23:34:21.322883  2111 net.cpp:454] attention_reduction1 <- attention_permute1
I0822 23:34:21.322887  2111 net.cpp:411] attention_reduction1 -> attention_reduction1
I0822 23:34:21.322917  2111 net.cpp:150] Setting up attention_reduction1
I0822 23:34:21.322921  2111 net.cpp:157] Top shape: 64 1 8 (512)
I0822 23:34:21.322922  2111 net.cpp:165] Memory required for data: 116343552
I0822 23:34:21.322926  2111 layer_factory.hpp:77] Creating layer attention_permute2
I0822 23:34:21.322928  2111 net.cpp:106] Creating Layer attention_permute2
I0822 23:34:21.322932  2111 net.cpp:454] attention_permute2 <- attention_scale2_attention_scale_slice_2_split_0
I0822 23:34:21.322934  2111 net.cpp:411] attention_permute2 -> attention_permute2
I0822 23:34:21.322988  2111 net.cpp:150] Setting up attention_permute2
I0822 23:34:21.322993  2111 net.cpp:157] Top shape: 64 1 8 64 (32768)
I0822 23:34:21.322996  2111 net.cpp:165] Memory required for data: 116474624
I0822 23:34:21.322999  2111 layer_factory.hpp:77] Creating layer attention_reduction2
I0822 23:34:21.323002  2111 net.cpp:106] Creating Layer attention_reduction2
I0822 23:34:21.323004  2111 net.cpp:454] attention_reduction2 <- attention_permute2
I0822 23:34:21.323007  2111 net.cpp:411] attention_reduction2 -> attention_reduction2
I0822 23:34:21.323035  2111 net.cpp:150] Setting up attention_reduction2
I0822 23:34:21.323040  2111 net.cpp:157] Top shape: 64 1 8 (512)
I0822 23:34:21.323041  2111 net.cpp:165] Memory required for data: 116476672
I0822 23:34:21.323043  2111 layer_factory.hpp:77] Creating layer attention_permute3
I0822 23:34:21.323047  2111 net.cpp:106] Creating Layer attention_permute3
I0822 23:34:21.323050  2111 net.cpp:454] attention_permute3 <- attention_scale3_attention_scale_slice_3_split_0
I0822 23:34:21.323053  2111 net.cpp:411] attention_permute3 -> attention_permute3
I0822 23:34:21.323107  2111 net.cpp:150] Setting up attention_permute3
I0822 23:34:21.323110  2111 net.cpp:157] Top shape: 64 1 8 64 (32768)
I0822 23:34:21.323112  2111 net.cpp:165] Memory required for data: 116607744
I0822 23:34:21.323114  2111 layer_factory.hpp:77] Creating layer attention_reduction3
I0822 23:34:21.323124  2111 net.cpp:106] Creating Layer attention_reduction3
I0822 23:34:21.323127  2111 net.cpp:454] attention_reduction3 <- attention_permute3
I0822 23:34:21.323132  2111 net.cpp:411] attention_reduction3 -> attention_reduction3
I0822 23:34:21.323160  2111 net.cpp:150] Setting up attention_reduction3
I0822 23:34:21.323164  2111 net.cpp:157] Top shape: 64 1 8 (512)
I0822 23:34:21.323166  2111 net.cpp:165] Memory required for data: 116609792
I0822 23:34:21.323168  2111 layer_factory.hpp:77] Creating layer attention_permute4
I0822 23:34:21.323173  2111 net.cpp:106] Creating Layer attention_permute4
I0822 23:34:21.323174  2111 net.cpp:454] attention_permute4 <- attention_scale4_attention_scale_slice_4_split_0
I0822 23:34:21.323179  2111 net.cpp:411] attention_permute4 -> attention_permute4
I0822 23:34:21.323230  2111 net.cpp:150] Setting up attention_permute4
I0822 23:34:21.323235  2111 net.cpp:157] Top shape: 64 1 8 64 (32768)
I0822 23:34:21.323236  2111 net.cpp:165] Memory required for data: 116740864
I0822 23:34:21.323240  2111 layer_factory.hpp:77] Creating layer attention_reduction4
I0822 23:34:21.323242  2111 net.cpp:106] Creating Layer attention_reduction4
I0822 23:34:21.323247  2111 net.cpp:454] attention_reduction4 <- attention_permute4
I0822 23:34:21.323251  2111 net.cpp:411] attention_reduction4 -> attention_reduction4
I0822 23:34:21.323279  2111 net.cpp:150] Setting up attention_reduction4
I0822 23:34:21.323283  2111 net.cpp:157] Top shape: 64 1 8 (512)
I0822 23:34:21.323285  2111 net.cpp:165] Memory required for data: 116742912
I0822 23:34:21.323287  2111 layer_factory.hpp:77] Creating layer attention_permute5
I0822 23:34:21.323292  2111 net.cpp:106] Creating Layer attention_permute5
I0822 23:34:21.323293  2111 net.cpp:454] attention_permute5 <- attention_scale5_attention_scale_slice_5_split_0
I0822 23:34:21.323297  2111 net.cpp:411] attention_permute5 -> attention_permute5
I0822 23:34:21.323350  2111 net.cpp:150] Setting up attention_permute5
I0822 23:34:21.323354  2111 net.cpp:157] Top shape: 64 1 8 64 (32768)
I0822 23:34:21.323356  2111 net.cpp:165] Memory required for data: 116873984
I0822 23:34:21.323359  2111 layer_factory.hpp:77] Creating layer attention_reduction5
I0822 23:34:21.323362  2111 net.cpp:106] Creating Layer attention_reduction5
I0822 23:34:21.323364  2111 net.cpp:454] attention_reduction5 <- attention_permute5
I0822 23:34:21.323369  2111 net.cpp:411] attention_reduction5 -> attention_reduction5
I0822 23:34:21.323398  2111 net.cpp:150] Setting up attention_reduction5
I0822 23:34:21.323402  2111 net.cpp:157] Top shape: 64 1 8 (512)
I0822 23:34:21.323405  2111 net.cpp:165] Memory required for data: 116876032
I0822 23:34:21.323407  2111 layer_factory.hpp:77] Creating layer attention_permute6
I0822 23:34:21.323410  2111 net.cpp:106] Creating Layer attention_permute6
I0822 23:34:21.323413  2111 net.cpp:454] attention_permute6 <- attention_scale6_attention_scale_slice_6_split_0
I0822 23:34:21.323416  2111 net.cpp:411] attention_permute6 -> attention_permute6
I0822 23:34:21.323469  2111 net.cpp:150] Setting up attention_permute6
I0822 23:34:21.323473  2111 net.cpp:157] Top shape: 64 1 8 64 (32768)
I0822 23:34:21.323475  2111 net.cpp:165] Memory required for data: 117007104
I0822 23:34:21.323477  2111 layer_factory.hpp:77] Creating layer attention_reduction6
I0822 23:34:21.323482  2111 net.cpp:106] Creating Layer attention_reduction6
I0822 23:34:21.323483  2111 net.cpp:454] attention_reduction6 <- attention_permute6
I0822 23:34:21.323489  2111 net.cpp:411] attention_reduction6 -> attention_reduction6
I0822 23:34:21.323518  2111 net.cpp:150] Setting up attention_reduction6
I0822 23:34:21.323521  2111 net.cpp:157] Top shape: 64 1 8 (512)
I0822 23:34:21.323523  2111 net.cpp:165] Memory required for data: 117009152
I0822 23:34:21.323525  2111 layer_factory.hpp:77] Creating layer attention_permute7
I0822 23:34:21.323529  2111 net.cpp:106] Creating Layer attention_permute7
I0822 23:34:21.323531  2111 net.cpp:454] attention_permute7 <- attention_scale7_attention_scale_slice_7_split_0
I0822 23:34:21.323535  2111 net.cpp:411] attention_permute7 -> attention_permute7
I0822 23:34:21.323590  2111 net.cpp:150] Setting up attention_permute7
I0822 23:34:21.323592  2111 net.cpp:157] Top shape: 64 1 8 64 (32768)
I0822 23:34:21.323596  2111 net.cpp:165] Memory required for data: 117140224
I0822 23:34:21.323597  2111 layer_factory.hpp:77] Creating layer attention_reduction7
I0822 23:34:21.323601  2111 net.cpp:106] Creating Layer attention_reduction7
I0822 23:34:21.323603  2111 net.cpp:454] attention_reduction7 <- attention_permute7
I0822 23:34:21.323606  2111 net.cpp:411] attention_reduction7 -> attention_reduction7
I0822 23:34:21.323637  2111 net.cpp:150] Setting up attention_reduction7
I0822 23:34:21.323640  2111 net.cpp:157] Top shape: 64 1 8 (512)
I0822 23:34:21.323642  2111 net.cpp:165] Memory required for data: 117142272
I0822 23:34:21.323645  2111 layer_factory.hpp:77] Creating layer attention_scale_concat
I0822 23:34:21.323649  2111 net.cpp:106] Creating Layer attention_scale_concat
I0822 23:34:21.323652  2111 net.cpp:454] attention_scale_concat <- attention_reduction0
I0822 23:34:21.323655  2111 net.cpp:454] attention_scale_concat <- attention_reduction1
I0822 23:34:21.323658  2111 net.cpp:454] attention_scale_concat <- attention_reduction2
I0822 23:34:21.323660  2111 net.cpp:454] attention_scale_concat <- attention_reduction3
I0822 23:34:21.323663  2111 net.cpp:454] attention_scale_concat <- attention_reduction4
I0822 23:34:21.323667  2111 net.cpp:454] attention_scale_concat <- attention_reduction5
I0822 23:34:21.323668  2111 net.cpp:454] attention_scale_concat <- attention_reduction6
I0822 23:34:21.323670  2111 net.cpp:454] attention_scale_concat <- attention_reduction7
I0822 23:34:21.323674  2111 net.cpp:411] attention_scale_concat -> attention_scale_concat
I0822 23:34:21.323688  2111 net.cpp:150] Setting up attention_scale_concat
I0822 23:34:21.323691  2111 net.cpp:157] Top shape: 64 8 8 (4096)
I0822 23:34:21.323693  2111 net.cpp:165] Memory required for data: 117158656
I0822 23:34:21.323695  2111 layer_factory.hpp:77] Creating layer attention_height_slice
I0822 23:34:21.323700  2111 net.cpp:106] Creating Layer attention_height_slice
I0822 23:34:21.323704  2111 net.cpp:454] attention_height_slice <- attention_scale_concat
I0822 23:34:21.323707  2111 net.cpp:411] attention_height_slice -> attention_height0
I0822 23:34:21.323711  2111 net.cpp:411] attention_height_slice -> attention_height1
I0822 23:34:21.323715  2111 net.cpp:411] attention_height_slice -> attention_height2
I0822 23:34:21.323719  2111 net.cpp:411] attention_height_slice -> attention_height3
I0822 23:34:21.323724  2111 net.cpp:411] attention_height_slice -> attention_height4
I0822 23:34:21.323727  2111 net.cpp:411] attention_height_slice -> attention_height5
I0822 23:34:21.323732  2111 net.cpp:411] attention_height_slice -> attention_height6
I0822 23:34:21.323736  2111 net.cpp:411] attention_height_slice -> attention_height7
I0822 23:34:21.323798  2111 net.cpp:150] Setting up attention_height_slice
I0822 23:34:21.323802  2111 net.cpp:157] Top shape: 64 8 1 (512)
I0822 23:34:21.323806  2111 net.cpp:157] Top shape: 64 8 1 (512)
I0822 23:34:21.323807  2111 net.cpp:157] Top shape: 64 8 1 (512)
I0822 23:34:21.323810  2111 net.cpp:157] Top shape: 64 8 1 (512)
I0822 23:34:21.323812  2111 net.cpp:157] Top shape: 64 8 1 (512)
I0822 23:34:21.323815  2111 net.cpp:157] Top shape: 64 8 1 (512)
I0822 23:34:21.323818  2111 net.cpp:157] Top shape: 64 8 1 (512)
I0822 23:34:21.323820  2111 net.cpp:157] Top shape: 64 8 1 (512)
I0822 23:34:21.323822  2111 net.cpp:165] Memory required for data: 117175040
I0822 23:34:21.323824  2111 layer_factory.hpp:77] Creating layer attention_height_fc1_0
I0822 23:34:21.323829  2111 net.cpp:106] Creating Layer attention_height_fc1_0
I0822 23:34:21.323832  2111 net.cpp:454] attention_height_fc1_0 <- attention_height0
I0822 23:34:21.323835  2111 net.cpp:411] attention_height_fc1_0 -> attention_height_fc1_0
I0822 23:34:21.323913  2111 net.cpp:150] Setting up attention_height_fc1_0
I0822 23:34:21.323917  2111 net.cpp:157] Top shape: 64 64 (4096)
I0822 23:34:21.323920  2111 net.cpp:165] Memory required for data: 117191424
I0822 23:34:21.323925  2111 layer_factory.hpp:77] Creating layer attention_height_fc2_0
I0822 23:34:21.323928  2111 net.cpp:106] Creating Layer attention_height_fc2_0
I0822 23:34:21.323930  2111 net.cpp:454] attention_height_fc2_0 <- attention_height_fc1_0
I0822 23:34:21.323935  2111 net.cpp:411] attention_height_fc2_0 -> attention_height_fc2_0
I0822 23:34:21.324050  2111 net.cpp:150] Setting up attention_height_fc2_0
I0822 23:34:21.324055  2111 net.cpp:157] Top shape: 64 32 (2048)
I0822 23:34:21.324057  2111 net.cpp:165] Memory required for data: 117199616
I0822 23:34:21.324061  2111 layer_factory.hpp:77] Creating layer attention_height_fc3_0
I0822 23:34:21.324065  2111 net.cpp:106] Creating Layer attention_height_fc3_0
I0822 23:34:21.324069  2111 net.cpp:454] attention_height_fc3_0 <- attention_height_fc2_0
I0822 23:34:21.324071  2111 net.cpp:411] attention_height_fc3_0 -> attention_height_fc3_0
I0822 23:34:21.324141  2111 net.cpp:150] Setting up attention_height_fc3_0
I0822 23:34:21.324146  2111 net.cpp:157] Top shape: 64 8 (512)
I0822 23:34:21.324147  2111 net.cpp:165] Memory required for data: 117201664
I0822 23:34:21.324151  2111 layer_factory.hpp:77] Creating layer attention_height_reshape0
I0822 23:34:21.324156  2111 net.cpp:106] Creating Layer attention_height_reshape0
I0822 23:34:21.324158  2111 net.cpp:454] attention_height_reshape0 <- attention_height_fc3_0
I0822 23:34:21.324162  2111 net.cpp:411] attention_height_reshape0 -> attention_height_reshape0
I0822 23:34:21.324182  2111 net.cpp:150] Setting up attention_height_reshape0
I0822 23:34:21.324187  2111 net.cpp:157] Top shape: 64 8 1 1 (512)
I0822 23:34:21.324188  2111 net.cpp:165] Memory required for data: 117203712
I0822 23:34:21.324190  2111 layer_factory.hpp:77] Creating layer attention_height_fc1_1
I0822 23:34:21.324194  2111 net.cpp:106] Creating Layer attention_height_fc1_1
I0822 23:34:21.324198  2111 net.cpp:454] attention_height_fc1_1 <- attention_height1
I0822 23:34:21.324201  2111 net.cpp:411] attention_height_fc1_1 -> attention_height_fc1_1
I0822 23:34:21.324278  2111 net.cpp:150] Setting up attention_height_fc1_1
I0822 23:34:21.324283  2111 net.cpp:157] Top shape: 64 64 (4096)
I0822 23:34:21.324285  2111 net.cpp:165] Memory required for data: 117220096
I0822 23:34:21.324288  2111 layer_factory.hpp:77] Creating layer attention_height_fc2_1
I0822 23:34:21.324292  2111 net.cpp:106] Creating Layer attention_height_fc2_1
I0822 23:34:21.324295  2111 net.cpp:454] attention_height_fc2_1 <- attention_height_fc1_1
I0822 23:34:21.324299  2111 net.cpp:411] attention_height_fc2_1 -> attention_height_fc2_1
I0822 23:34:21.324409  2111 net.cpp:150] Setting up attention_height_fc2_1
I0822 23:34:21.324414  2111 net.cpp:157] Top shape: 64 32 (2048)
I0822 23:34:21.324415  2111 net.cpp:165] Memory required for data: 117228288
I0822 23:34:21.324419  2111 layer_factory.hpp:77] Creating layer attention_height_fc3_1
I0822 23:34:21.324422  2111 net.cpp:106] Creating Layer attention_height_fc3_1
I0822 23:34:21.324425  2111 net.cpp:454] attention_height_fc3_1 <- attention_height_fc2_1
I0822 23:34:21.324429  2111 net.cpp:411] attention_height_fc3_1 -> attention_height_fc3_1
I0822 23:34:21.324494  2111 net.cpp:150] Setting up attention_height_fc3_1
I0822 23:34:21.324498  2111 net.cpp:157] Top shape: 64 8 (512)
I0822 23:34:21.324501  2111 net.cpp:165] Memory required for data: 117230336
I0822 23:34:21.324504  2111 layer_factory.hpp:77] Creating layer attention_height_reshape1
I0822 23:34:21.324508  2111 net.cpp:106] Creating Layer attention_height_reshape1
I0822 23:34:21.324510  2111 net.cpp:454] attention_height_reshape1 <- attention_height_fc3_1
I0822 23:34:21.324514  2111 net.cpp:411] attention_height_reshape1 -> attention_height_reshape1
I0822 23:34:21.324528  2111 net.cpp:150] Setting up attention_height_reshape1
I0822 23:34:21.324532  2111 net.cpp:157] Top shape: 64 8 1 1 (512)
I0822 23:34:21.324534  2111 net.cpp:165] Memory required for data: 117232384
I0822 23:34:21.324537  2111 layer_factory.hpp:77] Creating layer attention_height_fc1_2
I0822 23:34:21.324542  2111 net.cpp:106] Creating Layer attention_height_fc1_2
I0822 23:34:21.324543  2111 net.cpp:454] attention_height_fc1_2 <- attention_height2
I0822 23:34:21.324548  2111 net.cpp:411] attention_height_fc1_2 -> attention_height_fc1_2
I0822 23:34:21.324621  2111 net.cpp:150] Setting up attention_height_fc1_2
I0822 23:34:21.324625  2111 net.cpp:157] Top shape: 64 64 (4096)
I0822 23:34:21.324627  2111 net.cpp:165] Memory required for data: 117248768
I0822 23:34:21.324631  2111 layer_factory.hpp:77] Creating layer attention_height_fc2_2
I0822 23:34:21.324635  2111 net.cpp:106] Creating Layer attention_height_fc2_2
I0822 23:34:21.324637  2111 net.cpp:454] attention_height_fc2_2 <- attention_height_fc1_2
I0822 23:34:21.324641  2111 net.cpp:411] attention_height_fc2_2 -> attention_height_fc2_2
I0822 23:34:21.324753  2111 net.cpp:150] Setting up attention_height_fc2_2
I0822 23:34:21.324756  2111 net.cpp:157] Top shape: 64 32 (2048)
I0822 23:34:21.324759  2111 net.cpp:165] Memory required for data: 117256960
I0822 23:34:21.324767  2111 layer_factory.hpp:77] Creating layer attention_height_fc3_2
I0822 23:34:21.324771  2111 net.cpp:106] Creating Layer attention_height_fc3_2
I0822 23:34:21.324774  2111 net.cpp:454] attention_height_fc3_2 <- attention_height_fc2_2
I0822 23:34:21.324779  2111 net.cpp:411] attention_height_fc3_2 -> attention_height_fc3_2
I0822 23:34:21.324847  2111 net.cpp:150] Setting up attention_height_fc3_2
I0822 23:34:21.324851  2111 net.cpp:157] Top shape: 64 8 (512)
I0822 23:34:21.324853  2111 net.cpp:165] Memory required for data: 117259008
I0822 23:34:21.324857  2111 layer_factory.hpp:77] Creating layer attention_height_reshape2
I0822 23:34:21.324862  2111 net.cpp:106] Creating Layer attention_height_reshape2
I0822 23:34:21.324864  2111 net.cpp:454] attention_height_reshape2 <- attention_height_fc3_2
I0822 23:34:21.324867  2111 net.cpp:411] attention_height_reshape2 -> attention_height_reshape2
I0822 23:34:21.324882  2111 net.cpp:150] Setting up attention_height_reshape2
I0822 23:34:21.324887  2111 net.cpp:157] Top shape: 64 8 1 1 (512)
I0822 23:34:21.324888  2111 net.cpp:165] Memory required for data: 117261056
I0822 23:34:21.324890  2111 layer_factory.hpp:77] Creating layer attention_height_fc1_3
I0822 23:34:21.324894  2111 net.cpp:106] Creating Layer attention_height_fc1_3
I0822 23:34:21.324896  2111 net.cpp:454] attention_height_fc1_3 <- attention_height3
I0822 23:34:21.324900  2111 net.cpp:411] attention_height_fc1_3 -> attention_height_fc1_3
I0822 23:34:21.324975  2111 net.cpp:150] Setting up attention_height_fc1_3
I0822 23:34:21.324978  2111 net.cpp:157] Top shape: 64 64 (4096)
I0822 23:34:21.324980  2111 net.cpp:165] Memory required for data: 117277440
I0822 23:34:21.324985  2111 layer_factory.hpp:77] Creating layer attention_height_fc2_3
I0822 23:34:21.324988  2111 net.cpp:106] Creating Layer attention_height_fc2_3
I0822 23:34:21.324990  2111 net.cpp:454] attention_height_fc2_3 <- attention_height_fc1_3
I0822 23:34:21.324995  2111 net.cpp:411] attention_height_fc2_3 -> attention_height_fc2_3
I0822 23:34:21.325104  2111 net.cpp:150] Setting up attention_height_fc2_3
I0822 23:34:21.325109  2111 net.cpp:157] Top shape: 64 32 (2048)
I0822 23:34:21.325110  2111 net.cpp:165] Memory required for data: 117285632
I0822 23:34:21.325114  2111 layer_factory.hpp:77] Creating layer attention_height_fc3_3
I0822 23:34:21.325117  2111 net.cpp:106] Creating Layer attention_height_fc3_3
I0822 23:34:21.325120  2111 net.cpp:454] attention_height_fc3_3 <- attention_height_fc2_3
I0822 23:34:21.325124  2111 net.cpp:411] attention_height_fc3_3 -> attention_height_fc3_3
I0822 23:34:21.325191  2111 net.cpp:150] Setting up attention_height_fc3_3
I0822 23:34:21.325196  2111 net.cpp:157] Top shape: 64 8 (512)
I0822 23:34:21.325198  2111 net.cpp:165] Memory required for data: 117287680
I0822 23:34:21.325202  2111 layer_factory.hpp:77] Creating layer attention_height_reshape3
I0822 23:34:21.325208  2111 net.cpp:106] Creating Layer attention_height_reshape3
I0822 23:34:21.325211  2111 net.cpp:454] attention_height_reshape3 <- attention_height_fc3_3
I0822 23:34:21.325214  2111 net.cpp:411] attention_height_reshape3 -> attention_height_reshape3
I0822 23:34:21.325229  2111 net.cpp:150] Setting up attention_height_reshape3
I0822 23:34:21.325233  2111 net.cpp:157] Top shape: 64 8 1 1 (512)
I0822 23:34:21.325235  2111 net.cpp:165] Memory required for data: 117289728
I0822 23:34:21.325238  2111 layer_factory.hpp:77] Creating layer attention_height_fc1_4
I0822 23:34:21.325242  2111 net.cpp:106] Creating Layer attention_height_fc1_4
I0822 23:34:21.325244  2111 net.cpp:454] attention_height_fc1_4 <- attention_height4
I0822 23:34:21.325248  2111 net.cpp:411] attention_height_fc1_4 -> attention_height_fc1_4
I0822 23:34:21.325320  2111 net.cpp:150] Setting up attention_height_fc1_4
I0822 23:34:21.325325  2111 net.cpp:157] Top shape: 64 64 (4096)
I0822 23:34:21.325326  2111 net.cpp:165] Memory required for data: 117306112
I0822 23:34:21.325330  2111 layer_factory.hpp:77] Creating layer attention_height_fc2_4
I0822 23:34:21.325335  2111 net.cpp:106] Creating Layer attention_height_fc2_4
I0822 23:34:21.325336  2111 net.cpp:454] attention_height_fc2_4 <- attention_height_fc1_4
I0822 23:34:21.325342  2111 net.cpp:411] attention_height_fc2_4 -> attention_height_fc2_4
I0822 23:34:21.325474  2111 net.cpp:150] Setting up attention_height_fc2_4
I0822 23:34:21.325479  2111 net.cpp:157] Top shape: 64 32 (2048)
I0822 23:34:21.325481  2111 net.cpp:165] Memory required for data: 117314304
I0822 23:34:21.325485  2111 layer_factory.hpp:77] Creating layer attention_height_fc3_4
I0822 23:34:21.325489  2111 net.cpp:106] Creating Layer attention_height_fc3_4
I0822 23:34:21.325491  2111 net.cpp:454] attention_height_fc3_4 <- attention_height_fc2_4
I0822 23:34:21.325495  2111 net.cpp:411] attention_height_fc3_4 -> attention_height_fc3_4
I0822 23:34:21.325559  2111 net.cpp:150] Setting up attention_height_fc3_4
I0822 23:34:21.325563  2111 net.cpp:157] Top shape: 64 8 (512)
I0822 23:34:21.325565  2111 net.cpp:165] Memory required for data: 117316352
I0822 23:34:21.325569  2111 layer_factory.hpp:77] Creating layer attention_height_reshape4
I0822 23:34:21.325573  2111 net.cpp:106] Creating Layer attention_height_reshape4
I0822 23:34:21.325577  2111 net.cpp:454] attention_height_reshape4 <- attention_height_fc3_4
I0822 23:34:21.325580  2111 net.cpp:411] attention_height_reshape4 -> attention_height_reshape4
I0822 23:34:21.325594  2111 net.cpp:150] Setting up attention_height_reshape4
I0822 23:34:21.325598  2111 net.cpp:157] Top shape: 64 8 1 1 (512)
I0822 23:34:21.325599  2111 net.cpp:165] Memory required for data: 117318400
I0822 23:34:21.325603  2111 layer_factory.hpp:77] Creating layer attention_height_fc1_5
I0822 23:34:21.325606  2111 net.cpp:106] Creating Layer attention_height_fc1_5
I0822 23:34:21.325608  2111 net.cpp:454] attention_height_fc1_5 <- attention_height5
I0822 23:34:21.325613  2111 net.cpp:411] attention_height_fc1_5 -> attention_height_fc1_5
I0822 23:34:21.325683  2111 net.cpp:150] Setting up attention_height_fc1_5
I0822 23:34:21.325687  2111 net.cpp:157] Top shape: 64 64 (4096)
I0822 23:34:21.325690  2111 net.cpp:165] Memory required for data: 117334784
I0822 23:34:21.325692  2111 layer_factory.hpp:77] Creating layer attention_height_fc2_5
I0822 23:34:21.325697  2111 net.cpp:106] Creating Layer attention_height_fc2_5
I0822 23:34:21.325700  2111 net.cpp:454] attention_height_fc2_5 <- attention_height_fc1_5
I0822 23:34:21.325703  2111 net.cpp:411] attention_height_fc2_5 -> attention_height_fc2_5
I0822 23:34:21.325812  2111 net.cpp:150] Setting up attention_height_fc2_5
I0822 23:34:21.325817  2111 net.cpp:157] Top shape: 64 32 (2048)
I0822 23:34:21.325819  2111 net.cpp:165] Memory required for data: 117342976
I0822 23:34:21.325822  2111 layer_factory.hpp:77] Creating layer attention_height_fc3_5
I0822 23:34:21.325827  2111 net.cpp:106] Creating Layer attention_height_fc3_5
I0822 23:34:21.325829  2111 net.cpp:454] attention_height_fc3_5 <- attention_height_fc2_5
I0822 23:34:21.325832  2111 net.cpp:411] attention_height_fc3_5 -> attention_height_fc3_5
I0822 23:34:21.325897  2111 net.cpp:150] Setting up attention_height_fc3_5
I0822 23:34:21.325901  2111 net.cpp:157] Top shape: 64 8 (512)
I0822 23:34:21.325903  2111 net.cpp:165] Memory required for data: 117345024
I0822 23:34:21.325907  2111 layer_factory.hpp:77] Creating layer attention_height_reshape5
I0822 23:34:21.325911  2111 net.cpp:106] Creating Layer attention_height_reshape5
I0822 23:34:21.325913  2111 net.cpp:454] attention_height_reshape5 <- attention_height_fc3_5
I0822 23:34:21.325917  2111 net.cpp:411] attention_height_reshape5 -> attention_height_reshape5
I0822 23:34:21.325932  2111 net.cpp:150] Setting up attention_height_reshape5
I0822 23:34:21.325935  2111 net.cpp:157] Top shape: 64 8 1 1 (512)
I0822 23:34:21.325937  2111 net.cpp:165] Memory required for data: 117347072
I0822 23:34:21.325939  2111 layer_factory.hpp:77] Creating layer attention_height_fc1_6
I0822 23:34:21.325943  2111 net.cpp:106] Creating Layer attention_height_fc1_6
I0822 23:34:21.325947  2111 net.cpp:454] attention_height_fc1_6 <- attention_height6
I0822 23:34:21.325949  2111 net.cpp:411] attention_height_fc1_6 -> attention_height_fc1_6
I0822 23:34:21.326030  2111 net.cpp:150] Setting up attention_height_fc1_6
I0822 23:34:21.326035  2111 net.cpp:157] Top shape: 64 64 (4096)
I0822 23:34:21.326037  2111 net.cpp:165] Memory required for data: 117363456
I0822 23:34:21.326040  2111 layer_factory.hpp:77] Creating layer attention_height_fc2_6
I0822 23:34:21.326045  2111 net.cpp:106] Creating Layer attention_height_fc2_6
I0822 23:34:21.326047  2111 net.cpp:454] attention_height_fc2_6 <- attention_height_fc1_6
I0822 23:34:21.326051  2111 net.cpp:411] attention_height_fc2_6 -> attention_height_fc2_6
I0822 23:34:21.326750  2111 net.cpp:150] Setting up attention_height_fc2_6
I0822 23:34:21.326755  2111 net.cpp:157] Top shape: 64 32 (2048)
I0822 23:34:21.326757  2111 net.cpp:165] Memory required for data: 117371648
I0822 23:34:21.326761  2111 layer_factory.hpp:77] Creating layer attention_height_fc3_6
I0822 23:34:21.326766  2111 net.cpp:106] Creating Layer attention_height_fc3_6
I0822 23:34:21.326768  2111 net.cpp:454] attention_height_fc3_6 <- attention_height_fc2_6
I0822 23:34:21.326774  2111 net.cpp:411] attention_height_fc3_6 -> attention_height_fc3_6
I0822 23:34:21.326840  2111 net.cpp:150] Setting up attention_height_fc3_6
I0822 23:34:21.326844  2111 net.cpp:157] Top shape: 64 8 (512)
I0822 23:34:21.326846  2111 net.cpp:165] Memory required for data: 117373696
I0822 23:34:21.326850  2111 layer_factory.hpp:77] Creating layer attention_height_reshape6
I0822 23:34:21.326854  2111 net.cpp:106] Creating Layer attention_height_reshape6
I0822 23:34:21.326856  2111 net.cpp:454] attention_height_reshape6 <- attention_height_fc3_6
I0822 23:34:21.326860  2111 net.cpp:411] attention_height_reshape6 -> attention_height_reshape6
I0822 23:34:21.326874  2111 net.cpp:150] Setting up attention_height_reshape6
I0822 23:34:21.326879  2111 net.cpp:157] Top shape: 64 8 1 1 (512)
I0822 23:34:21.326880  2111 net.cpp:165] Memory required for data: 117375744
I0822 23:34:21.326882  2111 layer_factory.hpp:77] Creating layer attention_height_fc1_7
I0822 23:34:21.326887  2111 net.cpp:106] Creating Layer attention_height_fc1_7
I0822 23:34:21.326889  2111 net.cpp:454] attention_height_fc1_7 <- attention_height7
I0822 23:34:21.326895  2111 net.cpp:411] attention_height_fc1_7 -> attention_height_fc1_7
I0822 23:34:21.326967  2111 net.cpp:150] Setting up attention_height_fc1_7
I0822 23:34:21.326972  2111 net.cpp:157] Top shape: 64 64 (4096)
I0822 23:34:21.326973  2111 net.cpp:165] Memory required for data: 117392128
I0822 23:34:21.326977  2111 layer_factory.hpp:77] Creating layer attention_height_fc2_7
I0822 23:34:21.326982  2111 net.cpp:106] Creating Layer attention_height_fc2_7
I0822 23:34:21.326984  2111 net.cpp:454] attention_height_fc2_7 <- attention_height_fc1_7
I0822 23:34:21.326987  2111 net.cpp:411] attention_height_fc2_7 -> attention_height_fc2_7
I0822 23:34:21.327098  2111 net.cpp:150] Setting up attention_height_fc2_7
I0822 23:34:21.327102  2111 net.cpp:157] Top shape: 64 32 (2048)
I0822 23:34:21.327105  2111 net.cpp:165] Memory required for data: 117400320
I0822 23:34:21.327108  2111 layer_factory.hpp:77] Creating layer attention_height_fc3_7
I0822 23:34:21.327112  2111 net.cpp:106] Creating Layer attention_height_fc3_7
I0822 23:34:21.327114  2111 net.cpp:454] attention_height_fc3_7 <- attention_height_fc2_7
I0822 23:34:21.327118  2111 net.cpp:411] attention_height_fc3_7 -> attention_height_fc3_7
I0822 23:34:21.327183  2111 net.cpp:150] Setting up attention_height_fc3_7
I0822 23:34:21.327188  2111 net.cpp:157] Top shape: 64 8 (512)
I0822 23:34:21.327189  2111 net.cpp:165] Memory required for data: 117402368
I0822 23:34:21.327193  2111 layer_factory.hpp:77] Creating layer attention_height_reshape7
I0822 23:34:21.327198  2111 net.cpp:106] Creating Layer attention_height_reshape7
I0822 23:34:21.327199  2111 net.cpp:454] attention_height_reshape7 <- attention_height_fc3_7
I0822 23:34:21.327203  2111 net.cpp:411] attention_height_reshape7 -> attention_height_reshape7
I0822 23:34:21.327217  2111 net.cpp:150] Setting up attention_height_reshape7
I0822 23:34:21.327221  2111 net.cpp:157] Top shape: 64 8 1 1 (512)
I0822 23:34:21.327224  2111 net.cpp:165] Memory required for data: 117404416
I0822 23:34:21.327225  2111 layer_factory.hpp:77] Creating layer attention_height_concat
I0822 23:34:21.327229  2111 net.cpp:106] Creating Layer attention_height_concat
I0822 23:34:21.327232  2111 net.cpp:454] attention_height_concat <- attention_height_reshape0
I0822 23:34:21.327235  2111 net.cpp:454] attention_height_concat <- attention_height_reshape1
I0822 23:34:21.327237  2111 net.cpp:454] attention_height_concat <- attention_height_reshape2
I0822 23:34:21.327240  2111 net.cpp:454] attention_height_concat <- attention_height_reshape3
I0822 23:34:21.327244  2111 net.cpp:454] attention_height_concat <- attention_height_reshape4
I0822 23:34:21.327245  2111 net.cpp:454] attention_height_concat <- attention_height_reshape5
I0822 23:34:21.327248  2111 net.cpp:454] attention_height_concat <- attention_height_reshape6
I0822 23:34:21.327250  2111 net.cpp:454] attention_height_concat <- attention_height_reshape7
I0822 23:34:21.327253  2111 net.cpp:411] attention_height_concat -> attention_height_concat
I0822 23:34:21.327277  2111 net.cpp:150] Setting up attention_height_concat
I0822 23:34:21.327282  2111 net.cpp:157] Top shape: 64 8 8 1 (4096)
I0822 23:34:21.327284  2111 net.cpp:165] Memory required for data: 117420800
I0822 23:34:21.327286  2111 layer_factory.hpp:77] Creating layer attention_weight
I0822 23:34:21.327291  2111 net.cpp:106] Creating Layer attention_weight
I0822 23:34:21.327293  2111 net.cpp:454] attention_weight <- attention_height_concat
I0822 23:34:21.327297  2111 net.cpp:411] attention_weight -> attention_weight
I0822 23:34:21.327339  2111 net.cpp:150] Setting up attention_weight
I0822 23:34:21.327343  2111 net.cpp:157] Top shape: 64 8 8 1 (4096)
I0822 23:34:21.327347  2111 net.cpp:165] Memory required for data: 117437184
I0822 23:34:21.327348  2111 layer_factory.hpp:77] Creating layer attention_weight_slice
I0822 23:34:21.327353  2111 net.cpp:106] Creating Layer attention_weight_slice
I0822 23:34:21.327356  2111 net.cpp:454] attention_weight_slice <- attention_weight
I0822 23:34:21.327360  2111 net.cpp:411] attention_weight_slice -> attention_weight_slice0
I0822 23:34:21.327364  2111 net.cpp:411] attention_weight_slice -> attention_weight_slice1
I0822 23:34:21.327369  2111 net.cpp:411] attention_weight_slice -> attention_weight_slice2
I0822 23:34:21.327373  2111 net.cpp:411] attention_weight_slice -> attention_weight_slice3
I0822 23:34:21.327379  2111 net.cpp:411] attention_weight_slice -> attention_weight_slice4
I0822 23:34:21.327383  2111 net.cpp:411] attention_weight_slice -> attention_weight_slice5
I0822 23:34:21.327388  2111 net.cpp:411] attention_weight_slice -> attention_weight_slice6
I0822 23:34:21.327391  2111 net.cpp:411] attention_weight_slice -> attention_weight_slice7
I0822 23:34:21.327453  2111 net.cpp:150] Setting up attention_weight_slice
I0822 23:34:21.327457  2111 net.cpp:157] Top shape: 64 1 8 1 (512)
I0822 23:34:21.327461  2111 net.cpp:157] Top shape: 64 1 8 1 (512)
I0822 23:34:21.327462  2111 net.cpp:157] Top shape: 64 1 8 1 (512)
I0822 23:34:21.327466  2111 net.cpp:157] Top shape: 64 1 8 1 (512)
I0822 23:34:21.327468  2111 net.cpp:157] Top shape: 64 1 8 1 (512)
I0822 23:34:21.327471  2111 net.cpp:157] Top shape: 64 1 8 1 (512)
I0822 23:34:21.327473  2111 net.cpp:157] Top shape: 64 1 8 1 (512)
I0822 23:34:21.327476  2111 net.cpp:157] Top shape: 64 1 8 1 (512)
I0822 23:34:21.327478  2111 net.cpp:165] Memory required for data: 117453568
I0822 23:34:21.327481  2111 layer_factory.hpp:77] Creating layer attention_weight_tile0
I0822 23:34:21.327486  2111 net.cpp:106] Creating Layer attention_weight_tile0
I0822 23:34:21.327487  2111 net.cpp:454] attention_weight_tile0 <- attention_weight_slice0
I0822 23:34:21.327491  2111 net.cpp:411] attention_weight_tile0 -> attention_weight_tile0
I0822 23:34:21.327509  2111 net.cpp:150] Setting up attention_weight_tile0
I0822 23:34:21.327513  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.327515  2111 net.cpp:165] Memory required for data: 117584640
I0822 23:34:21.327518  2111 layer_factory.hpp:77] Creating layer attention_reweight0
I0822 23:34:21.327522  2111 net.cpp:106] Creating Layer attention_reweight0
I0822 23:34:21.327525  2111 net.cpp:454] attention_reweight0 <- attention_scale0_attention_scale_slice_0_split_1
I0822 23:34:21.327528  2111 net.cpp:454] attention_reweight0 <- attention_weight_tile0
I0822 23:34:21.327533  2111 net.cpp:411] attention_reweight0 -> attention_reweight0
I0822 23:34:21.327548  2111 net.cpp:150] Setting up attention_reweight0
I0822 23:34:21.327551  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.327553  2111 net.cpp:165] Memory required for data: 117715712
I0822 23:34:21.327556  2111 layer_factory.hpp:77] Creating layer attention_weight_tile1
I0822 23:34:21.327559  2111 net.cpp:106] Creating Layer attention_weight_tile1
I0822 23:34:21.327563  2111 net.cpp:454] attention_weight_tile1 <- attention_weight_slice1
I0822 23:34:21.327565  2111 net.cpp:411] attention_weight_tile1 -> attention_weight_tile1
I0822 23:34:21.327579  2111 net.cpp:150] Setting up attention_weight_tile1
I0822 23:34:21.327581  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.327584  2111 net.cpp:165] Memory required for data: 117846784
I0822 23:34:21.327585  2111 layer_factory.hpp:77] Creating layer attention_reweight1
I0822 23:34:21.327589  2111 net.cpp:106] Creating Layer attention_reweight1
I0822 23:34:21.327592  2111 net.cpp:454] attention_reweight1 <- attention_scale1_attention_scale_slice_1_split_1
I0822 23:34:21.327595  2111 net.cpp:454] attention_reweight1 <- attention_weight_tile1
I0822 23:34:21.327599  2111 net.cpp:411] attention_reweight1 -> attention_reweight1
I0822 23:34:21.327611  2111 net.cpp:150] Setting up attention_reweight1
I0822 23:34:21.327615  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.327617  2111 net.cpp:165] Memory required for data: 117977856
I0822 23:34:21.327621  2111 layer_factory.hpp:77] Creating layer attention_weight_tile2
I0822 23:34:21.327625  2111 net.cpp:106] Creating Layer attention_weight_tile2
I0822 23:34:21.327628  2111 net.cpp:454] attention_weight_tile2 <- attention_weight_slice2
I0822 23:34:21.327631  2111 net.cpp:411] attention_weight_tile2 -> attention_weight_tile2
I0822 23:34:21.327644  2111 net.cpp:150] Setting up attention_weight_tile2
I0822 23:34:21.327648  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.327651  2111 net.cpp:165] Memory required for data: 118108928
I0822 23:34:21.327652  2111 layer_factory.hpp:77] Creating layer attention_reweight2
I0822 23:34:21.327656  2111 net.cpp:106] Creating Layer attention_reweight2
I0822 23:34:21.327658  2111 net.cpp:454] attention_reweight2 <- attention_scale2_attention_scale_slice_2_split_1
I0822 23:34:21.327661  2111 net.cpp:454] attention_reweight2 <- attention_weight_tile2
I0822 23:34:21.327666  2111 net.cpp:411] attention_reweight2 -> attention_reweight2
I0822 23:34:21.327677  2111 net.cpp:150] Setting up attention_reweight2
I0822 23:34:21.327682  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.327683  2111 net.cpp:165] Memory required for data: 118240000
I0822 23:34:21.327685  2111 layer_factory.hpp:77] Creating layer attention_weight_tile3
I0822 23:34:21.327689  2111 net.cpp:106] Creating Layer attention_weight_tile3
I0822 23:34:21.327692  2111 net.cpp:454] attention_weight_tile3 <- attention_weight_slice3
I0822 23:34:21.327694  2111 net.cpp:411] attention_weight_tile3 -> attention_weight_tile3
I0822 23:34:21.327706  2111 net.cpp:150] Setting up attention_weight_tile3
I0822 23:34:21.327710  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.327713  2111 net.cpp:165] Memory required for data: 118371072
I0822 23:34:21.327714  2111 layer_factory.hpp:77] Creating layer attention_reweight3
I0822 23:34:21.327718  2111 net.cpp:106] Creating Layer attention_reweight3
I0822 23:34:21.327721  2111 net.cpp:454] attention_reweight3 <- attention_scale3_attention_scale_slice_3_split_1
I0822 23:34:21.327724  2111 net.cpp:454] attention_reweight3 <- attention_weight_tile3
I0822 23:34:21.327728  2111 net.cpp:411] attention_reweight3 -> attention_reweight3
I0822 23:34:21.327742  2111 net.cpp:150] Setting up attention_reweight3
I0822 23:34:21.327745  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.327749  2111 net.cpp:165] Memory required for data: 118502144
I0822 23:34:21.327750  2111 layer_factory.hpp:77] Creating layer attention_weight_tile4
I0822 23:34:21.327754  2111 net.cpp:106] Creating Layer attention_weight_tile4
I0822 23:34:21.327756  2111 net.cpp:454] attention_weight_tile4 <- attention_weight_slice4
I0822 23:34:21.327760  2111 net.cpp:411] attention_weight_tile4 -> attention_weight_tile4
I0822 23:34:21.327772  2111 net.cpp:150] Setting up attention_weight_tile4
I0822 23:34:21.327776  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.327778  2111 net.cpp:165] Memory required for data: 118633216
I0822 23:34:21.327780  2111 layer_factory.hpp:77] Creating layer attention_reweight4
I0822 23:34:21.327783  2111 net.cpp:106] Creating Layer attention_reweight4
I0822 23:34:21.327786  2111 net.cpp:454] attention_reweight4 <- attention_scale4_attention_scale_slice_4_split_1
I0822 23:34:21.327790  2111 net.cpp:454] attention_reweight4 <- attention_weight_tile4
I0822 23:34:21.327792  2111 net.cpp:411] attention_reweight4 -> attention_reweight4
I0822 23:34:21.327805  2111 net.cpp:150] Setting up attention_reweight4
I0822 23:34:21.327808  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.327811  2111 net.cpp:165] Memory required for data: 118764288
I0822 23:34:21.327812  2111 layer_factory.hpp:77] Creating layer attention_weight_tile5
I0822 23:34:21.327816  2111 net.cpp:106] Creating Layer attention_weight_tile5
I0822 23:34:21.327817  2111 net.cpp:454] attention_weight_tile5 <- attention_weight_slice5
I0822 23:34:21.327821  2111 net.cpp:411] attention_weight_tile5 -> attention_weight_tile5
I0822 23:34:21.327832  2111 net.cpp:150] Setting up attention_weight_tile5
I0822 23:34:21.327836  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.327838  2111 net.cpp:165] Memory required for data: 118895360
I0822 23:34:21.327841  2111 layer_factory.hpp:77] Creating layer attention_reweight5
I0822 23:34:21.327843  2111 net.cpp:106] Creating Layer attention_reweight5
I0822 23:34:21.327847  2111 net.cpp:454] attention_reweight5 <- attention_scale5_attention_scale_slice_5_split_1
I0822 23:34:21.327849  2111 net.cpp:454] attention_reweight5 <- attention_weight_tile5
I0822 23:34:21.327852  2111 net.cpp:411] attention_reweight5 -> attention_reweight5
I0822 23:34:21.327867  2111 net.cpp:150] Setting up attention_reweight5
I0822 23:34:21.327870  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.327872  2111 net.cpp:165] Memory required for data: 119026432
I0822 23:34:21.327875  2111 layer_factory.hpp:77] Creating layer attention_weight_tile6
I0822 23:34:21.327878  2111 net.cpp:106] Creating Layer attention_weight_tile6
I0822 23:34:21.327881  2111 net.cpp:454] attention_weight_tile6 <- attention_weight_slice6
I0822 23:34:21.327884  2111 net.cpp:411] attention_weight_tile6 -> attention_weight_tile6
I0822 23:34:21.327896  2111 net.cpp:150] Setting up attention_weight_tile6
I0822 23:34:21.327900  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.327903  2111 net.cpp:165] Memory required for data: 119157504
I0822 23:34:21.327904  2111 layer_factory.hpp:77] Creating layer attention_reweight6
I0822 23:34:21.327908  2111 net.cpp:106] Creating Layer attention_reweight6
I0822 23:34:21.327910  2111 net.cpp:454] attention_reweight6 <- attention_scale6_attention_scale_slice_6_split_1
I0822 23:34:21.327913  2111 net.cpp:454] attention_reweight6 <- attention_weight_tile6
I0822 23:34:21.327916  2111 net.cpp:411] attention_reweight6 -> attention_reweight6
I0822 23:34:21.327929  2111 net.cpp:150] Setting up attention_reweight6
I0822 23:34:21.327932  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.327934  2111 net.cpp:165] Memory required for data: 119288576
I0822 23:34:21.327936  2111 layer_factory.hpp:77] Creating layer attention_weight_tile7
I0822 23:34:21.327940  2111 net.cpp:106] Creating Layer attention_weight_tile7
I0822 23:34:21.327942  2111 net.cpp:454] attention_weight_tile7 <- attention_weight_slice7
I0822 23:34:21.327945  2111 net.cpp:411] attention_weight_tile7 -> attention_weight_tile7
I0822 23:34:21.327957  2111 net.cpp:150] Setting up attention_weight_tile7
I0822 23:34:21.327961  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.327963  2111 net.cpp:165] Memory required for data: 119419648
I0822 23:34:21.327965  2111 layer_factory.hpp:77] Creating layer attention_reweight7
I0822 23:34:21.327970  2111 net.cpp:106] Creating Layer attention_reweight7
I0822 23:34:21.327971  2111 net.cpp:454] attention_reweight7 <- attention_scale7_attention_scale_slice_7_split_1
I0822 23:34:21.327975  2111 net.cpp:454] attention_reweight7 <- attention_weight_tile7
I0822 23:34:21.327977  2111 net.cpp:411] attention_reweight7 -> attention_reweight7
I0822 23:34:21.327992  2111 net.cpp:150] Setting up attention_reweight7
I0822 23:34:21.327996  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.327998  2111 net.cpp:165] Memory required for data: 119550720
I0822 23:34:21.328001  2111 layer_factory.hpp:77] Creating layer attention_reweight_sum
I0822 23:34:21.328004  2111 net.cpp:106] Creating Layer attention_reweight_sum
I0822 23:34:21.328007  2111 net.cpp:454] attention_reweight_sum <- attention_reweight0
I0822 23:34:21.328011  2111 net.cpp:454] attention_reweight_sum <- attention_reweight1
I0822 23:34:21.328012  2111 net.cpp:454] attention_reweight_sum <- attention_reweight2
I0822 23:34:21.328016  2111 net.cpp:454] attention_reweight_sum <- attention_reweight3
I0822 23:34:21.328017  2111 net.cpp:454] attention_reweight_sum <- attention_reweight4
I0822 23:34:21.328021  2111 net.cpp:454] attention_reweight_sum <- attention_reweight5
I0822 23:34:21.328022  2111 net.cpp:454] attention_reweight_sum <- attention_reweight6
I0822 23:34:21.328024  2111 net.cpp:454] attention_reweight_sum <- attention_reweight7
I0822 23:34:21.328028  2111 net.cpp:411] attention_reweight_sum -> attention_reweight_sum
I0822 23:34:21.328040  2111 net.cpp:150] Setting up attention_reweight_sum
I0822 23:34:21.328044  2111 net.cpp:157] Top shape: 64 64 8 1 (32768)
I0822 23:34:21.328047  2111 net.cpp:165] Memory required for data: 119681792
I0822 23:34:21.328048  2111 layer_factory.hpp:77] Creating layer attention_reweight_flatten
I0822 23:34:21.328052  2111 net.cpp:106] Creating Layer attention_reweight_flatten
I0822 23:34:21.328054  2111 net.cpp:454] attention_reweight_flatten <- attention_reweight_sum
I0822 23:34:21.328058  2111 net.cpp:411] attention_reweight_flatten -> attention_reweight_flatten
I0822 23:34:21.328071  2111 net.cpp:150] Setting up attention_reweight_flatten
I0822 23:34:21.328074  2111 net.cpp:157] Top shape: 64 512 (32768)
I0822 23:34:21.328076  2111 net.cpp:165] Memory required for data: 119812864
I0822 23:34:21.328079  2111 layer_factory.hpp:77] Creating layer attention_reweight_dropout
I0822 23:34:21.328089  2111 net.cpp:106] Creating Layer attention_reweight_dropout
I0822 23:34:21.328092  2111 net.cpp:454] attention_reweight_dropout <- attention_reweight_flatten
I0822 23:34:21.328095  2111 net.cpp:411] attention_reweight_dropout -> attention_reweight_dropout
I0822 23:34:21.328125  2111 net.cpp:150] Setting up attention_reweight_dropout
I0822 23:34:21.328128  2111 net.cpp:157] Top shape: 64 512 (32768)
I0822 23:34:21.328130  2111 net.cpp:165] Memory required for data: 119943936
I0822 23:34:21.328133  2111 layer_factory.hpp:77] Creating layer classification_fc1
I0822 23:34:21.328137  2111 net.cpp:106] Creating Layer classification_fc1
I0822 23:34:21.328140  2111 net.cpp:454] classification_fc1 <- attention_reweight_dropout
I0822 23:34:21.328143  2111 net.cpp:411] classification_fc1 -> classification_fc1
I0822 23:34:21.329001  2111 net.cpp:150] Setting up classification_fc1
I0822 23:34:21.329006  2111 net.cpp:157] Top shape: 64 64 (4096)
I0822 23:34:21.329008  2111 net.cpp:165] Memory required for data: 119960320
I0822 23:34:21.329013  2111 layer_factory.hpp:77] Creating layer classification_dropout
I0822 23:34:21.329017  2111 net.cpp:106] Creating Layer classification_dropout
I0822 23:34:21.329020  2111 net.cpp:454] classification_dropout <- classification_fc1
I0822 23:34:21.329023  2111 net.cpp:411] classification_dropout -> classification_dropout
I0822 23:34:21.329044  2111 net.cpp:150] Setting up classification_dropout
I0822 23:34:21.329048  2111 net.cpp:157] Top shape: 64 64 (4096)
I0822 23:34:21.329051  2111 net.cpp:165] Memory required for data: 119976704
I0822 23:34:21.329053  2111 layer_factory.hpp:77] Creating layer classification_fc2
I0822 23:34:21.329057  2111 net.cpp:106] Creating Layer classification_fc2
I0822 23:34:21.329061  2111 net.cpp:454] classification_fc2 <- classification_dropout
I0822 23:34:21.329064  2111 net.cpp:411] classification_fc2 -> classification_fc2
I0822 23:34:21.329131  2111 net.cpp:150] Setting up classification_fc2
I0822 23:34:21.329135  2111 net.cpp:157] Top shape: 64 4 (256)
I0822 23:34:21.329138  2111 net.cpp:165] Memory required for data: 119977728
I0822 23:34:21.329141  2111 layer_factory.hpp:77] Creating layer classification_fc2_classification_fc2_0_split
I0822 23:34:21.329145  2111 net.cpp:106] Creating Layer classification_fc2_classification_fc2_0_split
I0822 23:34:21.329149  2111 net.cpp:454] classification_fc2_classification_fc2_0_split <- classification_fc2
I0822 23:34:21.329151  2111 net.cpp:411] classification_fc2_classification_fc2_0_split -> classification_fc2_classification_fc2_0_split_0
I0822 23:34:21.329155  2111 net.cpp:411] classification_fc2_classification_fc2_0_split -> classification_fc2_classification_fc2_0_split_1
I0822 23:34:21.329182  2111 net.cpp:150] Setting up classification_fc2_classification_fc2_0_split
I0822 23:34:21.329186  2111 net.cpp:157] Top shape: 64 4 (256)
I0822 23:34:21.329190  2111 net.cpp:157] Top shape: 64 4 (256)
I0822 23:34:21.329191  2111 net.cpp:165] Memory required for data: 119979776
I0822 23:34:21.329193  2111 layer_factory.hpp:77] Creating layer classification_loss
I0822 23:34:21.329198  2111 net.cpp:106] Creating Layer classification_loss
I0822 23:34:21.329206  2111 net.cpp:454] classification_loss <- classification_fc2_classification_fc2_0_split_0
I0822 23:34:21.329210  2111 net.cpp:454] classification_loss <- label_data_1_split_0
I0822 23:34:21.329213  2111 net.cpp:411] classification_loss -> classification_loss
I0822 23:34:21.329219  2111 layer_factory.hpp:77] Creating layer classification_loss
I0822 23:34:21.329275  2111 net.cpp:150] Setting up classification_loss
I0822 23:34:21.329279  2111 net.cpp:157] Top shape: (1)
I0822 23:34:21.329282  2111 net.cpp:160]     with loss weight 1
I0822 23:34:21.329290  2111 net.cpp:165] Memory required for data: 119979780
I0822 23:34:21.329293  2111 layer_factory.hpp:77] Creating layer classification_accuracy
I0822 23:34:21.329296  2111 net.cpp:106] Creating Layer classification_accuracy
I0822 23:34:21.329299  2111 net.cpp:454] classification_accuracy <- classification_fc2_classification_fc2_0_split_1
I0822 23:34:21.329303  2111 net.cpp:454] classification_accuracy <- label_data_1_split_1
I0822 23:34:21.329305  2111 net.cpp:411] classification_accuracy -> classification_accuracy
I0822 23:34:21.329310  2111 net.cpp:150] Setting up classification_accuracy
I0822 23:34:21.329313  2111 net.cpp:157] Top shape: (1)
I0822 23:34:21.329315  2111 net.cpp:165] Memory required for data: 119979784
I0822 23:34:21.329319  2111 net.cpp:228] classification_accuracy does not need backward computation.
I0822 23:34:21.329320  2111 net.cpp:226] classification_loss needs backward computation.
I0822 23:34:21.329324  2111 net.cpp:226] classification_fc2_classification_fc2_0_split needs backward computation.
I0822 23:34:21.329325  2111 net.cpp:226] classification_fc2 needs backward computation.
I0822 23:34:21.329329  2111 net.cpp:226] classification_dropout needs backward computation.
I0822 23:34:21.329330  2111 net.cpp:226] classification_fc1 needs backward computation.
I0822 23:34:21.329334  2111 net.cpp:226] attention_reweight_dropout needs backward computation.
I0822 23:34:21.329335  2111 net.cpp:226] attention_reweight_flatten needs backward computation.
I0822 23:34:21.329339  2111 net.cpp:226] attention_reweight_sum needs backward computation.
I0822 23:34:21.329341  2111 net.cpp:226] attention_reweight7 needs backward computation.
I0822 23:34:21.329344  2111 net.cpp:226] attention_weight_tile7 needs backward computation.
I0822 23:34:21.329346  2111 net.cpp:226] attention_reweight6 needs backward computation.
I0822 23:34:21.329349  2111 net.cpp:226] attention_weight_tile6 needs backward computation.
I0822 23:34:21.329352  2111 net.cpp:226] attention_reweight5 needs backward computation.
I0822 23:34:21.329355  2111 net.cpp:226] attention_weight_tile5 needs backward computation.
I0822 23:34:21.329357  2111 net.cpp:226] attention_reweight4 needs backward computation.
I0822 23:34:21.329360  2111 net.cpp:226] attention_weight_tile4 needs backward computation.
I0822 23:34:21.329362  2111 net.cpp:226] attention_reweight3 needs backward computation.
I0822 23:34:21.329365  2111 net.cpp:226] attention_weight_tile3 needs backward computation.
I0822 23:34:21.329370  2111 net.cpp:226] attention_reweight2 needs backward computation.
I0822 23:34:21.329372  2111 net.cpp:226] attention_weight_tile2 needs backward computation.
I0822 23:34:21.329375  2111 net.cpp:226] attention_reweight1 needs backward computation.
I0822 23:34:21.329377  2111 net.cpp:226] attention_weight_tile1 needs backward computation.
I0822 23:34:21.329380  2111 net.cpp:226] attention_reweight0 needs backward computation.
I0822 23:34:21.329381  2111 net.cpp:226] attention_weight_tile0 needs backward computation.
I0822 23:34:21.329385  2111 net.cpp:226] attention_weight_slice needs backward computation.
I0822 23:34:21.329386  2111 net.cpp:226] attention_weight needs backward computation.
I0822 23:34:21.329388  2111 net.cpp:226] attention_height_concat needs backward computation.
I0822 23:34:21.329392  2111 net.cpp:226] attention_height_reshape7 needs backward computation.
I0822 23:34:21.329396  2111 net.cpp:226] attention_height_fc3_7 needs backward computation.
I0822 23:34:21.329397  2111 net.cpp:226] attention_height_fc2_7 needs backward computation.
I0822 23:34:21.329399  2111 net.cpp:226] attention_height_fc1_7 needs backward computation.
I0822 23:34:21.329401  2111 net.cpp:226] attention_height_reshape6 needs backward computation.
I0822 23:34:21.329404  2111 net.cpp:226] attention_height_fc3_6 needs backward computation.
I0822 23:34:21.329406  2111 net.cpp:226] attention_height_fc2_6 needs backward computation.
I0822 23:34:21.329408  2111 net.cpp:226] attention_height_fc1_6 needs backward computation.
I0822 23:34:21.329411  2111 net.cpp:226] attention_height_reshape5 needs backward computation.
I0822 23:34:21.329412  2111 net.cpp:226] attention_height_fc3_5 needs backward computation.
I0822 23:34:21.329416  2111 net.cpp:226] attention_height_fc2_5 needs backward computation.
I0822 23:34:21.329417  2111 net.cpp:226] attention_height_fc1_5 needs backward computation.
I0822 23:34:21.329419  2111 net.cpp:226] attention_height_reshape4 needs backward computation.
I0822 23:34:21.329421  2111 net.cpp:226] attention_height_fc3_4 needs backward computation.
I0822 23:34:21.329424  2111 net.cpp:226] attention_height_fc2_4 needs backward computation.
I0822 23:34:21.329427  2111 net.cpp:226] attention_height_fc1_4 needs backward computation.
I0822 23:34:21.329428  2111 net.cpp:226] attention_height_reshape3 needs backward computation.
I0822 23:34:21.329430  2111 net.cpp:226] attention_height_fc3_3 needs backward computation.
I0822 23:34:21.329432  2111 net.cpp:226] attention_height_fc2_3 needs backward computation.
I0822 23:34:21.329434  2111 net.cpp:226] attention_height_fc1_3 needs backward computation.
I0822 23:34:21.329437  2111 net.cpp:226] attention_height_reshape2 needs backward computation.
I0822 23:34:21.329439  2111 net.cpp:226] attention_height_fc3_2 needs backward computation.
I0822 23:34:21.329442  2111 net.cpp:226] attention_height_fc2_2 needs backward computation.
I0822 23:34:21.329443  2111 net.cpp:226] attention_height_fc1_2 needs backward computation.
I0822 23:34:21.329445  2111 net.cpp:226] attention_height_reshape1 needs backward computation.
I0822 23:34:21.329447  2111 net.cpp:226] attention_height_fc3_1 needs backward computation.
I0822 23:34:21.329449  2111 net.cpp:226] attention_height_fc2_1 needs backward computation.
I0822 23:34:21.329452  2111 net.cpp:226] attention_height_fc1_1 needs backward computation.
I0822 23:34:21.329454  2111 net.cpp:226] attention_height_reshape0 needs backward computation.
I0822 23:34:21.329457  2111 net.cpp:226] attention_height_fc3_0 needs backward computation.
I0822 23:34:21.329459  2111 net.cpp:226] attention_height_fc2_0 needs backward computation.
I0822 23:34:21.329461  2111 net.cpp:226] attention_height_fc1_0 needs backward computation.
I0822 23:34:21.329463  2111 net.cpp:226] attention_height_slice needs backward computation.
I0822 23:34:21.329466  2111 net.cpp:226] attention_scale_concat needs backward computation.
I0822 23:34:21.329470  2111 net.cpp:226] attention_reduction7 needs backward computation.
I0822 23:34:21.329473  2111 net.cpp:226] attention_permute7 needs backward computation.
I0822 23:34:21.329475  2111 net.cpp:226] attention_reduction6 needs backward computation.
I0822 23:34:21.329478  2111 net.cpp:226] attention_permute6 needs backward computation.
I0822 23:34:21.329479  2111 net.cpp:226] attention_reduction5 needs backward computation.
I0822 23:34:21.329483  2111 net.cpp:226] attention_permute5 needs backward computation.
I0822 23:34:21.329486  2111 net.cpp:226] attention_reduction4 needs backward computation.
I0822 23:34:21.329488  2111 net.cpp:226] attention_permute4 needs backward computation.
I0822 23:34:21.329491  2111 net.cpp:226] attention_reduction3 needs backward computation.
I0822 23:34:21.329493  2111 net.cpp:226] attention_permute3 needs backward computation.
I0822 23:34:21.329496  2111 net.cpp:226] attention_reduction2 needs backward computation.
I0822 23:34:21.329499  2111 net.cpp:226] attention_permute2 needs backward computation.
I0822 23:34:21.329500  2111 net.cpp:226] attention_reduction1 needs backward computation.
I0822 23:34:21.329504  2111 net.cpp:226] attention_permute1 needs backward computation.
I0822 23:34:21.329505  2111 net.cpp:226] attention_reduction0 needs backward computation.
I0822 23:34:21.329507  2111 net.cpp:226] attention_permute0 needs backward computation.
I0822 23:34:21.329510  2111 net.cpp:226] attention_scale7_attention_scale_slice_7_split needs backward computation.
I0822 23:34:21.329512  2111 net.cpp:226] attention_scale6_attention_scale_slice_6_split needs backward computation.
I0822 23:34:21.329515  2111 net.cpp:226] attention_scale5_attention_scale_slice_5_split needs backward computation.
I0822 23:34:21.329517  2111 net.cpp:226] attention_scale4_attention_scale_slice_4_split needs backward computation.
I0822 23:34:21.329519  2111 net.cpp:226] attention_scale3_attention_scale_slice_3_split needs backward computation.
I0822 23:34:21.329522  2111 net.cpp:226] attention_scale2_attention_scale_slice_2_split needs backward computation.
I0822 23:34:21.329524  2111 net.cpp:226] attention_scale1_attention_scale_slice_1_split needs backward computation.
I0822 23:34:21.329526  2111 net.cpp:226] attention_scale0_attention_scale_slice_0_split needs backward computation.
I0822 23:34:21.329530  2111 net.cpp:226] attention_scale_slice needs backward computation.
I0822 23:34:21.329531  2111 net.cpp:226] attention_layer_relu1 needs backward computation.
I0822 23:34:21.329533  2111 net.cpp:226] attention_layer_bn1 needs backward computation.
I0822 23:34:21.329536  2111 net.cpp:226] attention_layer_conv1 needs backward computation.
I0822 23:34:21.329538  2111 net.cpp:226] attention_layer_relu0 needs backward computation.
I0822 23:34:21.329540  2111 net.cpp:226] attention_layer_bn0 needs backward computation.
I0822 23:34:21.329543  2111 net.cpp:226] attention_layer_conv0 needs backward computation.
I0822 23:34:21.329545  2111 net.cpp:226] dense_layer_relu4 needs backward computation.
I0822 23:34:21.329548  2111 net.cpp:226] dense_layer_bn4 needs backward computation.
I0822 23:34:21.329550  2111 net.cpp:226] dense_layer_concat4 needs backward computation.
I0822 23:34:21.329553  2111 net.cpp:226] dense_layer_conv4 needs backward computation.
I0822 23:34:21.329556  2111 net.cpp:226] dense_layer_pool3_dense_layer_pool3_0_split needs backward computation.
I0822 23:34:21.329558  2111 net.cpp:226] dense_layer_pool3 needs backward computation.
I0822 23:34:21.329561  2111 net.cpp:226] dense_layer_relu3 needs backward computation.
I0822 23:34:21.329563  2111 net.cpp:226] dense_layer_bn3 needs backward computation.
I0822 23:34:21.329566  2111 net.cpp:226] dense_layer_concat3 needs backward computation.
I0822 23:34:21.329569  2111 net.cpp:226] dense_layer_conv3 needs backward computation.
I0822 23:34:21.329571  2111 net.cpp:226] dense_layer_pool2_dense_layer_pool2_0_split needs backward computation.
I0822 23:34:21.329574  2111 net.cpp:226] dense_layer_pool2 needs backward computation.
I0822 23:34:21.329577  2111 net.cpp:226] dense_layer_relu2 needs backward computation.
I0822 23:34:21.329579  2111 net.cpp:226] dense_layer_bn2 needs backward computation.
I0822 23:34:21.329581  2111 net.cpp:226] dense_layer_concat2 needs backward computation.
I0822 23:34:21.329584  2111 net.cpp:226] dense_layer_conv2 needs backward computation.
I0822 23:34:21.329587  2111 net.cpp:226] dense_layer_pool1_dense_layer_pool1_0_split needs backward computation.
I0822 23:34:21.329589  2111 net.cpp:226] dense_layer_pool1 needs backward computation.
I0822 23:34:21.329592  2111 net.cpp:226] dense_layer_relu1 needs backward computation.
I0822 23:34:21.329594  2111 net.cpp:226] dense_layer_bn1 needs backward computation.
I0822 23:34:21.329597  2111 net.cpp:226] dense_layer_concat1 needs backward computation.
I0822 23:34:21.329602  2111 net.cpp:226] dense_layer_conv1 needs backward computation.
I0822 23:34:21.329605  2111 net.cpp:226] dense_layer_pool0_dense_layer_pool0_0_split needs backward computation.
I0822 23:34:21.329608  2111 net.cpp:226] dense_layer_pool0 needs backward computation.
I0822 23:34:21.329610  2111 net.cpp:226] dense_layer_relu0 needs backward computation.
I0822 23:34:21.329612  2111 net.cpp:226] dense_layer_bn0 needs backward computation.
I0822 23:34:21.329615  2111 net.cpp:226] dense_layer_concat0 needs backward computation.
I0822 23:34:21.329619  2111 net.cpp:226] dense_layer_conv0 needs backward computation.
I0822 23:34:21.329622  2111 net.cpp:226] trans_layer_relu2 needs backward computation.
I0822 23:34:21.329624  2111 net.cpp:226] trans_layer_bn2 needs backward computation.
I0822 23:34:21.329627  2111 net.cpp:226] trans_layer_conv2_trans_layer_conv2_0_split needs backward computation.
I0822 23:34:21.329629  2111 net.cpp:226] trans_layer_conv2 needs backward computation.
I0822 23:34:21.329632  2111 net.cpp:226] trans_layer_relu1 needs backward computation.
I0822 23:34:21.329634  2111 net.cpp:226] trans_layer_bn1 needs backward computation.
I0822 23:34:21.329636  2111 net.cpp:226] trans_layer_conv1_trans_layer_conv1_0_split needs backward computation.
I0822 23:34:21.329639  2111 net.cpp:226] trans_layer_conv1 needs backward computation.
I0822 23:34:21.329641  2111 net.cpp:226] trans_layer_relu0 needs backward computation.
I0822 23:34:21.329643  2111 net.cpp:226] trans_layer_bn0 needs backward computation.
I0822 23:34:21.329646  2111 net.cpp:226] trans_layer_conv0_trans_layer_conv0_0_split needs backward computation.
I0822 23:34:21.329649  2111 net.cpp:226] trans_layer_conv0 needs backward computation.
I0822 23:34:21.329653  2111 net.cpp:228] label_data_1_split does not need backward computation.
I0822 23:34:21.329655  2111 net.cpp:228] data does not need backward computation.
I0822 23:34:21.329658  2111 net.cpp:270] This network produces output classification_accuracy
I0822 23:34:21.329660  2111 net.cpp:270] This network produces output classification_loss
I0822 23:34:21.329730  2111 net.cpp:283] Network initialization done.
I0822 23:34:21.331601  2111 solver.cpp:181] Creating test net (#0) specified by net file: train_test_net.prototxt
I0822 23:34:21.331707  2111 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0822 23:34:21.332486  2111 net.cpp:49] Initializing net from parameters: 
name: "DenseAttentionNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "TextData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  text_data_param {
    label_source: "/ssd/ijcai18/data/agnews/test_label.txt"
    data_source: "/ssd/ijcai18/data/agnews/test_data.txt"
    dict_source: "/ssd/ijcai18/data/agnews/local_dic_index.txt"
    batch_size: 80
    channel: 1
    num_words: 100
    crop_height: 100
    crop_width: 300
    shuffle: false
  }
}
layer {
  name: "trans_layer_conv0"
  type: "Convolution"
  bottom: "data"
  top: "trans_layer_conv0"
  convolution_param {
    num_output: 64
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 300
  }
}
layer {
  name: "trans_layer_bn0"
  type: "BatchNorm"
  bottom: "trans_layer_conv0"
  top: "trans_layer_bn0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "trans_layer_relu0"
  type: "ReLU"
  bottom: "trans_layer_bn0"
  top: "trans_layer_bn0"
}
layer {
  name: "trans_layer_conv1"
  type: "Convolution"
  bottom: "trans_layer_bn0"
  top: "trans_layer_conv1"
  convolution_param {
    num_output: 64
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
  }
}
layer {
  name: "trans_layer_bn1"
  type: "BatchNorm"
  bottom: "trans_layer_conv1"
  top: "trans_layer_bn1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "trans_layer_relu1"
  type: "ReLU"
  bottom: "trans_layer_bn1"
  top: "trans_layer_bn1"
}
layer {
  name: "trans_layer_conv2"
  type: "Convolution"
  bottom: "trans_layer_bn1"
  top: "trans_layer_conv2"
  convolution_param {
    num_output: 64
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
  }
}
layer {
  name: "trans_layer_bn2"
  type: "BatchNorm"
  bottom: "trans_layer_conv2"
  top: "trans_layer_bn2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "trans_layer_relu2"
  type: "ReLU"
  bottom: "trans_layer_bn2"
  top: "trans_layer_bn2"
}
layer {
  name: "dense_layer_conv0"
  type: "Convolution"
  bottom: "trans_layer_bn2"
  top: "dense_layer_conv0"
  convolution_param {
    num_output: 64
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "dense_layer_concat0"
  type: "Concat"
  bottom: "dense_layer_conv0"
  bottom: "trans_layer_conv0"
  bottom: "trans_layer_conv1"
  bottom: "trans_layer_conv2"
  top: "dense_layer_concat0"
  concat_param {
    axis: 1
  }
}
layer {
  name: "dense_layer_bn0"
  type: "BatchNorm"
  bottom: "dense_layer_concat0"
  top: "dense_layer_bn0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "dense_layer_relu0"
  type: "ReLU"
  bottom: "dense_layer_bn0"
  top: "dense_layer_bn0"
}
layer {
  name: "dense_layer_pool0"
  type: "Pooling"
  bottom: "dense_layer_bn0"
  top: "dense_layer_pool0"
  pooling_param {
    pool: AVE
    kernel_h: 3
    kernel_w: 1
    stride_h: 2
    stride_w: 1
    pad_h: 1
    pad_w: 0
  }
}
layer {
  name: "dense_layer_conv1"
  type: "Convolution"
  bottom: "dense_layer_pool0"
  top: "dense_layer_conv1"
  convolution_param {
    num_output: 64
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "dense_layer_concat1"
  type: "Concat"
  bottom: "dense_layer_conv1"
  bottom: "dense_layer_pool0"
  top: "dense_layer_concat1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "dense_layer_bn1"
  type: "BatchNorm"
  bottom: "dense_layer_concat1"
  top: "dense_layer_bn1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "dense_layer_relu1"
  type: "ReLU"
  bottom: "dense_layer_bn1"
  top: "dense_layer_bn1"
}
layer {
  name: "dense_layer_pool1"
  type: "Pooling"
  bottom: "dense_layer_bn1"
  top: "dense_layer_pool1"
  pooling_param {
    pool: AVE
    kernel_h: 3
    kernel_w: 1
    stride_h: 2
    stride_w: 1
    pad_h: 1
    pad_w: 0
  }
}
layer {
  name: "dense_layer_conv2"
  type: "Convolution"
  bottom: "dense_layer_pool1"
  top: "dense_layer_conv2"
  convolution_param {
    num_output: 64
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "dense_layer_concat2"
  type: "Concat"
  bottom: "dense_layer_conv2"
  bottom: "dense_layer_pool1"
  top: "dense_layer_concat2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "dense_layer_bn2"
  type: "BatchNorm"
  bottom: "dense_layer_concat2"
  top: "dense_layer_bn2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "dense_layer_relu2"
  type: "ReLU"
  bottom: "dense_layer_bn2"
  top: "dense_layer_bn2"
}
layer {
  name: "dense_layer_pool2"
  type: "Pooling"
  bottom: "dense_layer_bn2"
  top: "dense_layer_pool2"
  pooling_param {
    pool: AVE
    kernel_h: 3
    kernel_w: 1
    stride_h: 2
    stride_w: 1
    pad_h: 1
    pad_w: 0
  }
}
layer {
  name: "dense_layer_conv3"
  type: "Convolution"
  bottom: "dense_layer_pool2"
  top: "dense_layer_conv3"
  convolution_param {
    num_output: 64
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "dense_layer_concat3"
  type: "Concat"
  bottom: "dense_layer_conv3"
  bottom: "dense_layer_pool2"
  top: "dense_layer_concat3"
  concat_param {
    axis: 1
  }
}
layer {
  name: "dense_layer_bn3"
  type: "BatchNorm"
  bottom: "dense_layer_concat3"
  top: "dense_layer_bn3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "dense_layer_relu3"
  type: "ReLU"
  bottom: "dense_layer_bn3"
  top: "dense_layer_bn3"
}
layer {
  name: "dense_layer_pool3"
  type: "Pooling"
  bottom: "dense_layer_bn3"
  top: "dense_layer_pool3"
  pooling_param {
    pool: AVE
    kernel_h: 3
    kernel_w: 1
    stride_h: 2
    stride_w: 1
    pad_h: 1
    pad_w: 0
  }
}
layer {
  name: "dense_layer_conv4"
  type: "Convolution"
  bottom: "dense_layer_pool3"
  top: "dense_layer_conv4"
  convolution_param {
    num_output: 64
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "dense_layer_concat4"
  type: "Concat"
  bottom: "dense_layer_conv4"
  bottom: "dense_layer_pool3"
  top: "dense_layer_concat4"
  concat_param {
    axis: 1
  }
}
layer {
  name: "dense_layer_bn4"
  type: "BatchNorm"
  bottom: "dense_layer_concat4"
  top: "dense_layer_bn4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "dense_layer_relu4"
  type: "ReLU"
  bottom: "dense_layer_bn4"
  top: "dense_layer_bn4"
}
layer {
  name: "attention_layer_conv0"
  type: "Convolution"
  bottom: "dense_layer_bn4"
  top: "attention_layer_conv0"
  convolution_param {
    num_output: 512
    group: 8
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
  }
}
layer {
  name: "attention_layer_bn0"
  type: "BatchNorm"
  bottom: "attention_layer_conv0"
  top: "attention_layer_bn0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "attention_layer_relu0"
  type: "ReLU"
  bottom: "attention_layer_bn0"
  top: "attention_layer_bn0"
}
layer {
  name: "attention_layer_conv1"
  type: "Convolution"
  bottom: "attention_layer_bn0"
  top: "attention_layer_conv1"
  convolution_param {
    num_output: 512
    group: 8
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.058925565
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
  }
}
layer {
  name: "attention_layer_bn1"
  type: "BatchNorm"
  bottom: "attention_layer_conv1"
  top: "attention_layer_bn1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "attention_layer_relu1"
  type: "ReLU"
  bottom: "attention_layer_bn1"
  top: "attention_layer_bn1"
}
layer {
  name: "attention_scale_slice"
  type: "Slice"
  bottom: "attention_layer_bn1"
  top: "attention_scale0"
  top: "attention_scale1"
  top: "attention_scale2"
  top: "attention_scale3"
  top: "attention_scale4"
  top: "attention_scale5"
  top: "attention_scale6"
  top: "attention_scale7"
  slice_param {
    slice_point: 64
    slice_point: 128
    slice_point: 192
    slice_point: 256
    slice_point: 320
    slice_point: 384
    slice_point: 448
    axis: 1
  }
}
layer {
  name: "attention_permute0"
  type: "Permute"
  bottom: "attention_scale0"
  top: "attention_permute0"
  permute_param {
    order: 0
    order: 3
    order: 2
    order: 1
  }
}
layer {
  name: "attention_reduction0"
  type: "Reduction"
  bottom: "attention_permute0"
  top: "attention_reduction0"
  reduction_param {
    axis: 3
  }
}
layer {
  name: "attention_permute1"
  type: "Permute"
  bottom: "attention_scale1"
  top: "attention_permute1"
  permute_param {
    order: 0
    order: 3
    order: 2
    order: 1
  }
}
layer {
  name: "attention_reduction1"
  type: "Reduction"
  bottom: "attention_permute1"
  top: "attention_reduction1"
  reduction_param {
    axis: 3
  }
}
layer {
  name: "attention_permute2"
  type: "Permute"
  bottom: "attention_scale2"
  top: "attention_permute2"
  permute_param {
    order: 0
    order: 3
    order: 2
    order: 1
  }
}
layer {
  name: "attention_reduction2"
  type: "Reduction"
  bottom: "attention_permute2"
  top: "attention_reduction2"
  reduction_param {
    axis: 3
  }
}
layer {
  name: "attention_permute3"
  type: "Permute"
  bottom: "attention_scale3"
  top: "attention_permute3"
  permute_param {
    order: 0
    order: 3
    order: 2
    order: 1
  }
}
layer {
  name: "attention_reduction3"
  type: "Reduction"
  bottom: "attention_permute3"
  top: "attention_reduction3"
  reduction_param {
    axis: 3
  }
}
layer {
  name: "attention_permute4"
  type: "Permute"
  bottom: "attention_scale4"
  top: "attention_permute4"
  permute_param {
    order: 0
    order: 3
    order: 2
    order: 1
  }
}
layer {
  name: "attention_reduction4"
  type: "Reduction"
  bottom: "attention_permute4"
  top: "attention_reduction4"
  reduction_param {
    axis: 3
  }
}
layer {
  name: "attention_permute5"
  type: "Permute"
  bottom: "attention_scale5"
  top: "attention_permute5"
  permute_param {
    order: 0
    order: 3
    order: 2
    order: 1
  }
}
layer {
  name: "attention_reduction5"
  type: "Reduction"
  bottom: "attention_permute5"
  top: "attention_reduction5"
  reduction_param {
    axis: 3
  }
}
layer {
  name: "attention_permute6"
  type: "Permute"
  bottom: "attention_scale6"
  top: "attention_permute6"
  permute_param {
    order: 0
    order: 3
    order: 2
    order: 1
  }
}
layer {
  name: "attention_reduction6"
  type: "Reduction"
  bottom: "attention_permute6"
  top: "attention_reduction6"
  reduction_param {
    axis: 3
  }
}
layer {
  name: "attention_permute7"
  type: "Permute"
  bottom: "attention_scale7"
  top: "attention_permute7"
  permute_param {
    order: 0
    order: 3
    order: 2
    order: 1
  }
}
layer {
  name: "attention_reduction7"
  type: "Reduction"
  bottom: "attention_permute7"
  top: "attention_reduction7"
  reduction_param {
    axis: 3
  }
}
layer {
  name: "attention_scale_concat"
  type: "Concat"
  bottom: "attention_reduction0"
  bottom: "attention_reduction1"
  bottom: "attention_reduction2"
  bottom: "attention_reduction3"
  bottom: "attention_reduction4"
  bottom: "attention_reduction5"
  bottom: "attention_reduction6"
  bottom: "attention_reduction7"
  top: "attention_scale_concat"
  concat_param {
    axis: 1
  }
}
layer {
  name: "attention_height_slice"
  type: "Slice"
  bottom: "attention_scale_concat"
  top: "attention_height0"
  top: "attention_height1"
  top: "attention_height2"
  top: "attention_height3"
  top: "attention_height4"
  top: "attention_height5"
  top: "attention_height6"
  top: "attention_height7"
  slice_param {
    slice_point: 1
    slice_point: 2
    slice_point: 3
    slice_point: 4
    slice_point: 5
    slice_point: 6
    slice_point: 7
    axis: 2
  }
}
layer {
  name: "attention_height_fc1_0"
  type: "InnerProduct"
  bottom: "attention_height0"
  top: "attention_height_fc1_0"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc2_0"
  type: "InnerProduct"
  bottom: "attention_height_fc1_0"
  top: "attention_height_fc2_0"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc3_0"
  type: "InnerProduct"
  bottom: "attention_height_fc2_0"
  top: "attention_height_fc3_0"
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_reshape0"
  type: "Reshape"
  bottom: "attention_height_fc3_0"
  top: "attention_height_reshape0"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "attention_height_fc1_1"
  type: "InnerProduct"
  bottom: "attention_height1"
  top: "attention_height_fc1_1"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc2_1"
  type: "InnerProduct"
  bottom: "attention_height_fc1_1"
  top: "attention_height_fc2_1"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc3_1"
  type: "InnerProduct"
  bottom: "attention_height_fc2_1"
  top: "attention_height_fc3_1"
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_reshape1"
  type: "Reshape"
  bottom: "attention_height_fc3_1"
  top: "attention_height_reshape1"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "attention_height_fc1_2"
  type: "InnerProduct"
  bottom: "attention_height2"
  top: "attention_height_fc1_2"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc2_2"
  type: "InnerProduct"
  bottom: "attention_height_fc1_2"
  top: "attention_height_fc2_2"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc3_2"
  type: "InnerProduct"
  bottom: "attention_height_fc2_2"
  top: "attention_height_fc3_2"
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_reshape2"
  type: "Reshape"
  bottom: "attention_height_fc3_2"
  top: "attention_height_reshape2"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "attention_height_fc1_3"
  type: "InnerProduct"
  bottom: "attention_height3"
  top: "attention_height_fc1_3"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc2_3"
  type: "InnerProduct"
  bottom: "attention_height_fc1_3"
  top: "attention_height_fc2_3"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc3_3"
  type: "InnerProduct"
  bottom: "attention_height_fc2_3"
  top: "attention_height_fc3_3"
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_reshape3"
  type: "Reshape"
  bottom: "attention_height_fc3_3"
  top: "attention_height_reshape3"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "attention_height_fc1_4"
  type: "InnerProduct"
  bottom: "attention_height4"
  top: "attention_height_fc1_4"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc2_4"
  type: "InnerProduct"
  bottom: "attention_height_fc1_4"
  top: "attention_height_fc2_4"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc3_4"
  type: "InnerProduct"
  bottom: "attention_height_fc2_4"
  top: "attention_height_fc3_4"
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_reshape4"
  type: "Reshape"
  bottom: "attention_height_fc3_4"
  top: "attention_height_reshape4"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "attention_height_fc1_5"
  type: "InnerProduct"
  bottom: "attention_height5"
  top: "attention_height_fc1_5"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc2_5"
  type: "InnerProduct"
  bottom: "attention_height_fc1_5"
  top: "attention_height_fc2_5"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc3_5"
  type: "InnerProduct"
  bottom: "attention_height_fc2_5"
  top: "attention_height_fc3_5"
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_reshape5"
  type: "Reshape"
  bottom: "attention_height_fc3_5"
  top: "attention_height_reshape5"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "attention_height_fc1_6"
  type: "InnerProduct"
  bottom: "attention_height6"
  top: "attention_height_fc1_6"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc2_6"
  type: "InnerProduct"
  bottom: "attention_height_fc1_6"
  top: "attention_height_fc2_6"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc3_6"
  type: "InnerProduct"
  bottom: "attention_height_fc2_6"
  top: "attention_height_fc3_6"
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_reshape6"
  type: "Reshape"
  bottom: "attention_height_fc3_6"
  top: "attention_height_reshape6"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "attention_height_fc1_7"
  type: "InnerProduct"
  bottom: "attention_height7"
  top: "attention_height_fc1_7"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc2_7"
  type: "InnerProduct"
  bottom: "attention_height_fc1_7"
  top: "attention_height_fc2_7"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_fc3_7"
  type: "InnerProduct"
  bottom: "attention_height_fc2_7"
  top: "attention_height_fc3_7"
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "attention_height_reshape7"
  type: "Reshape"
  bottom: "attention_height_fc3_7"
  top: "attention_height_reshape7"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "attention_height_concat"
  type: "Concat"
  bottom: "attention_height_reshape0"
  bottom: "attention_height_reshape1"
  bottom: "attention_height_reshape2"
  bottom: "attention_height_reshape3"
  bottom: "attention_height_reshape4"
  bottom: "attention_height_reshape5"
  bottom: "attention_height_reshape6"
  bottom: "attention_height_reshape7"
  top: "attention_height_concat"
  concat_param {
    axis: 2
  }
}
layer {
  name: "attention_weight"
  type: "Softmax"
  bottom: "attention_height_concat"
  top: "attention_weight"
}
layer {
  name: "attention_weight_slice"
  type: "Slice"
  bottom: "attention_weight"
  top: "attention_weight_slice0"
  top: "attention_weight_slice1"
  top: "attention_weight_slice2"
  top: "attention_weight_slice3"
  top: "attention_weight_slice4"
  top: "attention_weight_slice5"
  top: "attention_weight_slice6"
  top: "attention_weight_slice7"
  slice_param {
    slice_point: 1
    slice_point: 2
    slice_point: 3
    slice_point: 4
    slice_point: 5
    slice_point: 6
    slice_point: 7
    axis: 1
  }
}
layer {
  name: "attention_weight_tile0"
  type: "Tile"
  bottom: "attention_weight_slice0"
  top: "attention_weight_tile0"
  tile_param {
    axis: 1
    tiles: 64
  }
}
layer {
  name: "attention_reweight0"
  type: "Eltwise"
  bottom: "attention_scale0"
  bottom: "attention_weight_tile0"
  top: "attention_reweight0"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "attention_weight_tile1"
  type: "Tile"
  bottom: "attention_weight_slice1"
  top: "attention_weight_tile1"
  tile_param {
    axis: 1
    tiles: 64
  }
}
layer {
  name: "attention_reweight1"
  type: "Eltwise"
  bottom: "attention_scale1"
  bottom: "attention_weight_tile1"
  top: "attention_reweight1"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "attention_weight_tile2"
  type: "Tile"
  bottom: "attention_weight_slice2"
  top: "attention_weight_tile2"
  tile_param {
    axis: 1
    tiles: 64
  }
}
layer {
  name: "attention_reweight2"
  type: "Eltwise"
  bottom: "attention_scale2"
  bottom: "attention_weight_tile2"
  top: "attention_reweight2"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "attention_weight_tile3"
  type: "Tile"
  bottom: "attention_weight_slice3"
  top: "attention_weight_tile3"
  tile_param {
    axis: 1
    tiles: 64
  }
}
layer {
  name: "attention_reweight3"
  type: "Eltwise"
  bottom: "attention_scale3"
  bottom: "attention_weight_tile3"
  top: "attention_reweight3"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "attention_weight_tile4"
  type: "Tile"
  bottom: "attention_weight_slice4"
  top: "attention_weight_tile4"
  tile_param {
    axis: 1
    tiles: 64
  }
}
layer {
  name: "attention_reweight4"
  type: "Eltwise"
  bottom: "attention_scale4"
  bottom: "attention_weight_tile4"
  top: "attention_reweight4"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "attention_weight_tile5"
  type: "Tile"
  bottom: "attention_weight_slice5"
  top: "attention_weight_tile5"
  tile_param {
    axis: 1
    tiles: 64
  }
}
layer {
  name: "attention_reweight5"
  type: "Eltwise"
  bottom: "attention_scale5"
  bottom: "attention_weight_tile5"
  top: "attention_reweight5"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "attention_weight_tile6"
  type: "Tile"
  bottom: "attention_weight_slice6"
  top: "attention_weight_tile6"
  tile_param {
    axis: 1
    tiles: 64
  }
}
layer {
  name: "attention_reweight6"
  type: "Eltwise"
  bottom: "attention_scale6"
  bottom: "attention_weight_tile6"
  top: "attention_reweight6"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "attention_weight_tile7"
  type: "Tile"
  bottom: "attention_weight_slice7"
  top: "attention_weight_tile7"
  tile_param {
    axis: 1
    tiles: 64
  }
}
layer {
  name: "attention_reweight7"
  type: "Eltwise"
  bottom: "attention_scale7"
  bottom: "attention_weight_tile7"
  top: "attention_reweight7"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "attention_reweight_sum"
  type: "Eltwise"
  bottom: "attention_reweight0"
  bottom: "attention_reweight1"
  bottom: "attention_reweight2"
  bottom: "attention_reweight3"
  bottom: "attention_reweight4"
  bottom: "attention_reweight5"
  bottom: "attention_reweight6"
  bottom: "attention_reweight7"
  top: "attention_reweight_sum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "attention_reweight_flatten"
  type: "Flatten"
  bottom: "attention_reweight_sum"
  top: "attention_reweight_flatten"
}
layer {
  name: "attention_reweight_dropout"
  type: "Dropout"
  bottom: "attention_reweight_flatten"
  top: "attention_reweight_dropout"
  dropout_param {
    dropout_ratio: 0.7
  }
}
layer {
  name: "classification_fc1"
  type: "InnerProduct"
  bottom: "attention_reweight_dropout"
  top: "classification_fc1"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "classification_dropout"
  type: "Dropout"
  bottom: "classification_fc1"
  top: "classification_dropout"
  dropout_param {
    dropout_ratio: 0.7
  }
}
layer {
  name: "classification_fc2"
  type: "InnerProduct"
  bottom: "classification_dropout"
  top: "classification_fc2"
  param {
    lr_mult: 10
    decay_mult: 2
  }
  param {
    lr_mult: 10
    decay_mult: 2
  }
  inner_product_param {
    num_output: 4
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "classification_loss"
  type: "SoftmaxWithLoss"
  bottom: "classification_fc2"
  bottom: "label"
  top: "classification_loss"
  loss_weight: 1
}
layer {
  name: "classification_accuracy"
  type: "Accuracy"
  bottom: "classification_fc2"
  bottom: "label"
  top: "classification_accuracy"
}
I0822 23:34:21.334229  2111 layer_factory.hpp:77] Creating layer data
I0822 23:34:21.334241  2111 net.cpp:106] Creating Layer data
I0822 23:34:21.334244  2111 net.cpp:411] data -> data
I0822 23:34:21.334250  2111 net.cpp:411] data -> label
I0822 23:34:21.334627  2111 text_data_layer.cpp:83] read label done (7600 samples)
I0822 23:34:21.334631  2111 text_data_layer.cpp:86] Reading content
I0822 23:34:21.335728  2111 text_data_layer.cpp:102] read content done: #7600 samples.
I0822 23:34:21.335777  2111 text_data_layer.cpp:117] output data size: 80,1,100,300
I0822 23:34:21.335826  2111 text_data_layer.cpp:125] Reading vector dict...
I0822 23:34:25.863332  2111 text_data_layer.cpp:132] vec_dict size: 56249
I0822 23:34:25.874315  2111 net.cpp:150] Setting up data
I0822 23:34:25.874349  2111 net.cpp:157] Top shape: 80 1 100 300 (2400000)
I0822 23:34:25.874354  2111 net.cpp:157] Top shape: 80 (80)
I0822 23:34:25.874356  2111 net.cpp:165] Memory required for data: 9600320
I0822 23:34:25.874362  2111 layer_factory.hpp:77] Creating layer label_data_1_split
I0822 23:34:25.874373  2111 net.cpp:106] Creating Layer label_data_1_split
I0822 23:34:25.874377  2111 net.cpp:454] label_data_1_split <- label
I0822 23:34:25.874382  2111 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0822 23:34:25.874389  2111 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0822 23:34:25.874431  2111 net.cpp:150] Setting up label_data_1_split
I0822 23:34:25.874438  2111 net.cpp:157] Top shape: 80 (80)
I0822 23:34:25.874440  2111 net.cpp:157] Top shape: 80 (80)
I0822 23:34:25.874444  2111 net.cpp:165] Memory required for data: 9600960
I0822 23:34:25.874445  2111 layer_factory.hpp:77] Creating layer trans_layer_conv0
I0822 23:34:25.874455  2111 net.cpp:106] Creating Layer trans_layer_conv0
I0822 23:34:25.874460  2111 net.cpp:454] trans_layer_conv0 <- data
I0822 23:34:25.874465  2111 net.cpp:411] trans_layer_conv0 -> trans_layer_conv0
I0822 23:34:25.875169  2111 net.cpp:150] Setting up trans_layer_conv0
I0822 23:34:25.875177  2111 net.cpp:157] Top shape: 80 64 100 1 (512000)
I0822 23:34:25.875191  2111 net.cpp:165] Memory required for data: 11648960
I0822 23:34:25.875200  2111 layer_factory.hpp:77] Creating layer trans_layer_conv0_trans_layer_conv0_0_split
I0822 23:34:25.875205  2111 net.cpp:106] Creating Layer trans_layer_conv0_trans_layer_conv0_0_split
I0822 23:34:25.875208  2111 net.cpp:454] trans_layer_conv0_trans_layer_conv0_0_split <- trans_layer_conv0
I0822 23:34:25.875211  2111 net.cpp:411] trans_layer_conv0_trans_layer_conv0_0_split -> trans_layer_conv0_trans_layer_conv0_0_split_0
I0822 23:34:25.875216  2111 net.cpp:411] trans_layer_conv0_trans_layer_conv0_0_split -> trans_layer_conv0_trans_layer_conv0_0_split_1
I0822 23:34:25.875241  2111 net.cpp:150] Setting up trans_layer_conv0_trans_layer_conv0_0_split
I0822 23:34:25.875247  2111 net.cpp:157] Top shape: 80 64 100 1 (512000)
I0822 23:34:25.875250  2111 net.cpp:157] Top shape: 80 64 100 1 (512000)
I0822 23:34:25.875253  2111 net.cpp:165] Memory required for data: 15744960
I0822 23:34:25.875255  2111 layer_factory.hpp:77] Creating layer trans_layer_bn0
I0822 23:34:25.875262  2111 net.cpp:106] Creating Layer trans_layer_bn0
I0822 23:34:25.875264  2111 net.cpp:454] trans_layer_bn0 <- trans_layer_conv0_trans_layer_conv0_0_split_0
I0822 23:34:25.875268  2111 net.cpp:411] trans_layer_bn0 -> trans_layer_bn0
I0822 23:34:25.875394  2111 net.cpp:150] Setting up trans_layer_bn0
I0822 23:34:25.875401  2111 net.cpp:157] Top shape: 80 64 100 1 (512000)
I0822 23:34:25.875403  2111 net.cpp:165] Memory required for data: 17792960
I0822 23:34:25.875411  2111 layer_factory.hpp:77] Creating layer trans_layer_relu0
I0822 23:34:25.875416  2111 net.cpp:106] Creating Layer trans_layer_relu0
I0822 23:34:25.875419  2111 net.cpp:454] trans_layer_relu0 <- trans_layer_bn0
I0822 23:34:25.875422  2111 net.cpp:397] trans_layer_relu0 -> trans_layer_bn0 (in-place)
I0822 23:34:25.875427  2111 net.cpp:150] Setting up trans_layer_relu0
I0822 23:34:25.875430  2111 net.cpp:157] Top shape: 80 64 100 1 (512000)
I0822 23:34:25.875432  2111 net.cpp:165] Memory required for data: 19840960
I0822 23:34:25.875435  2111 layer_factory.hpp:77] Creating layer trans_layer_conv1
I0822 23:34:25.875440  2111 net.cpp:106] Creating Layer trans_layer_conv1
I0822 23:34:25.875443  2111 net.cpp:454] trans_layer_conv1 <- trans_layer_bn0
I0822 23:34:25.875447  2111 net.cpp:411] trans_layer_conv1 -> trans_layer_conv1
I0822 23:34:25.875696  2111 net.cpp:150] Setting up trans_layer_conv1
I0822 23:34:25.875703  2111 net.cpp:157] Top shape: 80 64 100 1 (512000)
I0822 23:34:25.875706  2111 net.cpp:165] Memory required for data: 21888960
I0822 23:34:25.875710  2111 layer_factory.hpp:77] Creating layer trans_layer_conv1_trans_layer_conv1_0_split
I0822 23:34:25.875715  2111 net.cpp:106] Creating Layer trans_layer_conv1_trans_layer_conv1_0_split
I0822 23:34:25.875717  2111 net.cpp:454] trans_layer_conv1_trans_layer_conv1_0_split <- trans_layer_conv1
I0822 23:34:25.875720  2111 net.cpp:411] trans_layer_conv1_trans_layer_conv1_0_split -> trans_layer_conv1_trans_layer_conv1_0_split_0
I0822 23:34:25.875725  2111 net.cpp:411] trans_layer_conv1_trans_layer_conv1_0_split -> trans_layer_conv1_trans_layer_conv1_0_split_1
I0822 23:34:25.875747  2111 net.cpp:150] Setting up trans_layer_conv1_trans_layer_conv1_0_split
I0822 23:34:25.875753  2111 net.cpp:157] Top shape: 80 64 100 1 (512000)
I0822 23:34:25.875756  2111 net.cpp:157] Top shape: 80 64 100 1 (512000)
I0822 23:34:25.875758  2111 net.cpp:165] Memory required for data: 25984960
I0822 23:34:25.875761  2111 layer_factory.hpp:77] Creating layer trans_layer_bn1
I0822 23:34:25.875766  2111 net.cpp:106] Creating Layer trans_layer_bn1
I0822 23:34:25.875768  2111 net.cpp:454] trans_layer_bn1 <- trans_layer_conv1_trans_layer_conv1_0_split_0
I0822 23:34:25.875772  2111 net.cpp:411] trans_layer_bn1 -> trans_layer_bn1
I0822 23:34:25.875900  2111 net.cpp:150] Setting up trans_layer_bn1
I0822 23:34:25.875906  2111 net.cpp:157] Top shape: 80 64 100 1 (512000)
I0822 23:34:25.875908  2111 net.cpp:165] Memory required for data: 28032960
I0822 23:34:25.875916  2111 layer_factory.hpp:77] Creating layer trans_layer_relu1
I0822 23:34:25.875921  2111 net.cpp:106] Creating Layer trans_layer_relu1
I0822 23:34:25.875922  2111 net.cpp:454] trans_layer_relu1 <- trans_layer_bn1
I0822 23:34:25.875926  2111 net.cpp:397] trans_layer_relu1 -> trans_layer_bn1 (in-place)
I0822 23:34:25.875931  2111 net.cpp:150] Setting up trans_layer_relu1
I0822 23:34:25.875933  2111 net.cpp:157] Top shape: 80 64 100 1 (512000)
I0822 23:34:25.875936  2111 net.cpp:165] Memory required for data: 30080960
I0822 23:34:25.875937  2111 layer_factory.hpp:77] Creating layer trans_layer_conv2
I0822 23:34:25.875943  2111 net.cpp:106] Creating Layer trans_layer_conv2
I0822 23:34:25.875946  2111 net.cpp:454] trans_layer_conv2 <- trans_layer_bn1
I0822 23:34:25.875949  2111 net.cpp:411] trans_layer_conv2 -> trans_layer_conv2
I0822 23:34:25.876195  2111 net.cpp:150] Setting up trans_layer_conv2
I0822 23:34:25.876204  2111 net.cpp:157] Top shape: 80 64 100 1 (512000)
I0822 23:34:25.876205  2111 net.cpp:165] Memory required for data: 32128960
I0822 23:34:25.876209  2111 layer_factory.hpp:77] Creating layer trans_layer_conv2_trans_layer_conv2_0_split
I0822 23:34:25.876214  2111 net.cpp:106] Creating Layer trans_layer_conv2_trans_layer_conv2_0_split
I0822 23:34:25.876215  2111 net.cpp:454] trans_layer_conv2_trans_layer_conv2_0_split <- trans_layer_conv2
I0822 23:34:25.876219  2111 net.cpp:411] trans_layer_conv2_trans_layer_conv2_0_split -> trans_layer_conv2_trans_layer_conv2_0_split_0
I0822 23:34:25.876224  2111 net.cpp:411] trans_layer_conv2_trans_layer_conv2_0_split -> trans_layer_conv2_trans_layer_conv2_0_split_1
I0822 23:34:25.876246  2111 net.cpp:150] Setting up trans_layer_conv2_trans_layer_conv2_0_split
I0822 23:34:25.876252  2111 net.cpp:157] Top shape: 80 64 100 1 (512000)
I0822 23:34:25.876255  2111 net.cpp:157] Top shape: 80 64 100 1 (512000)
I0822 23:34:25.876257  2111 net.cpp:165] Memory required for data: 36224960
I0822 23:34:25.876260  2111 layer_factory.hpp:77] Creating layer trans_layer_bn2
I0822 23:34:25.876265  2111 net.cpp:106] Creating Layer trans_layer_bn2
I0822 23:34:25.876266  2111 net.cpp:454] trans_layer_bn2 <- trans_layer_conv2_trans_layer_conv2_0_split_0
I0822 23:34:25.876271  2111 net.cpp:411] trans_layer_bn2 -> trans_layer_bn2
I0822 23:34:25.876396  2111 net.cpp:150] Setting up trans_layer_bn2
I0822 23:34:25.876402  2111 net.cpp:157] Top shape: 80 64 100 1 (512000)
I0822 23:34:25.876405  2111 net.cpp:165] Memory required for data: 38272960
I0822 23:34:25.876410  2111 layer_factory.hpp:77] Creating layer trans_layer_relu2
I0822 23:34:25.876415  2111 net.cpp:106] Creating Layer trans_layer_relu2
I0822 23:34:25.876417  2111 net.cpp:454] trans_layer_relu2 <- trans_layer_bn2
I0822 23:34:25.876421  2111 net.cpp:397] trans_layer_relu2 -> trans_layer_bn2 (in-place)
I0822 23:34:25.876425  2111 net.cpp:150] Setting up trans_layer_relu2
I0822 23:34:25.876428  2111 net.cpp:157] Top shape: 80 64 100 1 (512000)
I0822 23:34:25.876430  2111 net.cpp:165] Memory required for data: 40320960
I0822 23:34:25.876433  2111 layer_factory.hpp:77] Creating layer dense_layer_conv0
I0822 23:34:25.876438  2111 net.cpp:106] Creating Layer dense_layer_conv0
I0822 23:34:25.876441  2111 net.cpp:454] dense_layer_conv0 <- trans_layer_bn2
I0822 23:34:25.876444  2111 net.cpp:411] dense_layer_conv0 -> dense_layer_conv0
I0822 23:34:25.876979  2111 net.cpp:150] Setting up dense_layer_conv0
I0822 23:34:25.876987  2111 net.cpp:157] Top shape: 80 64 100 1 (512000)
I0822 23:34:25.876991  2111 net.cpp:165] Memory required for data: 42368960
I0822 23:34:25.876996  2111 layer_factory.hpp:77] Creating layer dense_layer_concat0
I0822 23:34:25.877002  2111 net.cpp:106] Creating Layer dense_layer_concat0
I0822 23:34:25.877004  2111 net.cpp:454] dense_layer_concat0 <- dense_layer_conv0
I0822 23:34:25.877008  2111 net.cpp:454] dense_layer_concat0 <- trans_layer_conv0_trans_layer_conv0_0_split_1
I0822 23:34:25.877010  2111 net.cpp:454] dense_layer_concat0 <- trans_layer_conv1_trans_layer_conv1_0_split_1
I0822 23:34:25.877013  2111 net.cpp:454] dense_layer_concat0 <- trans_layer_conv2_trans_layer_conv2_0_split_1
I0822 23:34:25.877017  2111 net.cpp:411] dense_layer_concat0 -> dense_layer_concat0
I0822 23:34:25.877804  2111 net.cpp:150] Setting up dense_layer_concat0
I0822 23:34:25.877830  2111 net.cpp:157] Top shape: 80 256 100 1 (2048000)
I0822 23:34:25.877832  2111 net.cpp:165] Memory required for data: 50560960
I0822 23:34:25.877835  2111 layer_factory.hpp:77] Creating layer dense_layer_bn0
I0822 23:34:25.877842  2111 net.cpp:106] Creating Layer dense_layer_bn0
I0822 23:34:25.877846  2111 net.cpp:454] dense_layer_bn0 <- dense_layer_concat0
I0822 23:34:25.877851  2111 net.cpp:411] dense_layer_bn0 -> dense_layer_bn0
I0822 23:34:25.877985  2111 net.cpp:150] Setting up dense_layer_bn0
I0822 23:34:25.877992  2111 net.cpp:157] Top shape: 80 256 100 1 (2048000)
I0822 23:34:25.877995  2111 net.cpp:165] Memory required for data: 58752960
I0822 23:34:25.878001  2111 layer_factory.hpp:77] Creating layer dense_layer_relu0
I0822 23:34:25.878006  2111 net.cpp:106] Creating Layer dense_layer_relu0
I0822 23:34:25.878010  2111 net.cpp:454] dense_layer_relu0 <- dense_layer_bn0
I0822 23:34:25.878013  2111 net.cpp:397] dense_layer_relu0 -> dense_layer_bn0 (in-place)
I0822 23:34:25.878017  2111 net.cpp:150] Setting up dense_layer_relu0
I0822 23:34:25.878021  2111 net.cpp:157] Top shape: 80 256 100 1 (2048000)
I0822 23:34:25.878022  2111 net.cpp:165] Memory required for data: 66944960
I0822 23:34:25.878026  2111 layer_factory.hpp:77] Creating layer dense_layer_pool0
I0822 23:34:25.878031  2111 net.cpp:106] Creating Layer dense_layer_pool0
I0822 23:34:25.878032  2111 net.cpp:454] dense_layer_pool0 <- dense_layer_bn0
I0822 23:34:25.878036  2111 net.cpp:411] dense_layer_pool0 -> dense_layer_pool0
I0822 23:34:25.878053  2111 net.cpp:150] Setting up dense_layer_pool0
I0822 23:34:25.878059  2111 net.cpp:157] Top shape: 80 256 51 1 (1044480)
I0822 23:34:25.878062  2111 net.cpp:165] Memory required for data: 71122880
I0822 23:34:25.878063  2111 layer_factory.hpp:77] Creating layer dense_layer_pool0_dense_layer_pool0_0_split
I0822 23:34:25.878068  2111 net.cpp:106] Creating Layer dense_layer_pool0_dense_layer_pool0_0_split
I0822 23:34:25.878072  2111 net.cpp:454] dense_layer_pool0_dense_layer_pool0_0_split <- dense_layer_pool0
I0822 23:34:25.878074  2111 net.cpp:411] dense_layer_pool0_dense_layer_pool0_0_split -> dense_layer_pool0_dense_layer_pool0_0_split_0
I0822 23:34:25.878078  2111 net.cpp:411] dense_layer_pool0_dense_layer_pool0_0_split -> dense_layer_pool0_dense_layer_pool0_0_split_1
I0822 23:34:25.878100  2111 net.cpp:150] Setting up dense_layer_pool0_dense_layer_pool0_0_split
I0822 23:34:25.878105  2111 net.cpp:157] Top shape: 80 256 51 1 (1044480)
I0822 23:34:25.878108  2111 net.cpp:157] Top shape: 80 256 51 1 (1044480)
I0822 23:34:25.878111  2111 net.cpp:165] Memory required for data: 79478720
I0822 23:34:25.878113  2111 layer_factory.hpp:77] Creating layer dense_layer_conv1
I0822 23:34:25.878119  2111 net.cpp:106] Creating Layer dense_layer_conv1
I0822 23:34:25.878123  2111 net.cpp:454] dense_layer_conv1 <- dense_layer_pool0_dense_layer_pool0_0_split_0
I0822 23:34:25.878127  2111 net.cpp:411] dense_layer_conv1 -> dense_layer_conv1
I0822 23:34:25.879467  2111 net.cpp:150] Setting up dense_layer_conv1
I0822 23:34:25.879474  2111 net.cpp:157] Top shape: 80 64 51 1 (261120)
I0822 23:34:25.879477  2111 net.cpp:165] Memory required for data: 80523200
I0822 23:34:25.879492  2111 layer_factory.hpp:77] Creating layer dense_layer_concat1
I0822 23:34:25.879498  2111 net.cpp:106] Creating Layer dense_layer_concat1
I0822 23:34:25.879500  2111 net.cpp:454] dense_layer_concat1 <- dense_layer_conv1
I0822 23:34:25.879503  2111 net.cpp:454] dense_layer_concat1 <- dense_layer_pool0_dense_layer_pool0_0_split_1
I0822 23:34:25.879508  2111 net.cpp:411] dense_layer_concat1 -> dense_layer_concat1
I0822 23:34:25.879524  2111 net.cpp:150] Setting up dense_layer_concat1
I0822 23:34:25.879528  2111 net.cpp:157] Top shape: 80 320 51 1 (1305600)
I0822 23:34:25.879530  2111 net.cpp:165] Memory required for data: 85745600
I0822 23:34:25.879534  2111 layer_factory.hpp:77] Creating layer dense_layer_bn1
I0822 23:34:25.879537  2111 net.cpp:106] Creating Layer dense_layer_bn1
I0822 23:34:25.879539  2111 net.cpp:454] dense_layer_bn1 <- dense_layer_concat1
I0822 23:34:25.879544  2111 net.cpp:411] dense_layer_bn1 -> dense_layer_bn1
I0822 23:34:25.879670  2111 net.cpp:150] Setting up dense_layer_bn1
I0822 23:34:25.879676  2111 net.cpp:157] Top shape: 80 320 51 1 (1305600)
I0822 23:34:25.879679  2111 net.cpp:165] Memory required for data: 90968000
I0822 23:34:25.879684  2111 layer_factory.hpp:77] Creating layer dense_layer_relu1
I0822 23:34:25.879689  2111 net.cpp:106] Creating Layer dense_layer_relu1
I0822 23:34:25.879691  2111 net.cpp:454] dense_layer_relu1 <- dense_layer_bn1
I0822 23:34:25.879695  2111 net.cpp:397] dense_layer_relu1 -> dense_layer_bn1 (in-place)
I0822 23:34:25.879699  2111 net.cpp:150] Setting up dense_layer_relu1
I0822 23:34:25.879703  2111 net.cpp:157] Top shape: 80 320 51 1 (1305600)
I0822 23:34:25.879704  2111 net.cpp:165] Memory required for data: 96190400
I0822 23:34:25.879706  2111 layer_factory.hpp:77] Creating layer dense_layer_pool1
I0822 23:34:25.879711  2111 net.cpp:106] Creating Layer dense_layer_pool1
I0822 23:34:25.879714  2111 net.cpp:454] dense_layer_pool1 <- dense_layer_bn1
I0822 23:34:25.879717  2111 net.cpp:411] dense_layer_pool1 -> dense_layer_pool1
I0822 23:34:25.879732  2111 net.cpp:150] Setting up dense_layer_pool1
I0822 23:34:25.879739  2111 net.cpp:157] Top shape: 80 320 26 1 (665600)
I0822 23:34:25.879741  2111 net.cpp:165] Memory required for data: 98852800
I0822 23:34:25.879743  2111 layer_factory.hpp:77] Creating layer dense_layer_pool1_dense_layer_pool1_0_split
I0822 23:34:25.879747  2111 net.cpp:106] Creating Layer dense_layer_pool1_dense_layer_pool1_0_split
I0822 23:34:25.879750  2111 net.cpp:454] dense_layer_pool1_dense_layer_pool1_0_split <- dense_layer_pool1
I0822 23:34:25.879753  2111 net.cpp:411] dense_layer_pool1_dense_layer_pool1_0_split -> dense_layer_pool1_dense_layer_pool1_0_split_0
I0822 23:34:25.879757  2111 net.cpp:411] dense_layer_pool1_dense_layer_pool1_0_split -> dense_layer_pool1_dense_layer_pool1_0_split_1
I0822 23:34:25.879778  2111 net.cpp:150] Setting up dense_layer_pool1_dense_layer_pool1_0_split
I0822 23:34:25.879782  2111 net.cpp:157] Top shape: 80 320 26 1 (665600)
I0822 23:34:25.879784  2111 net.cpp:157] Top shape: 80 320 26 1 (665600)
I0822 23:34:25.879787  2111 net.cpp:165] Memory required for data: 104177600
I0822 23:34:25.879789  2111 layer_factory.hpp:77] Creating layer dense_layer_conv2
I0822 23:34:25.879796  2111 net.cpp:106] Creating Layer dense_layer_conv2
I0822 23:34:25.879797  2111 net.cpp:454] dense_layer_conv2 <- dense_layer_pool1_dense_layer_pool1_0_split_0
I0822 23:34:25.879802  2111 net.cpp:411] dense_layer_conv2 -> dense_layer_conv2
I0822 23:34:25.881446  2111 net.cpp:150] Setting up dense_layer_conv2
I0822 23:34:25.881455  2111 net.cpp:157] Top shape: 80 64 26 1 (133120)
I0822 23:34:25.881458  2111 net.cpp:165] Memory required for data: 104710080
I0822 23:34:25.881462  2111 layer_factory.hpp:77] Creating layer dense_layer_concat2
I0822 23:34:25.881466  2111 net.cpp:106] Creating Layer dense_layer_concat2
I0822 23:34:25.881469  2111 net.cpp:454] dense_layer_concat2 <- dense_layer_conv2
I0822 23:34:25.881472  2111 net.cpp:454] dense_layer_concat2 <- dense_layer_pool1_dense_layer_pool1_0_split_1
I0822 23:34:25.881476  2111 net.cpp:411] dense_layer_concat2 -> dense_layer_concat2
I0822 23:34:25.881492  2111 net.cpp:150] Setting up dense_layer_concat2
I0822 23:34:25.881498  2111 net.cpp:157] Top shape: 80 384 26 1 (798720)
I0822 23:34:25.881500  2111 net.cpp:165] Memory required for data: 107904960
I0822 23:34:25.881502  2111 layer_factory.hpp:77] Creating layer dense_layer_bn2
I0822 23:34:25.881507  2111 net.cpp:106] Creating Layer dense_layer_bn2
I0822 23:34:25.881510  2111 net.cpp:454] dense_layer_bn2 <- dense_layer_concat2
I0822 23:34:25.881513  2111 net.cpp:411] dense_layer_bn2 -> dense_layer_bn2
I0822 23:34:25.881641  2111 net.cpp:150] Setting up dense_layer_bn2
I0822 23:34:25.881647  2111 net.cpp:157] Top shape: 80 384 26 1 (798720)
I0822 23:34:25.881650  2111 net.cpp:165] Memory required for data: 111099840
I0822 23:34:25.881655  2111 layer_factory.hpp:77] Creating layer dense_layer_relu2
I0822 23:34:25.881660  2111 net.cpp:106] Creating Layer dense_layer_relu2
I0822 23:34:25.881662  2111 net.cpp:454] dense_layer_relu2 <- dense_layer_bn2
I0822 23:34:25.881666  2111 net.cpp:397] dense_layer_relu2 -> dense_layer_bn2 (in-place)
I0822 23:34:25.881670  2111 net.cpp:150] Setting up dense_layer_relu2
I0822 23:34:25.881673  2111 net.cpp:157] Top shape: 80 384 26 1 (798720)
I0822 23:34:25.881676  2111 net.cpp:165] Memory required for data: 114294720
I0822 23:34:25.881678  2111 layer_factory.hpp:77] Creating layer dense_layer_pool2
I0822 23:34:25.881683  2111 net.cpp:106] Creating Layer dense_layer_pool2
I0822 23:34:25.881685  2111 net.cpp:454] dense_layer_pool2 <- dense_layer_bn2
I0822 23:34:25.881690  2111 net.cpp:411] dense_layer_pool2 -> dense_layer_pool2
I0822 23:34:25.881705  2111 net.cpp:150] Setting up dense_layer_pool2
I0822 23:34:25.881711  2111 net.cpp:157] Top shape: 80 384 14 1 (430080)
I0822 23:34:25.881712  2111 net.cpp:165] Memory required for data: 116015040
I0822 23:34:25.881714  2111 layer_factory.hpp:77] Creating layer dense_layer_pool2_dense_layer_pool2_0_split
I0822 23:34:25.881719  2111 net.cpp:106] Creating Layer dense_layer_pool2_dense_layer_pool2_0_split
I0822 23:34:25.881721  2111 net.cpp:454] dense_layer_pool2_dense_layer_pool2_0_split <- dense_layer_pool2
I0822 23:34:25.881726  2111 net.cpp:411] dense_layer_pool2_dense_layer_pool2_0_split -> dense_layer_pool2_dense_layer_pool2_0_split_0
I0822 23:34:25.881731  2111 net.cpp:411] dense_layer_pool2_dense_layer_pool2_0_split -> dense_layer_pool2_dense_layer_pool2_0_split_1
I0822 23:34:25.881752  2111 net.cpp:150] Setting up dense_layer_pool2_dense_layer_pool2_0_split
I0822 23:34:25.881758  2111 net.cpp:157] Top shape: 80 384 14 1 (430080)
I0822 23:34:25.881762  2111 net.cpp:157] Top shape: 80 384 14 1 (430080)
I0822 23:34:25.881763  2111 net.cpp:165] Memory required for data: 119455680
I0822 23:34:25.881765  2111 layer_factory.hpp:77] Creating layer dense_layer_conv3
I0822 23:34:25.881773  2111 net.cpp:106] Creating Layer dense_layer_conv3
I0822 23:34:25.881777  2111 net.cpp:454] dense_layer_conv3 <- dense_layer_pool2_dense_layer_pool2_0_split_0
I0822 23:34:25.881780  2111 net.cpp:411] dense_layer_conv3 -> dense_layer_conv3
I0822 23:34:25.883695  2111 net.cpp:150] Setting up dense_layer_conv3
I0822 23:34:25.883702  2111 net.cpp:157] Top shape: 80 64 14 1 (71680)
I0822 23:34:25.883715  2111 net.cpp:165] Memory required for data: 119742400
I0822 23:34:25.883719  2111 layer_factory.hpp:77] Creating layer dense_layer_concat3
I0822 23:34:25.883723  2111 net.cpp:106] Creating Layer dense_layer_concat3
I0822 23:34:25.883726  2111 net.cpp:454] dense_layer_concat3 <- dense_layer_conv3
I0822 23:34:25.883729  2111 net.cpp:454] dense_layer_concat3 <- dense_layer_pool2_dense_layer_pool2_0_split_1
I0822 23:34:25.883733  2111 net.cpp:411] dense_layer_concat3 -> dense_layer_concat3
I0822 23:34:25.883749  2111 net.cpp:150] Setting up dense_layer_concat3
I0822 23:34:25.883752  2111 net.cpp:157] Top shape: 80 448 14 1 (501760)
I0822 23:34:25.883754  2111 net.cpp:165] Memory required for data: 121749440
I0822 23:34:25.883757  2111 layer_factory.hpp:77] Creating layer dense_layer_bn3
I0822 23:34:25.883761  2111 net.cpp:106] Creating Layer dense_layer_bn3
I0822 23:34:25.883764  2111 net.cpp:454] dense_layer_bn3 <- dense_layer_concat3
I0822 23:34:25.883767  2111 net.cpp:411] dense_layer_bn3 -> dense_layer_bn3
I0822 23:34:25.883893  2111 net.cpp:150] Setting up dense_layer_bn3
I0822 23:34:25.883898  2111 net.cpp:157] Top shape: 80 448 14 1 (501760)
I0822 23:34:25.883900  2111 net.cpp:165] Memory required for data: 123756480
I0822 23:34:25.883909  2111 layer_factory.hpp:77] Creating layer dense_layer_relu3
I0822 23:34:25.883914  2111 net.cpp:106] Creating Layer dense_layer_relu3
I0822 23:34:25.883916  2111 net.cpp:454] dense_layer_relu3 <- dense_layer_bn3
I0822 23:34:25.883920  2111 net.cpp:397] dense_layer_relu3 -> dense_layer_bn3 (in-place)
I0822 23:34:25.883924  2111 net.cpp:150] Setting up dense_layer_relu3
I0822 23:34:25.883927  2111 net.cpp:157] Top shape: 80 448 14 1 (501760)
I0822 23:34:25.883930  2111 net.cpp:165] Memory required for data: 125763520
I0822 23:34:25.883932  2111 layer_factory.hpp:77] Creating layer dense_layer_pool3
I0822 23:34:25.883936  2111 net.cpp:106] Creating Layer dense_layer_pool3
I0822 23:34:25.883939  2111 net.cpp:454] dense_layer_pool3 <- dense_layer_bn3
I0822 23:34:25.883942  2111 net.cpp:411] dense_layer_pool3 -> dense_layer_pool3
I0822 23:34:25.883958  2111 net.cpp:150] Setting up dense_layer_pool3
I0822 23:34:25.883963  2111 net.cpp:157] Top shape: 80 448 8 1 (286720)
I0822 23:34:25.883965  2111 net.cpp:165] Memory required for data: 126910400
I0822 23:34:25.883968  2111 layer_factory.hpp:77] Creating layer dense_layer_pool3_dense_layer_pool3_0_split
I0822 23:34:25.883971  2111 net.cpp:106] Creating Layer dense_layer_pool3_dense_layer_pool3_0_split
I0822 23:34:25.883975  2111 net.cpp:454] dense_layer_pool3_dense_layer_pool3_0_split <- dense_layer_pool3
I0822 23:34:25.883977  2111 net.cpp:411] dense_layer_pool3_dense_layer_pool3_0_split -> dense_layer_pool3_dense_layer_pool3_0_split_0
I0822 23:34:25.883982  2111 net.cpp:411] dense_layer_pool3_dense_layer_pool3_0_split -> dense_layer_pool3_dense_layer_pool3_0_split_1
I0822 23:34:25.884028  2111 net.cpp:150] Setting up dense_layer_pool3_dense_layer_pool3_0_split
I0822 23:34:25.884035  2111 net.cpp:157] Top shape: 80 448 8 1 (286720)
I0822 23:34:25.884038  2111 net.cpp:157] Top shape: 80 448 8 1 (286720)
I0822 23:34:25.884040  2111 net.cpp:165] Memory required for data: 129204160
I0822 23:34:25.884043  2111 layer_factory.hpp:77] Creating layer dense_layer_conv4
I0822 23:34:25.884049  2111 net.cpp:106] Creating Layer dense_layer_conv4
I0822 23:34:25.884053  2111 net.cpp:454] dense_layer_conv4 <- dense_layer_pool3_dense_layer_pool3_0_split_0
I0822 23:34:25.884057  2111 net.cpp:411] dense_layer_conv4 -> dense_layer_conv4
I0822 23:34:25.887007  2111 net.cpp:150] Setting up dense_layer_conv4
I0822 23:34:25.887022  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.887025  2111 net.cpp:165] Memory required for data: 129368000
I0822 23:34:25.887030  2111 layer_factory.hpp:77] Creating layer dense_layer_concat4
I0822 23:34:25.887037  2111 net.cpp:106] Creating Layer dense_layer_concat4
I0822 23:34:25.887039  2111 net.cpp:454] dense_layer_concat4 <- dense_layer_conv4
I0822 23:34:25.887043  2111 net.cpp:454] dense_layer_concat4 <- dense_layer_pool3_dense_layer_pool3_0_split_1
I0822 23:34:25.887048  2111 net.cpp:411] dense_layer_concat4 -> dense_layer_concat4
I0822 23:34:25.887068  2111 net.cpp:150] Setting up dense_layer_concat4
I0822 23:34:25.887073  2111 net.cpp:157] Top shape: 80 512 8 1 (327680)
I0822 23:34:25.887075  2111 net.cpp:165] Memory required for data: 130678720
I0822 23:34:25.887079  2111 layer_factory.hpp:77] Creating layer dense_layer_bn4
I0822 23:34:25.887084  2111 net.cpp:106] Creating Layer dense_layer_bn4
I0822 23:34:25.887085  2111 net.cpp:454] dense_layer_bn4 <- dense_layer_concat4
I0822 23:34:25.887089  2111 net.cpp:411] dense_layer_bn4 -> dense_layer_bn4
I0822 23:34:25.887218  2111 net.cpp:150] Setting up dense_layer_bn4
I0822 23:34:25.887225  2111 net.cpp:157] Top shape: 80 512 8 1 (327680)
I0822 23:34:25.887228  2111 net.cpp:165] Memory required for data: 131989440
I0822 23:34:25.887233  2111 layer_factory.hpp:77] Creating layer dense_layer_relu4
I0822 23:34:25.887238  2111 net.cpp:106] Creating Layer dense_layer_relu4
I0822 23:34:25.887241  2111 net.cpp:454] dense_layer_relu4 <- dense_layer_bn4
I0822 23:34:25.887245  2111 net.cpp:397] dense_layer_relu4 -> dense_layer_bn4 (in-place)
I0822 23:34:25.887249  2111 net.cpp:150] Setting up dense_layer_relu4
I0822 23:34:25.887253  2111 net.cpp:157] Top shape: 80 512 8 1 (327680)
I0822 23:34:25.887255  2111 net.cpp:165] Memory required for data: 133300160
I0822 23:34:25.887257  2111 layer_factory.hpp:77] Creating layer attention_layer_conv0
I0822 23:34:25.887264  2111 net.cpp:106] Creating Layer attention_layer_conv0
I0822 23:34:25.887266  2111 net.cpp:454] attention_layer_conv0 <- dense_layer_bn4
I0822 23:34:25.887269  2111 net.cpp:411] attention_layer_conv0 -> attention_layer_conv0
I0822 23:34:25.888211  2111 net.cpp:150] Setting up attention_layer_conv0
I0822 23:34:25.888218  2111 net.cpp:157] Top shape: 80 512 8 1 (327680)
I0822 23:34:25.888222  2111 net.cpp:165] Memory required for data: 134610880
I0822 23:34:25.888226  2111 layer_factory.hpp:77] Creating layer attention_layer_bn0
I0822 23:34:25.888231  2111 net.cpp:106] Creating Layer attention_layer_bn0
I0822 23:34:25.888233  2111 net.cpp:454] attention_layer_bn0 <- attention_layer_conv0
I0822 23:34:25.888237  2111 net.cpp:411] attention_layer_bn0 -> attention_layer_bn0
I0822 23:34:25.888360  2111 net.cpp:150] Setting up attention_layer_bn0
I0822 23:34:25.888365  2111 net.cpp:157] Top shape: 80 512 8 1 (327680)
I0822 23:34:25.888367  2111 net.cpp:165] Memory required for data: 135921600
I0822 23:34:25.888372  2111 layer_factory.hpp:77] Creating layer attention_layer_relu0
I0822 23:34:25.888376  2111 net.cpp:106] Creating Layer attention_layer_relu0
I0822 23:34:25.888379  2111 net.cpp:454] attention_layer_relu0 <- attention_layer_bn0
I0822 23:34:25.888382  2111 net.cpp:397] attention_layer_relu0 -> attention_layer_bn0 (in-place)
I0822 23:34:25.888387  2111 net.cpp:150] Setting up attention_layer_relu0
I0822 23:34:25.888389  2111 net.cpp:157] Top shape: 80 512 8 1 (327680)
I0822 23:34:25.888392  2111 net.cpp:165] Memory required for data: 137232320
I0822 23:34:25.888394  2111 layer_factory.hpp:77] Creating layer attention_layer_conv1
I0822 23:34:25.888399  2111 net.cpp:106] Creating Layer attention_layer_conv1
I0822 23:34:25.888402  2111 net.cpp:454] attention_layer_conv1 <- attention_layer_bn0
I0822 23:34:25.888406  2111 net.cpp:411] attention_layer_conv1 -> attention_layer_conv1
I0822 23:34:25.889344  2111 net.cpp:150] Setting up attention_layer_conv1
I0822 23:34:25.889353  2111 net.cpp:157] Top shape: 80 512 8 1 (327680)
I0822 23:34:25.889360  2111 net.cpp:165] Memory required for data: 138543040
I0822 23:34:25.889364  2111 layer_factory.hpp:77] Creating layer attention_layer_bn1
I0822 23:34:25.889369  2111 net.cpp:106] Creating Layer attention_layer_bn1
I0822 23:34:25.889372  2111 net.cpp:454] attention_layer_bn1 <- attention_layer_conv1
I0822 23:34:25.889376  2111 net.cpp:411] attention_layer_bn1 -> attention_layer_bn1
I0822 23:34:25.889508  2111 net.cpp:150] Setting up attention_layer_bn1
I0822 23:34:25.889513  2111 net.cpp:157] Top shape: 80 512 8 1 (327680)
I0822 23:34:25.889515  2111 net.cpp:165] Memory required for data: 139853760
I0822 23:34:25.889520  2111 layer_factory.hpp:77] Creating layer attention_layer_relu1
I0822 23:34:25.889525  2111 net.cpp:106] Creating Layer attention_layer_relu1
I0822 23:34:25.889528  2111 net.cpp:454] attention_layer_relu1 <- attention_layer_bn1
I0822 23:34:25.889531  2111 net.cpp:397] attention_layer_relu1 -> attention_layer_bn1 (in-place)
I0822 23:34:25.889535  2111 net.cpp:150] Setting up attention_layer_relu1
I0822 23:34:25.889539  2111 net.cpp:157] Top shape: 80 512 8 1 (327680)
I0822 23:34:25.889541  2111 net.cpp:165] Memory required for data: 141164480
I0822 23:34:25.889544  2111 layer_factory.hpp:77] Creating layer attention_scale_slice
I0822 23:34:25.889549  2111 net.cpp:106] Creating Layer attention_scale_slice
I0822 23:34:25.889552  2111 net.cpp:454] attention_scale_slice <- attention_layer_bn1
I0822 23:34:25.889555  2111 net.cpp:411] attention_scale_slice -> attention_scale0
I0822 23:34:25.889560  2111 net.cpp:411] attention_scale_slice -> attention_scale1
I0822 23:34:25.889565  2111 net.cpp:411] attention_scale_slice -> attention_scale2
I0822 23:34:25.889569  2111 net.cpp:411] attention_scale_slice -> attention_scale3
I0822 23:34:25.889573  2111 net.cpp:411] attention_scale_slice -> attention_scale4
I0822 23:34:25.889578  2111 net.cpp:411] attention_scale_slice -> attention_scale5
I0822 23:34:25.889582  2111 net.cpp:411] attention_scale_slice -> attention_scale6
I0822 23:34:25.889586  2111 net.cpp:411] attention_scale_slice -> attention_scale7
I0822 23:34:25.889669  2111 net.cpp:150] Setting up attention_scale_slice
I0822 23:34:25.889678  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889680  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889683  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889685  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889688  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889691  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889694  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889698  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889699  2111 net.cpp:165] Memory required for data: 142475200
I0822 23:34:25.889701  2111 layer_factory.hpp:77] Creating layer attention_scale0_attention_scale_slice_0_split
I0822 23:34:25.889706  2111 net.cpp:106] Creating Layer attention_scale0_attention_scale_slice_0_split
I0822 23:34:25.889708  2111 net.cpp:454] attention_scale0_attention_scale_slice_0_split <- attention_scale0
I0822 23:34:25.889714  2111 net.cpp:411] attention_scale0_attention_scale_slice_0_split -> attention_scale0_attention_scale_slice_0_split_0
I0822 23:34:25.889717  2111 net.cpp:411] attention_scale0_attention_scale_slice_0_split -> attention_scale0_attention_scale_slice_0_split_1
I0822 23:34:25.889739  2111 net.cpp:150] Setting up attention_scale0_attention_scale_slice_0_split
I0822 23:34:25.889744  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889747  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889750  2111 net.cpp:165] Memory required for data: 142802880
I0822 23:34:25.889752  2111 layer_factory.hpp:77] Creating layer attention_scale1_attention_scale_slice_1_split
I0822 23:34:25.889756  2111 net.cpp:106] Creating Layer attention_scale1_attention_scale_slice_1_split
I0822 23:34:25.889758  2111 net.cpp:454] attention_scale1_attention_scale_slice_1_split <- attention_scale1
I0822 23:34:25.889763  2111 net.cpp:411] attention_scale1_attention_scale_slice_1_split -> attention_scale1_attention_scale_slice_1_split_0
I0822 23:34:25.889767  2111 net.cpp:411] attention_scale1_attention_scale_slice_1_split -> attention_scale1_attention_scale_slice_1_split_1
I0822 23:34:25.889787  2111 net.cpp:150] Setting up attention_scale1_attention_scale_slice_1_split
I0822 23:34:25.889792  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889796  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889797  2111 net.cpp:165] Memory required for data: 143130560
I0822 23:34:25.889801  2111 layer_factory.hpp:77] Creating layer attention_scale2_attention_scale_slice_2_split
I0822 23:34:25.889804  2111 net.cpp:106] Creating Layer attention_scale2_attention_scale_slice_2_split
I0822 23:34:25.889806  2111 net.cpp:454] attention_scale2_attention_scale_slice_2_split <- attention_scale2
I0822 23:34:25.889811  2111 net.cpp:411] attention_scale2_attention_scale_slice_2_split -> attention_scale2_attention_scale_slice_2_split_0
I0822 23:34:25.889813  2111 net.cpp:411] attention_scale2_attention_scale_slice_2_split -> attention_scale2_attention_scale_slice_2_split_1
I0822 23:34:25.889833  2111 net.cpp:150] Setting up attention_scale2_attention_scale_slice_2_split
I0822 23:34:25.889838  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889842  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889850  2111 net.cpp:165] Memory required for data: 143458240
I0822 23:34:25.889853  2111 layer_factory.hpp:77] Creating layer attention_scale3_attention_scale_slice_3_split
I0822 23:34:25.889856  2111 net.cpp:106] Creating Layer attention_scale3_attention_scale_slice_3_split
I0822 23:34:25.889859  2111 net.cpp:454] attention_scale3_attention_scale_slice_3_split <- attention_scale3
I0822 23:34:25.889863  2111 net.cpp:411] attention_scale3_attention_scale_slice_3_split -> attention_scale3_attention_scale_slice_3_split_0
I0822 23:34:25.889866  2111 net.cpp:411] attention_scale3_attention_scale_slice_3_split -> attention_scale3_attention_scale_slice_3_split_1
I0822 23:34:25.889889  2111 net.cpp:150] Setting up attention_scale3_attention_scale_slice_3_split
I0822 23:34:25.889892  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889895  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889897  2111 net.cpp:165] Memory required for data: 143785920
I0822 23:34:25.889899  2111 layer_factory.hpp:77] Creating layer attention_scale4_attention_scale_slice_4_split
I0822 23:34:25.889904  2111 net.cpp:106] Creating Layer attention_scale4_attention_scale_slice_4_split
I0822 23:34:25.889905  2111 net.cpp:454] attention_scale4_attention_scale_slice_4_split <- attention_scale4
I0822 23:34:25.889909  2111 net.cpp:411] attention_scale4_attention_scale_slice_4_split -> attention_scale4_attention_scale_slice_4_split_0
I0822 23:34:25.889912  2111 net.cpp:411] attention_scale4_attention_scale_slice_4_split -> attention_scale4_attention_scale_slice_4_split_1
I0822 23:34:25.889936  2111 net.cpp:150] Setting up attention_scale4_attention_scale_slice_4_split
I0822 23:34:25.889940  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889943  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889946  2111 net.cpp:165] Memory required for data: 144113600
I0822 23:34:25.889948  2111 layer_factory.hpp:77] Creating layer attention_scale5_attention_scale_slice_5_split
I0822 23:34:25.889952  2111 net.cpp:106] Creating Layer attention_scale5_attention_scale_slice_5_split
I0822 23:34:25.889955  2111 net.cpp:454] attention_scale5_attention_scale_slice_5_split <- attention_scale5
I0822 23:34:25.889958  2111 net.cpp:411] attention_scale5_attention_scale_slice_5_split -> attention_scale5_attention_scale_slice_5_split_0
I0822 23:34:25.889961  2111 net.cpp:411] attention_scale5_attention_scale_slice_5_split -> attention_scale5_attention_scale_slice_5_split_1
I0822 23:34:25.889981  2111 net.cpp:150] Setting up attention_scale5_attention_scale_slice_5_split
I0822 23:34:25.889984  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889987  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.889989  2111 net.cpp:165] Memory required for data: 144441280
I0822 23:34:25.889991  2111 layer_factory.hpp:77] Creating layer attention_scale6_attention_scale_slice_6_split
I0822 23:34:25.889994  2111 net.cpp:106] Creating Layer attention_scale6_attention_scale_slice_6_split
I0822 23:34:25.889997  2111 net.cpp:454] attention_scale6_attention_scale_slice_6_split <- attention_scale6
I0822 23:34:25.890000  2111 net.cpp:411] attention_scale6_attention_scale_slice_6_split -> attention_scale6_attention_scale_slice_6_split_0
I0822 23:34:25.890003  2111 net.cpp:411] attention_scale6_attention_scale_slice_6_split -> attention_scale6_attention_scale_slice_6_split_1
I0822 23:34:25.890027  2111 net.cpp:150] Setting up attention_scale6_attention_scale_slice_6_split
I0822 23:34:25.890031  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.890034  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.890036  2111 net.cpp:165] Memory required for data: 144768960
I0822 23:34:25.890038  2111 layer_factory.hpp:77] Creating layer attention_scale7_attention_scale_slice_7_split
I0822 23:34:25.890043  2111 net.cpp:106] Creating Layer attention_scale7_attention_scale_slice_7_split
I0822 23:34:25.890044  2111 net.cpp:454] attention_scale7_attention_scale_slice_7_split <- attention_scale7
I0822 23:34:25.890048  2111 net.cpp:411] attention_scale7_attention_scale_slice_7_split -> attention_scale7_attention_scale_slice_7_split_0
I0822 23:34:25.890053  2111 net.cpp:411] attention_scale7_attention_scale_slice_7_split -> attention_scale7_attention_scale_slice_7_split_1
I0822 23:34:25.890072  2111 net.cpp:150] Setting up attention_scale7_attention_scale_slice_7_split
I0822 23:34:25.890075  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.890079  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.890080  2111 net.cpp:165] Memory required for data: 145096640
I0822 23:34:25.890082  2111 layer_factory.hpp:77] Creating layer attention_permute0
I0822 23:34:25.890087  2111 net.cpp:106] Creating Layer attention_permute0
I0822 23:34:25.890091  2111 net.cpp:454] attention_permute0 <- attention_scale0_attention_scale_slice_0_split_0
I0822 23:34:25.890095  2111 net.cpp:411] attention_permute0 -> attention_permute0
I0822 23:34:25.890153  2111 net.cpp:150] Setting up attention_permute0
I0822 23:34:25.890157  2111 net.cpp:157] Top shape: 80 1 8 64 (40960)
I0822 23:34:25.890159  2111 net.cpp:165] Memory required for data: 145260480
I0822 23:34:25.890161  2111 layer_factory.hpp:77] Creating layer attention_reduction0
I0822 23:34:25.890166  2111 net.cpp:106] Creating Layer attention_reduction0
I0822 23:34:25.890169  2111 net.cpp:454] attention_reduction0 <- attention_permute0
I0822 23:34:25.890172  2111 net.cpp:411] attention_reduction0 -> attention_reduction0
I0822 23:34:25.890215  2111 net.cpp:150] Setting up attention_reduction0
I0822 23:34:25.890219  2111 net.cpp:157] Top shape: 80 1 8 (640)
I0822 23:34:25.890233  2111 net.cpp:165] Memory required for data: 145263040
I0822 23:34:25.890234  2111 layer_factory.hpp:77] Creating layer attention_permute1
I0822 23:34:25.890239  2111 net.cpp:106] Creating Layer attention_permute1
I0822 23:34:25.890241  2111 net.cpp:454] attention_permute1 <- attention_scale1_attention_scale_slice_1_split_0
I0822 23:34:25.890244  2111 net.cpp:411] attention_permute1 -> attention_permute1
I0822 23:34:25.890301  2111 net.cpp:150] Setting up attention_permute1
I0822 23:34:25.890305  2111 net.cpp:157] Top shape: 80 1 8 64 (40960)
I0822 23:34:25.890307  2111 net.cpp:165] Memory required for data: 145426880
I0822 23:34:25.890310  2111 layer_factory.hpp:77] Creating layer attention_reduction1
I0822 23:34:25.890313  2111 net.cpp:106] Creating Layer attention_reduction1
I0822 23:34:25.890316  2111 net.cpp:454] attention_reduction1 <- attention_permute1
I0822 23:34:25.890318  2111 net.cpp:411] attention_reduction1 -> attention_reduction1
I0822 23:34:25.890350  2111 net.cpp:150] Setting up attention_reduction1
I0822 23:34:25.890354  2111 net.cpp:157] Top shape: 80 1 8 (640)
I0822 23:34:25.890357  2111 net.cpp:165] Memory required for data: 145429440
I0822 23:34:25.890359  2111 layer_factory.hpp:77] Creating layer attention_permute2
I0822 23:34:25.890362  2111 net.cpp:106] Creating Layer attention_permute2
I0822 23:34:25.890365  2111 net.cpp:454] attention_permute2 <- attention_scale2_attention_scale_slice_2_split_0
I0822 23:34:25.890368  2111 net.cpp:411] attention_permute2 -> attention_permute2
I0822 23:34:25.890425  2111 net.cpp:150] Setting up attention_permute2
I0822 23:34:25.890429  2111 net.cpp:157] Top shape: 80 1 8 64 (40960)
I0822 23:34:25.890431  2111 net.cpp:165] Memory required for data: 145593280
I0822 23:34:25.890434  2111 layer_factory.hpp:77] Creating layer attention_reduction2
I0822 23:34:25.890437  2111 net.cpp:106] Creating Layer attention_reduction2
I0822 23:34:25.890439  2111 net.cpp:454] attention_reduction2 <- attention_permute2
I0822 23:34:25.890444  2111 net.cpp:411] attention_reduction2 -> attention_reduction2
I0822 23:34:25.890472  2111 net.cpp:150] Setting up attention_reduction2
I0822 23:34:25.890476  2111 net.cpp:157] Top shape: 80 1 8 (640)
I0822 23:34:25.890478  2111 net.cpp:165] Memory required for data: 145595840
I0822 23:34:25.890480  2111 layer_factory.hpp:77] Creating layer attention_permute3
I0822 23:34:25.890485  2111 net.cpp:106] Creating Layer attention_permute3
I0822 23:34:25.890487  2111 net.cpp:454] attention_permute3 <- attention_scale3_attention_scale_slice_3_split_0
I0822 23:34:25.890491  2111 net.cpp:411] attention_permute3 -> attention_permute3
I0822 23:34:25.890548  2111 net.cpp:150] Setting up attention_permute3
I0822 23:34:25.890552  2111 net.cpp:157] Top shape: 80 1 8 64 (40960)
I0822 23:34:25.890554  2111 net.cpp:165] Memory required for data: 145759680
I0822 23:34:25.890558  2111 layer_factory.hpp:77] Creating layer attention_reduction3
I0822 23:34:25.890565  2111 net.cpp:106] Creating Layer attention_reduction3
I0822 23:34:25.890568  2111 net.cpp:454] attention_reduction3 <- attention_permute3
I0822 23:34:25.890571  2111 net.cpp:411] attention_reduction3 -> attention_reduction3
I0822 23:34:25.890605  2111 net.cpp:150] Setting up attention_reduction3
I0822 23:34:25.890609  2111 net.cpp:157] Top shape: 80 1 8 (640)
I0822 23:34:25.890611  2111 net.cpp:165] Memory required for data: 145762240
I0822 23:34:25.890614  2111 layer_factory.hpp:77] Creating layer attention_permute4
I0822 23:34:25.890619  2111 net.cpp:106] Creating Layer attention_permute4
I0822 23:34:25.890620  2111 net.cpp:454] attention_permute4 <- attention_scale4_attention_scale_slice_4_split_0
I0822 23:34:25.890625  2111 net.cpp:411] attention_permute4 -> attention_permute4
I0822 23:34:25.890681  2111 net.cpp:150] Setting up attention_permute4
I0822 23:34:25.890684  2111 net.cpp:157] Top shape: 80 1 8 64 (40960)
I0822 23:34:25.890686  2111 net.cpp:165] Memory required for data: 145926080
I0822 23:34:25.890688  2111 layer_factory.hpp:77] Creating layer attention_reduction4
I0822 23:34:25.890692  2111 net.cpp:106] Creating Layer attention_reduction4
I0822 23:34:25.890694  2111 net.cpp:454] attention_reduction4 <- attention_permute4
I0822 23:34:25.890698  2111 net.cpp:411] attention_reduction4 -> attention_reduction4
I0822 23:34:25.890738  2111 net.cpp:150] Setting up attention_reduction4
I0822 23:34:25.890743  2111 net.cpp:157] Top shape: 80 1 8 (640)
I0822 23:34:25.890744  2111 net.cpp:165] Memory required for data: 145928640
I0822 23:34:25.890746  2111 layer_factory.hpp:77] Creating layer attention_permute5
I0822 23:34:25.890750  2111 net.cpp:106] Creating Layer attention_permute5
I0822 23:34:25.890753  2111 net.cpp:454] attention_permute5 <- attention_scale5_attention_scale_slice_5_split_0
I0822 23:34:25.890756  2111 net.cpp:411] attention_permute5 -> attention_permute5
I0822 23:34:25.890810  2111 net.cpp:150] Setting up attention_permute5
I0822 23:34:25.890813  2111 net.cpp:157] Top shape: 80 1 8 64 (40960)
I0822 23:34:25.890815  2111 net.cpp:165] Memory required for data: 146092480
I0822 23:34:25.890817  2111 layer_factory.hpp:77] Creating layer attention_reduction5
I0822 23:34:25.890821  2111 net.cpp:106] Creating Layer attention_reduction5
I0822 23:34:25.890825  2111 net.cpp:454] attention_reduction5 <- attention_permute5
I0822 23:34:25.890828  2111 net.cpp:411] attention_reduction5 -> attention_reduction5
I0822 23:34:25.890863  2111 net.cpp:150] Setting up attention_reduction5
I0822 23:34:25.890868  2111 net.cpp:157] Top shape: 80 1 8 (640)
I0822 23:34:25.890871  2111 net.cpp:165] Memory required for data: 146095040
I0822 23:34:25.890872  2111 layer_factory.hpp:77] Creating layer attention_permute6
I0822 23:34:25.890875  2111 net.cpp:106] Creating Layer attention_permute6
I0822 23:34:25.890878  2111 net.cpp:454] attention_permute6 <- attention_scale6_attention_scale_slice_6_split_0
I0822 23:34:25.890882  2111 net.cpp:411] attention_permute6 -> attention_permute6
I0822 23:34:25.890946  2111 net.cpp:150] Setting up attention_permute6
I0822 23:34:25.890950  2111 net.cpp:157] Top shape: 80 1 8 64 (40960)
I0822 23:34:25.890952  2111 net.cpp:165] Memory required for data: 146258880
I0822 23:34:25.890954  2111 layer_factory.hpp:77] Creating layer attention_reduction6
I0822 23:34:25.890959  2111 net.cpp:106] Creating Layer attention_reduction6
I0822 23:34:25.890960  2111 net.cpp:454] attention_reduction6 <- attention_permute6
I0822 23:34:25.890964  2111 net.cpp:411] attention_reduction6 -> attention_reduction6
I0822 23:34:25.891757  2111 net.cpp:150] Setting up attention_reduction6
I0822 23:34:25.891769  2111 net.cpp:157] Top shape: 80 1 8 (640)
I0822 23:34:25.891772  2111 net.cpp:165] Memory required for data: 146261440
I0822 23:34:25.891775  2111 layer_factory.hpp:77] Creating layer attention_permute7
I0822 23:34:25.891780  2111 net.cpp:106] Creating Layer attention_permute7
I0822 23:34:25.891783  2111 net.cpp:454] attention_permute7 <- attention_scale7_attention_scale_slice_7_split_0
I0822 23:34:25.891788  2111 net.cpp:411] attention_permute7 -> attention_permute7
I0822 23:34:25.891850  2111 net.cpp:150] Setting up attention_permute7
I0822 23:34:25.891855  2111 net.cpp:157] Top shape: 80 1 8 64 (40960)
I0822 23:34:25.891857  2111 net.cpp:165] Memory required for data: 146425280
I0822 23:34:25.891860  2111 layer_factory.hpp:77] Creating layer attention_reduction7
I0822 23:34:25.891863  2111 net.cpp:106] Creating Layer attention_reduction7
I0822 23:34:25.891866  2111 net.cpp:454] attention_reduction7 <- attention_permute7
I0822 23:34:25.891870  2111 net.cpp:411] attention_reduction7 -> attention_reduction7
I0822 23:34:25.891901  2111 net.cpp:150] Setting up attention_reduction7
I0822 23:34:25.891906  2111 net.cpp:157] Top shape: 80 1 8 (640)
I0822 23:34:25.891907  2111 net.cpp:165] Memory required for data: 146427840
I0822 23:34:25.891911  2111 layer_factory.hpp:77] Creating layer attention_scale_concat
I0822 23:34:25.891916  2111 net.cpp:106] Creating Layer attention_scale_concat
I0822 23:34:25.891917  2111 net.cpp:454] attention_scale_concat <- attention_reduction0
I0822 23:34:25.891921  2111 net.cpp:454] attention_scale_concat <- attention_reduction1
I0822 23:34:25.891923  2111 net.cpp:454] attention_scale_concat <- attention_reduction2
I0822 23:34:25.891927  2111 net.cpp:454] attention_scale_concat <- attention_reduction3
I0822 23:34:25.891928  2111 net.cpp:454] attention_scale_concat <- attention_reduction4
I0822 23:34:25.891932  2111 net.cpp:454] attention_scale_concat <- attention_reduction5
I0822 23:34:25.891934  2111 net.cpp:454] attention_scale_concat <- attention_reduction6
I0822 23:34:25.891937  2111 net.cpp:454] attention_scale_concat <- attention_reduction7
I0822 23:34:25.891940  2111 net.cpp:411] attention_scale_concat -> attention_scale_concat
I0822 23:34:25.891955  2111 net.cpp:150] Setting up attention_scale_concat
I0822 23:34:25.891959  2111 net.cpp:157] Top shape: 80 8 8 (5120)
I0822 23:34:25.891961  2111 net.cpp:165] Memory required for data: 146448320
I0822 23:34:25.891963  2111 layer_factory.hpp:77] Creating layer attention_height_slice
I0822 23:34:25.891968  2111 net.cpp:106] Creating Layer attention_height_slice
I0822 23:34:25.891971  2111 net.cpp:454] attention_height_slice <- attention_scale_concat
I0822 23:34:25.891975  2111 net.cpp:411] attention_height_slice -> attention_height0
I0822 23:34:25.891980  2111 net.cpp:411] attention_height_slice -> attention_height1
I0822 23:34:25.891984  2111 net.cpp:411] attention_height_slice -> attention_height2
I0822 23:34:25.891988  2111 net.cpp:411] attention_height_slice -> attention_height3
I0822 23:34:25.891993  2111 net.cpp:411] attention_height_slice -> attention_height4
I0822 23:34:25.891996  2111 net.cpp:411] attention_height_slice -> attention_height5
I0822 23:34:25.891999  2111 net.cpp:411] attention_height_slice -> attention_height6
I0822 23:34:25.892004  2111 net.cpp:411] attention_height_slice -> attention_height7
I0822 23:34:25.892066  2111 net.cpp:150] Setting up attention_height_slice
I0822 23:34:25.892071  2111 net.cpp:157] Top shape: 80 8 1 (640)
I0822 23:34:25.892072  2111 net.cpp:157] Top shape: 80 8 1 (640)
I0822 23:34:25.892076  2111 net.cpp:157] Top shape: 80 8 1 (640)
I0822 23:34:25.892078  2111 net.cpp:157] Top shape: 80 8 1 (640)
I0822 23:34:25.892081  2111 net.cpp:157] Top shape: 80 8 1 (640)
I0822 23:34:25.892083  2111 net.cpp:157] Top shape: 80 8 1 (640)
I0822 23:34:25.892086  2111 net.cpp:157] Top shape: 80 8 1 (640)
I0822 23:34:25.892089  2111 net.cpp:157] Top shape: 80 8 1 (640)
I0822 23:34:25.892091  2111 net.cpp:165] Memory required for data: 146468800
I0822 23:34:25.892093  2111 layer_factory.hpp:77] Creating layer attention_height_fc1_0
I0822 23:34:25.892099  2111 net.cpp:106] Creating Layer attention_height_fc1_0
I0822 23:34:25.892102  2111 net.cpp:454] attention_height_fc1_0 <- attention_height0
I0822 23:34:25.892105  2111 net.cpp:411] attention_height_fc1_0 -> attention_height_fc1_0
I0822 23:34:25.892182  2111 net.cpp:150] Setting up attention_height_fc1_0
I0822 23:34:25.892187  2111 net.cpp:157] Top shape: 80 64 (5120)
I0822 23:34:25.892189  2111 net.cpp:165] Memory required for data: 146489280
I0822 23:34:25.892194  2111 layer_factory.hpp:77] Creating layer attention_height_fc2_0
I0822 23:34:25.892199  2111 net.cpp:106] Creating Layer attention_height_fc2_0
I0822 23:34:25.892201  2111 net.cpp:454] attention_height_fc2_0 <- attention_height_fc1_0
I0822 23:34:25.892205  2111 net.cpp:411] attention_height_fc2_0 -> attention_height_fc2_0
I0822 23:34:25.892323  2111 net.cpp:150] Setting up attention_height_fc2_0
I0822 23:34:25.892328  2111 net.cpp:157] Top shape: 80 32 (2560)
I0822 23:34:25.892329  2111 net.cpp:165] Memory required for data: 146499520
I0822 23:34:25.892333  2111 layer_factory.hpp:77] Creating layer attention_height_fc3_0
I0822 23:34:25.892338  2111 net.cpp:106] Creating Layer attention_height_fc3_0
I0822 23:34:25.892339  2111 net.cpp:454] attention_height_fc3_0 <- attention_height_fc2_0
I0822 23:34:25.892343  2111 net.cpp:411] attention_height_fc3_0 -> attention_height_fc3_0
I0822 23:34:25.892408  2111 net.cpp:150] Setting up attention_height_fc3_0
I0822 23:34:25.892412  2111 net.cpp:157] Top shape: 80 8 (640)
I0822 23:34:25.892415  2111 net.cpp:165] Memory required for data: 146502080
I0822 23:34:25.892418  2111 layer_factory.hpp:77] Creating layer attention_height_reshape0
I0822 23:34:25.892424  2111 net.cpp:106] Creating Layer attention_height_reshape0
I0822 23:34:25.892426  2111 net.cpp:454] attention_height_reshape0 <- attention_height_fc3_0
I0822 23:34:25.892431  2111 net.cpp:411] attention_height_reshape0 -> attention_height_reshape0
I0822 23:34:25.892447  2111 net.cpp:150] Setting up attention_height_reshape0
I0822 23:34:25.892451  2111 net.cpp:157] Top shape: 80 8 1 1 (640)
I0822 23:34:25.892452  2111 net.cpp:165] Memory required for data: 146504640
I0822 23:34:25.892454  2111 layer_factory.hpp:77] Creating layer attention_height_fc1_1
I0822 23:34:25.892459  2111 net.cpp:106] Creating Layer attention_height_fc1_1
I0822 23:34:25.892462  2111 net.cpp:454] attention_height_fc1_1 <- attention_height1
I0822 23:34:25.892465  2111 net.cpp:411] attention_height_fc1_1 -> attention_height_fc1_1
I0822 23:34:25.892537  2111 net.cpp:150] Setting up attention_height_fc1_1
I0822 23:34:25.892541  2111 net.cpp:157] Top shape: 80 64 (5120)
I0822 23:34:25.892544  2111 net.cpp:165] Memory required for data: 146525120
I0822 23:34:25.892547  2111 layer_factory.hpp:77] Creating layer attention_height_fc2_1
I0822 23:34:25.892551  2111 net.cpp:106] Creating Layer attention_height_fc2_1
I0822 23:34:25.892554  2111 net.cpp:454] attention_height_fc2_1 <- attention_height_fc1_1
I0822 23:34:25.892558  2111 net.cpp:411] attention_height_fc2_1 -> attention_height_fc2_1
I0822 23:34:25.892663  2111 net.cpp:150] Setting up attention_height_fc2_1
I0822 23:34:25.892668  2111 net.cpp:157] Top shape: 80 32 (2560)
I0822 23:34:25.892669  2111 net.cpp:165] Memory required for data: 146535360
I0822 23:34:25.892673  2111 layer_factory.hpp:77] Creating layer attention_height_fc3_1
I0822 23:34:25.892678  2111 net.cpp:106] Creating Layer attention_height_fc3_1
I0822 23:34:25.892680  2111 net.cpp:454] attention_height_fc3_1 <- attention_height_fc2_1
I0822 23:34:25.892683  2111 net.cpp:411] attention_height_fc3_1 -> attention_height_fc3_1
I0822 23:34:25.892746  2111 net.cpp:150] Setting up attention_height_fc3_1
I0822 23:34:25.892750  2111 net.cpp:157] Top shape: 80 8 (640)
I0822 23:34:25.892752  2111 net.cpp:165] Memory required for data: 146537920
I0822 23:34:25.892756  2111 layer_factory.hpp:77] Creating layer attention_height_reshape1
I0822 23:34:25.892760  2111 net.cpp:106] Creating Layer attention_height_reshape1
I0822 23:34:25.892762  2111 net.cpp:454] attention_height_reshape1 <- attention_height_fc3_1
I0822 23:34:25.892766  2111 net.cpp:411] attention_height_reshape1 -> attention_height_reshape1
I0822 23:34:25.892781  2111 net.cpp:150] Setting up attention_height_reshape1
I0822 23:34:25.892784  2111 net.cpp:157] Top shape: 80 8 1 1 (640)
I0822 23:34:25.892786  2111 net.cpp:165] Memory required for data: 146540480
I0822 23:34:25.892788  2111 layer_factory.hpp:77] Creating layer attention_height_fc1_2
I0822 23:34:25.892792  2111 net.cpp:106] Creating Layer attention_height_fc1_2
I0822 23:34:25.892794  2111 net.cpp:454] attention_height_fc1_2 <- attention_height2
I0822 23:34:25.892798  2111 net.cpp:411] attention_height_fc1_2 -> attention_height_fc1_2
I0822 23:34:25.892869  2111 net.cpp:150] Setting up attention_height_fc1_2
I0822 23:34:25.892874  2111 net.cpp:157] Top shape: 80 64 (5120)
I0822 23:34:25.892875  2111 net.cpp:165] Memory required for data: 146560960
I0822 23:34:25.892879  2111 layer_factory.hpp:77] Creating layer attention_height_fc2_2
I0822 23:34:25.892884  2111 net.cpp:106] Creating Layer attention_height_fc2_2
I0822 23:34:25.892886  2111 net.cpp:454] attention_height_fc2_2 <- attention_height_fc1_2
I0822 23:34:25.892889  2111 net.cpp:411] attention_height_fc2_2 -> attention_height_fc2_2
I0822 23:34:25.892995  2111 net.cpp:150] Setting up attention_height_fc2_2
I0822 23:34:25.892999  2111 net.cpp:157] Top shape: 80 32 (2560)
I0822 23:34:25.893002  2111 net.cpp:165] Memory required for data: 146571200
I0822 23:34:25.893012  2111 layer_factory.hpp:77] Creating layer attention_height_fc3_2
I0822 23:34:25.893016  2111 net.cpp:106] Creating Layer attention_height_fc3_2
I0822 23:34:25.893019  2111 net.cpp:454] attention_height_fc3_2 <- attention_height_fc2_2
I0822 23:34:25.893023  2111 net.cpp:411] attention_height_fc3_2 -> attention_height_fc3_2
I0822 23:34:25.893090  2111 net.cpp:150] Setting up attention_height_fc3_2
I0822 23:34:25.893093  2111 net.cpp:157] Top shape: 80 8 (640)
I0822 23:34:25.893095  2111 net.cpp:165] Memory required for data: 146573760
I0822 23:34:25.893100  2111 layer_factory.hpp:77] Creating layer attention_height_reshape2
I0822 23:34:25.893103  2111 net.cpp:106] Creating Layer attention_height_reshape2
I0822 23:34:25.893106  2111 net.cpp:454] attention_height_reshape2 <- attention_height_fc3_2
I0822 23:34:25.893110  2111 net.cpp:411] attention_height_reshape2 -> attention_height_reshape2
I0822 23:34:25.893124  2111 net.cpp:150] Setting up attention_height_reshape2
I0822 23:34:25.893139  2111 net.cpp:157] Top shape: 80 8 1 1 (640)
I0822 23:34:25.893141  2111 net.cpp:165] Memory required for data: 146576320
I0822 23:34:25.893143  2111 layer_factory.hpp:77] Creating layer attention_height_fc1_3
I0822 23:34:25.893147  2111 net.cpp:106] Creating Layer attention_height_fc1_3
I0822 23:34:25.893151  2111 net.cpp:454] attention_height_fc1_3 <- attention_height3
I0822 23:34:25.893153  2111 net.cpp:411] attention_height_fc1_3 -> attention_height_fc1_3
I0822 23:34:25.893234  2111 net.cpp:150] Setting up attention_height_fc1_3
I0822 23:34:25.893239  2111 net.cpp:157] Top shape: 80 64 (5120)
I0822 23:34:25.893240  2111 net.cpp:165] Memory required for data: 146596800
I0822 23:34:25.893244  2111 layer_factory.hpp:77] Creating layer attention_height_fc2_3
I0822 23:34:25.893249  2111 net.cpp:106] Creating Layer attention_height_fc2_3
I0822 23:34:25.893251  2111 net.cpp:454] attention_height_fc2_3 <- attention_height_fc1_3
I0822 23:34:25.893255  2111 net.cpp:411] attention_height_fc2_3 -> attention_height_fc2_3
I0822 23:34:25.893365  2111 net.cpp:150] Setting up attention_height_fc2_3
I0822 23:34:25.893369  2111 net.cpp:157] Top shape: 80 32 (2560)
I0822 23:34:25.893371  2111 net.cpp:165] Memory required for data: 146607040
I0822 23:34:25.893375  2111 layer_factory.hpp:77] Creating layer attention_height_fc3_3
I0822 23:34:25.893380  2111 net.cpp:106] Creating Layer attention_height_fc3_3
I0822 23:34:25.893383  2111 net.cpp:454] attention_height_fc3_3 <- attention_height_fc2_3
I0822 23:34:25.893386  2111 net.cpp:411] attention_height_fc3_3 -> attention_height_fc3_3
I0822 23:34:25.893451  2111 net.cpp:150] Setting up attention_height_fc3_3
I0822 23:34:25.893455  2111 net.cpp:157] Top shape: 80 8 (640)
I0822 23:34:25.893457  2111 net.cpp:165] Memory required for data: 146609600
I0822 23:34:25.893461  2111 layer_factory.hpp:77] Creating layer attention_height_reshape3
I0822 23:34:25.893465  2111 net.cpp:106] Creating Layer attention_height_reshape3
I0822 23:34:25.893468  2111 net.cpp:454] attention_height_reshape3 <- attention_height_fc3_3
I0822 23:34:25.893471  2111 net.cpp:411] attention_height_reshape3 -> attention_height_reshape3
I0822 23:34:25.893486  2111 net.cpp:150] Setting up attention_height_reshape3
I0822 23:34:25.893491  2111 net.cpp:157] Top shape: 80 8 1 1 (640)
I0822 23:34:25.893492  2111 net.cpp:165] Memory required for data: 146612160
I0822 23:34:25.893494  2111 layer_factory.hpp:77] Creating layer attention_height_fc1_4
I0822 23:34:25.893498  2111 net.cpp:106] Creating Layer attention_height_fc1_4
I0822 23:34:25.893501  2111 net.cpp:454] attention_height_fc1_4 <- attention_height4
I0822 23:34:25.893504  2111 net.cpp:411] attention_height_fc1_4 -> attention_height_fc1_4
I0822 23:34:25.893586  2111 net.cpp:150] Setting up attention_height_fc1_4
I0822 23:34:25.893590  2111 net.cpp:157] Top shape: 80 64 (5120)
I0822 23:34:25.893592  2111 net.cpp:165] Memory required for data: 146632640
I0822 23:34:25.893596  2111 layer_factory.hpp:77] Creating layer attention_height_fc2_4
I0822 23:34:25.893600  2111 net.cpp:106] Creating Layer attention_height_fc2_4
I0822 23:34:25.893602  2111 net.cpp:454] attention_height_fc2_4 <- attention_height_fc1_4
I0822 23:34:25.893606  2111 net.cpp:411] attention_height_fc2_4 -> attention_height_fc2_4
I0822 23:34:25.893712  2111 net.cpp:150] Setting up attention_height_fc2_4
I0822 23:34:25.893715  2111 net.cpp:157] Top shape: 80 32 (2560)
I0822 23:34:25.893718  2111 net.cpp:165] Memory required for data: 146642880
I0822 23:34:25.893721  2111 layer_factory.hpp:77] Creating layer attention_height_fc3_4
I0822 23:34:25.893725  2111 net.cpp:106] Creating Layer attention_height_fc3_4
I0822 23:34:25.893728  2111 net.cpp:454] attention_height_fc3_4 <- attention_height_fc2_4
I0822 23:34:25.893731  2111 net.cpp:411] attention_height_fc3_4 -> attention_height_fc3_4
I0822 23:34:25.893795  2111 net.cpp:150] Setting up attention_height_fc3_4
I0822 23:34:25.893800  2111 net.cpp:157] Top shape: 80 8 (640)
I0822 23:34:25.893801  2111 net.cpp:165] Memory required for data: 146645440
I0822 23:34:25.893805  2111 layer_factory.hpp:77] Creating layer attention_height_reshape4
I0822 23:34:25.893810  2111 net.cpp:106] Creating Layer attention_height_reshape4
I0822 23:34:25.893811  2111 net.cpp:454] attention_height_reshape4 <- attention_height_fc3_4
I0822 23:34:25.893815  2111 net.cpp:411] attention_height_reshape4 -> attention_height_reshape4
I0822 23:34:25.893829  2111 net.cpp:150] Setting up attention_height_reshape4
I0822 23:34:25.893832  2111 net.cpp:157] Top shape: 80 8 1 1 (640)
I0822 23:34:25.893834  2111 net.cpp:165] Memory required for data: 146648000
I0822 23:34:25.893836  2111 layer_factory.hpp:77] Creating layer attention_height_fc1_5
I0822 23:34:25.893841  2111 net.cpp:106] Creating Layer attention_height_fc1_5
I0822 23:34:25.893843  2111 net.cpp:454] attention_height_fc1_5 <- attention_height5
I0822 23:34:25.893847  2111 net.cpp:411] attention_height_fc1_5 -> attention_height_fc1_5
I0822 23:34:25.893918  2111 net.cpp:150] Setting up attention_height_fc1_5
I0822 23:34:25.893921  2111 net.cpp:157] Top shape: 80 64 (5120)
I0822 23:34:25.893923  2111 net.cpp:165] Memory required for data: 146668480
I0822 23:34:25.893927  2111 layer_factory.hpp:77] Creating layer attention_height_fc2_5
I0822 23:34:25.893931  2111 net.cpp:106] Creating Layer attention_height_fc2_5
I0822 23:34:25.893934  2111 net.cpp:454] attention_height_fc2_5 <- attention_height_fc1_5
I0822 23:34:25.893937  2111 net.cpp:411] attention_height_fc2_5 -> attention_height_fc2_5
I0822 23:34:25.894057  2111 net.cpp:150] Setting up attention_height_fc2_5
I0822 23:34:25.894060  2111 net.cpp:157] Top shape: 80 32 (2560)
I0822 23:34:25.894062  2111 net.cpp:165] Memory required for data: 146678720
I0822 23:34:25.894065  2111 layer_factory.hpp:77] Creating layer attention_height_fc3_5
I0822 23:34:25.894070  2111 net.cpp:106] Creating Layer attention_height_fc3_5
I0822 23:34:25.894073  2111 net.cpp:454] attention_height_fc3_5 <- attention_height_fc2_5
I0822 23:34:25.894076  2111 net.cpp:411] attention_height_fc3_5 -> attention_height_fc3_5
I0822 23:34:25.894140  2111 net.cpp:150] Setting up attention_height_fc3_5
I0822 23:34:25.894145  2111 net.cpp:157] Top shape: 80 8 (640)
I0822 23:34:25.894146  2111 net.cpp:165] Memory required for data: 146681280
I0822 23:34:25.894150  2111 layer_factory.hpp:77] Creating layer attention_height_reshape5
I0822 23:34:25.894153  2111 net.cpp:106] Creating Layer attention_height_reshape5
I0822 23:34:25.894156  2111 net.cpp:454] attention_height_reshape5 <- attention_height_fc3_5
I0822 23:34:25.894160  2111 net.cpp:411] attention_height_reshape5 -> attention_height_reshape5
I0822 23:34:25.894174  2111 net.cpp:150] Setting up attention_height_reshape5
I0822 23:34:25.894178  2111 net.cpp:157] Top shape: 80 8 1 1 (640)
I0822 23:34:25.894181  2111 net.cpp:165] Memory required for data: 146683840
I0822 23:34:25.894182  2111 layer_factory.hpp:77] Creating layer attention_height_fc1_6
I0822 23:34:25.894186  2111 net.cpp:106] Creating Layer attention_height_fc1_6
I0822 23:34:25.894189  2111 net.cpp:454] attention_height_fc1_6 <- attention_height6
I0822 23:34:25.894192  2111 net.cpp:411] attention_height_fc1_6 -> attention_height_fc1_6
I0822 23:34:25.894274  2111 net.cpp:150] Setting up attention_height_fc1_6
I0822 23:34:25.894279  2111 net.cpp:157] Top shape: 80 64 (5120)
I0822 23:34:25.894280  2111 net.cpp:165] Memory required for data: 146704320
I0822 23:34:25.894284  2111 layer_factory.hpp:77] Creating layer attention_height_fc2_6
I0822 23:34:25.894289  2111 net.cpp:106] Creating Layer attention_height_fc2_6
I0822 23:34:25.894290  2111 net.cpp:454] attention_height_fc2_6 <- attention_height_fc1_6
I0822 23:34:25.894294  2111 net.cpp:411] attention_height_fc2_6 -> attention_height_fc2_6
I0822 23:34:25.894399  2111 net.cpp:150] Setting up attention_height_fc2_6
I0822 23:34:25.894403  2111 net.cpp:157] Top shape: 80 32 (2560)
I0822 23:34:25.894405  2111 net.cpp:165] Memory required for data: 146714560
I0822 23:34:25.894409  2111 layer_factory.hpp:77] Creating layer attention_height_fc3_6
I0822 23:34:25.894413  2111 net.cpp:106] Creating Layer attention_height_fc3_6
I0822 23:34:25.894415  2111 net.cpp:454] attention_height_fc3_6 <- attention_height_fc2_6
I0822 23:34:25.894419  2111 net.cpp:411] attention_height_fc3_6 -> attention_height_fc3_6
I0822 23:34:25.894484  2111 net.cpp:150] Setting up attention_height_fc3_6
I0822 23:34:25.894487  2111 net.cpp:157] Top shape: 80 8 (640)
I0822 23:34:25.894490  2111 net.cpp:165] Memory required for data: 146717120
I0822 23:34:25.894492  2111 layer_factory.hpp:77] Creating layer attention_height_reshape6
I0822 23:34:25.894496  2111 net.cpp:106] Creating Layer attention_height_reshape6
I0822 23:34:25.894500  2111 net.cpp:454] attention_height_reshape6 <- attention_height_fc3_6
I0822 23:34:25.894503  2111 net.cpp:411] attention_height_reshape6 -> attention_height_reshape6
I0822 23:34:25.894517  2111 net.cpp:150] Setting up attention_height_reshape6
I0822 23:34:25.894520  2111 net.cpp:157] Top shape: 80 8 1 1 (640)
I0822 23:34:25.894522  2111 net.cpp:165] Memory required for data: 146719680
I0822 23:34:25.894526  2111 layer_factory.hpp:77] Creating layer attention_height_fc1_7
I0822 23:34:25.894529  2111 net.cpp:106] Creating Layer attention_height_fc1_7
I0822 23:34:25.894531  2111 net.cpp:454] attention_height_fc1_7 <- attention_height7
I0822 23:34:25.894536  2111 net.cpp:411] attention_height_fc1_7 -> attention_height_fc1_7
I0822 23:34:25.894608  2111 net.cpp:150] Setting up attention_height_fc1_7
I0822 23:34:25.894611  2111 net.cpp:157] Top shape: 80 64 (5120)
I0822 23:34:25.894613  2111 net.cpp:165] Memory required for data: 146740160
I0822 23:34:25.894616  2111 layer_factory.hpp:77] Creating layer attention_height_fc2_7
I0822 23:34:25.894621  2111 net.cpp:106] Creating Layer attention_height_fc2_7
I0822 23:34:25.894623  2111 net.cpp:454] attention_height_fc2_7 <- attention_height_fc1_7
I0822 23:34:25.894628  2111 net.cpp:411] attention_height_fc2_7 -> attention_height_fc2_7
I0822 23:34:25.894733  2111 net.cpp:150] Setting up attention_height_fc2_7
I0822 23:34:25.894737  2111 net.cpp:157] Top shape: 80 32 (2560)
I0822 23:34:25.894739  2111 net.cpp:165] Memory required for data: 146750400
I0822 23:34:25.894742  2111 layer_factory.hpp:77] Creating layer attention_height_fc3_7
I0822 23:34:25.894747  2111 net.cpp:106] Creating Layer attention_height_fc3_7
I0822 23:34:25.894749  2111 net.cpp:454] attention_height_fc3_7 <- attention_height_fc2_7
I0822 23:34:25.894753  2111 net.cpp:411] attention_height_fc3_7 -> attention_height_fc3_7
I0822 23:34:25.894816  2111 net.cpp:150] Setting up attention_height_fc3_7
I0822 23:34:25.894820  2111 net.cpp:157] Top shape: 80 8 (640)
I0822 23:34:25.894822  2111 net.cpp:165] Memory required for data: 146752960
I0822 23:34:25.894826  2111 layer_factory.hpp:77] Creating layer attention_height_reshape7
I0822 23:34:25.894830  2111 net.cpp:106] Creating Layer attention_height_reshape7
I0822 23:34:25.894832  2111 net.cpp:454] attention_height_reshape7 <- attention_height_fc3_7
I0822 23:34:25.894835  2111 net.cpp:411] attention_height_reshape7 -> attention_height_reshape7
I0822 23:34:25.894850  2111 net.cpp:150] Setting up attention_height_reshape7
I0822 23:34:25.894853  2111 net.cpp:157] Top shape: 80 8 1 1 (640)
I0822 23:34:25.894855  2111 net.cpp:165] Memory required for data: 146755520
I0822 23:34:25.894858  2111 layer_factory.hpp:77] Creating layer attention_height_concat
I0822 23:34:25.894863  2111 net.cpp:106] Creating Layer attention_height_concat
I0822 23:34:25.894866  2111 net.cpp:454] attention_height_concat <- attention_height_reshape0
I0822 23:34:25.894870  2111 net.cpp:454] attention_height_concat <- attention_height_reshape1
I0822 23:34:25.894871  2111 net.cpp:454] attention_height_concat <- attention_height_reshape2
I0822 23:34:25.894875  2111 net.cpp:454] attention_height_concat <- attention_height_reshape3
I0822 23:34:25.894876  2111 net.cpp:454] attention_height_concat <- attention_height_reshape4
I0822 23:34:25.894879  2111 net.cpp:454] attention_height_concat <- attention_height_reshape5
I0822 23:34:25.894881  2111 net.cpp:454] attention_height_concat <- attention_height_reshape6
I0822 23:34:25.894883  2111 net.cpp:454] attention_height_concat <- attention_height_reshape7
I0822 23:34:25.894887  2111 net.cpp:411] attention_height_concat -> attention_height_concat
I0822 23:34:25.894909  2111 net.cpp:150] Setting up attention_height_concat
I0822 23:34:25.894913  2111 net.cpp:157] Top shape: 80 8 8 1 (5120)
I0822 23:34:25.894917  2111 net.cpp:165] Memory required for data: 146776000
I0822 23:34:25.894918  2111 layer_factory.hpp:77] Creating layer attention_weight
I0822 23:34:25.894922  2111 net.cpp:106] Creating Layer attention_weight
I0822 23:34:25.894925  2111 net.cpp:454] attention_weight <- attention_height_concat
I0822 23:34:25.894929  2111 net.cpp:411] attention_weight -> attention_weight
I0822 23:34:25.894966  2111 net.cpp:150] Setting up attention_weight
I0822 23:34:25.894970  2111 net.cpp:157] Top shape: 80 8 8 1 (5120)
I0822 23:34:25.894973  2111 net.cpp:165] Memory required for data: 146796480
I0822 23:34:25.894974  2111 layer_factory.hpp:77] Creating layer attention_weight_slice
I0822 23:34:25.894980  2111 net.cpp:106] Creating Layer attention_weight_slice
I0822 23:34:25.894982  2111 net.cpp:454] attention_weight_slice <- attention_weight
I0822 23:34:25.894987  2111 net.cpp:411] attention_weight_slice -> attention_weight_slice0
I0822 23:34:25.894991  2111 net.cpp:411] attention_weight_slice -> attention_weight_slice1
I0822 23:34:25.894996  2111 net.cpp:411] attention_weight_slice -> attention_weight_slice2
I0822 23:34:25.895000  2111 net.cpp:411] attention_weight_slice -> attention_weight_slice3
I0822 23:34:25.895004  2111 net.cpp:411] attention_weight_slice -> attention_weight_slice4
I0822 23:34:25.895007  2111 net.cpp:411] attention_weight_slice -> attention_weight_slice5
I0822 23:34:25.895011  2111 net.cpp:411] attention_weight_slice -> attention_weight_slice6
I0822 23:34:25.895015  2111 net.cpp:411] attention_weight_slice -> attention_weight_slice7
I0822 23:34:25.895076  2111 net.cpp:150] Setting up attention_weight_slice
I0822 23:34:25.895079  2111 net.cpp:157] Top shape: 80 1 8 1 (640)
I0822 23:34:25.895082  2111 net.cpp:157] Top shape: 80 1 8 1 (640)
I0822 23:34:25.895085  2111 net.cpp:157] Top shape: 80 1 8 1 (640)
I0822 23:34:25.895087  2111 net.cpp:157] Top shape: 80 1 8 1 (640)
I0822 23:34:25.895090  2111 net.cpp:157] Top shape: 80 1 8 1 (640)
I0822 23:34:25.895093  2111 net.cpp:157] Top shape: 80 1 8 1 (640)
I0822 23:34:25.895095  2111 net.cpp:157] Top shape: 80 1 8 1 (640)
I0822 23:34:25.895098  2111 net.cpp:157] Top shape: 80 1 8 1 (640)
I0822 23:34:25.895100  2111 net.cpp:165] Memory required for data: 146816960
I0822 23:34:25.895102  2111 layer_factory.hpp:77] Creating layer attention_weight_tile0
I0822 23:34:25.895107  2111 net.cpp:106] Creating Layer attention_weight_tile0
I0822 23:34:25.895109  2111 net.cpp:454] attention_weight_tile0 <- attention_weight_slice0
I0822 23:34:25.895113  2111 net.cpp:411] attention_weight_tile0 -> attention_weight_tile0
I0822 23:34:25.895128  2111 net.cpp:150] Setting up attention_weight_tile0
I0822 23:34:25.895131  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895133  2111 net.cpp:165] Memory required for data: 146980800
I0822 23:34:25.895135  2111 layer_factory.hpp:77] Creating layer attention_reweight0
I0822 23:34:25.895140  2111 net.cpp:106] Creating Layer attention_reweight0
I0822 23:34:25.895143  2111 net.cpp:454] attention_reweight0 <- attention_scale0_attention_scale_slice_0_split_1
I0822 23:34:25.895146  2111 net.cpp:454] attention_reweight0 <- attention_weight_tile0
I0822 23:34:25.895149  2111 net.cpp:411] attention_reweight0 -> attention_reweight0
I0822 23:34:25.895164  2111 net.cpp:150] Setting up attention_reweight0
I0822 23:34:25.895167  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895169  2111 net.cpp:165] Memory required for data: 147144640
I0822 23:34:25.895171  2111 layer_factory.hpp:77] Creating layer attention_weight_tile1
I0822 23:34:25.895175  2111 net.cpp:106] Creating Layer attention_weight_tile1
I0822 23:34:25.895177  2111 net.cpp:454] attention_weight_tile1 <- attention_weight_slice1
I0822 23:34:25.895180  2111 net.cpp:411] attention_weight_tile1 -> attention_weight_tile1
I0822 23:34:25.895192  2111 net.cpp:150] Setting up attention_weight_tile1
I0822 23:34:25.895196  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895198  2111 net.cpp:165] Memory required for data: 147308480
I0822 23:34:25.895200  2111 layer_factory.hpp:77] Creating layer attention_reweight1
I0822 23:34:25.895205  2111 net.cpp:106] Creating Layer attention_reweight1
I0822 23:34:25.895206  2111 net.cpp:454] attention_reweight1 <- attention_scale1_attention_scale_slice_1_split_1
I0822 23:34:25.895210  2111 net.cpp:454] attention_reweight1 <- attention_weight_tile1
I0822 23:34:25.895213  2111 net.cpp:411] attention_reweight1 -> attention_reweight1
I0822 23:34:25.895226  2111 net.cpp:150] Setting up attention_reweight1
I0822 23:34:25.895228  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895231  2111 net.cpp:165] Memory required for data: 147472320
I0822 23:34:25.895233  2111 layer_factory.hpp:77] Creating layer attention_weight_tile2
I0822 23:34:25.895236  2111 net.cpp:106] Creating Layer attention_weight_tile2
I0822 23:34:25.895239  2111 net.cpp:454] attention_weight_tile2 <- attention_weight_slice2
I0822 23:34:25.895242  2111 net.cpp:411] attention_weight_tile2 -> attention_weight_tile2
I0822 23:34:25.895254  2111 net.cpp:150] Setting up attention_weight_tile2
I0822 23:34:25.895257  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895259  2111 net.cpp:165] Memory required for data: 147636160
I0822 23:34:25.895262  2111 layer_factory.hpp:77] Creating layer attention_reweight2
I0822 23:34:25.895265  2111 net.cpp:106] Creating Layer attention_reweight2
I0822 23:34:25.895267  2111 net.cpp:454] attention_reweight2 <- attention_scale2_attention_scale_slice_2_split_1
I0822 23:34:25.895272  2111 net.cpp:454] attention_reweight2 <- attention_weight_tile2
I0822 23:34:25.895274  2111 net.cpp:411] attention_reweight2 -> attention_reweight2
I0822 23:34:25.895287  2111 net.cpp:150] Setting up attention_reweight2
I0822 23:34:25.895289  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895292  2111 net.cpp:165] Memory required for data: 147800000
I0822 23:34:25.895294  2111 layer_factory.hpp:77] Creating layer attention_weight_tile3
I0822 23:34:25.895298  2111 net.cpp:106] Creating Layer attention_weight_tile3
I0822 23:34:25.895299  2111 net.cpp:454] attention_weight_tile3 <- attention_weight_slice3
I0822 23:34:25.895303  2111 net.cpp:411] attention_weight_tile3 -> attention_weight_tile3
I0822 23:34:25.895314  2111 net.cpp:150] Setting up attention_weight_tile3
I0822 23:34:25.895318  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895320  2111 net.cpp:165] Memory required for data: 147963840
I0822 23:34:25.895323  2111 layer_factory.hpp:77] Creating layer attention_reweight3
I0822 23:34:25.895326  2111 net.cpp:106] Creating Layer attention_reweight3
I0822 23:34:25.895328  2111 net.cpp:454] attention_reweight3 <- attention_scale3_attention_scale_slice_3_split_1
I0822 23:34:25.895331  2111 net.cpp:454] attention_reweight3 <- attention_weight_tile3
I0822 23:34:25.895334  2111 net.cpp:411] attention_reweight3 -> attention_reweight3
I0822 23:34:25.895347  2111 net.cpp:150] Setting up attention_reweight3
I0822 23:34:25.895351  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895354  2111 net.cpp:165] Memory required for data: 148127680
I0822 23:34:25.895355  2111 layer_factory.hpp:77] Creating layer attention_weight_tile4
I0822 23:34:25.895359  2111 net.cpp:106] Creating Layer attention_weight_tile4
I0822 23:34:25.895360  2111 net.cpp:454] attention_weight_tile4 <- attention_weight_slice4
I0822 23:34:25.895364  2111 net.cpp:411] attention_weight_tile4 -> attention_weight_tile4
I0822 23:34:25.895376  2111 net.cpp:150] Setting up attention_weight_tile4
I0822 23:34:25.895380  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895381  2111 net.cpp:165] Memory required for data: 148291520
I0822 23:34:25.895383  2111 layer_factory.hpp:77] Creating layer attention_reweight4
I0822 23:34:25.895387  2111 net.cpp:106] Creating Layer attention_reweight4
I0822 23:34:25.895390  2111 net.cpp:454] attention_reweight4 <- attention_scale4_attention_scale_slice_4_split_1
I0822 23:34:25.895393  2111 net.cpp:454] attention_reweight4 <- attention_weight_tile4
I0822 23:34:25.895395  2111 net.cpp:411] attention_reweight4 -> attention_reweight4
I0822 23:34:25.895407  2111 net.cpp:150] Setting up attention_reweight4
I0822 23:34:25.895411  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895413  2111 net.cpp:165] Memory required for data: 148455360
I0822 23:34:25.895416  2111 layer_factory.hpp:77] Creating layer attention_weight_tile5
I0822 23:34:25.895418  2111 net.cpp:106] Creating Layer attention_weight_tile5
I0822 23:34:25.895421  2111 net.cpp:454] attention_weight_tile5 <- attention_weight_slice5
I0822 23:34:25.895423  2111 net.cpp:411] attention_weight_tile5 -> attention_weight_tile5
I0822 23:34:25.895436  2111 net.cpp:150] Setting up attention_weight_tile5
I0822 23:34:25.895438  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895440  2111 net.cpp:165] Memory required for data: 148619200
I0822 23:34:25.895442  2111 layer_factory.hpp:77] Creating layer attention_reweight5
I0822 23:34:25.895447  2111 net.cpp:106] Creating Layer attention_reweight5
I0822 23:34:25.895449  2111 net.cpp:454] attention_reweight5 <- attention_scale5_attention_scale_slice_5_split_1
I0822 23:34:25.895452  2111 net.cpp:454] attention_reweight5 <- attention_weight_tile5
I0822 23:34:25.895454  2111 net.cpp:411] attention_reweight5 -> attention_reweight5
I0822 23:34:25.895467  2111 net.cpp:150] Setting up attention_reweight5
I0822 23:34:25.895470  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895473  2111 net.cpp:165] Memory required for data: 148783040
I0822 23:34:25.895474  2111 layer_factory.hpp:77] Creating layer attention_weight_tile6
I0822 23:34:25.895478  2111 net.cpp:106] Creating Layer attention_weight_tile6
I0822 23:34:25.895480  2111 net.cpp:454] attention_weight_tile6 <- attention_weight_slice6
I0822 23:34:25.895483  2111 net.cpp:411] attention_weight_tile6 -> attention_weight_tile6
I0822 23:34:25.895495  2111 net.cpp:150] Setting up attention_weight_tile6
I0822 23:34:25.895498  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895500  2111 net.cpp:165] Memory required for data: 148946880
I0822 23:34:25.895503  2111 layer_factory.hpp:77] Creating layer attention_reweight6
I0822 23:34:25.895506  2111 net.cpp:106] Creating Layer attention_reweight6
I0822 23:34:25.895509  2111 net.cpp:454] attention_reweight6 <- attention_scale6_attention_scale_slice_6_split_1
I0822 23:34:25.895512  2111 net.cpp:454] attention_reweight6 <- attention_weight_tile6
I0822 23:34:25.895515  2111 net.cpp:411] attention_reweight6 -> attention_reweight6
I0822 23:34:25.895527  2111 net.cpp:150] Setting up attention_reweight6
I0822 23:34:25.895530  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895532  2111 net.cpp:165] Memory required for data: 149110720
I0822 23:34:25.895534  2111 layer_factory.hpp:77] Creating layer attention_weight_tile7
I0822 23:34:25.895539  2111 net.cpp:106] Creating Layer attention_weight_tile7
I0822 23:34:25.895542  2111 net.cpp:454] attention_weight_tile7 <- attention_weight_slice7
I0822 23:34:25.895546  2111 net.cpp:411] attention_weight_tile7 -> attention_weight_tile7
I0822 23:34:25.895558  2111 net.cpp:150] Setting up attention_weight_tile7
I0822 23:34:25.895561  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895563  2111 net.cpp:165] Memory required for data: 149274560
I0822 23:34:25.895566  2111 layer_factory.hpp:77] Creating layer attention_reweight7
I0822 23:34:25.895570  2111 net.cpp:106] Creating Layer attention_reweight7
I0822 23:34:25.895572  2111 net.cpp:454] attention_reweight7 <- attention_scale7_attention_scale_slice_7_split_1
I0822 23:34:25.895576  2111 net.cpp:454] attention_reweight7 <- attention_weight_tile7
I0822 23:34:25.895578  2111 net.cpp:411] attention_reweight7 -> attention_reweight7
I0822 23:34:25.895591  2111 net.cpp:150] Setting up attention_reweight7
I0822 23:34:25.895593  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895596  2111 net.cpp:165] Memory required for data: 149438400
I0822 23:34:25.895597  2111 layer_factory.hpp:77] Creating layer attention_reweight_sum
I0822 23:34:25.895601  2111 net.cpp:106] Creating Layer attention_reweight_sum
I0822 23:34:25.895604  2111 net.cpp:454] attention_reweight_sum <- attention_reweight0
I0822 23:34:25.895607  2111 net.cpp:454] attention_reweight_sum <- attention_reweight1
I0822 23:34:25.895609  2111 net.cpp:454] attention_reweight_sum <- attention_reweight2
I0822 23:34:25.895612  2111 net.cpp:454] attention_reweight_sum <- attention_reweight3
I0822 23:34:25.895614  2111 net.cpp:454] attention_reweight_sum <- attention_reweight4
I0822 23:34:25.895617  2111 net.cpp:454] attention_reweight_sum <- attention_reweight5
I0822 23:34:25.895619  2111 net.cpp:454] attention_reweight_sum <- attention_reweight6
I0822 23:34:25.895622  2111 net.cpp:454] attention_reweight_sum <- attention_reweight7
I0822 23:34:25.895624  2111 net.cpp:411] attention_reweight_sum -> attention_reweight_sum
I0822 23:34:25.895637  2111 net.cpp:150] Setting up attention_reweight_sum
I0822 23:34:25.895642  2111 net.cpp:157] Top shape: 80 64 8 1 (40960)
I0822 23:34:25.895643  2111 net.cpp:165] Memory required for data: 149602240
I0822 23:34:25.895645  2111 layer_factory.hpp:77] Creating layer attention_reweight_flatten
I0822 23:34:25.895650  2111 net.cpp:106] Creating Layer attention_reweight_flatten
I0822 23:34:25.895653  2111 net.cpp:454] attention_reweight_flatten <- attention_reweight_sum
I0822 23:34:25.895655  2111 net.cpp:411] attention_reweight_flatten -> attention_reweight_flatten
I0822 23:34:25.895668  2111 net.cpp:150] Setting up attention_reweight_flatten
I0822 23:34:25.895671  2111 net.cpp:157] Top shape: 80 512 (40960)
I0822 23:34:25.895673  2111 net.cpp:165] Memory required for data: 149766080
I0822 23:34:25.895676  2111 layer_factory.hpp:77] Creating layer attention_reweight_dropout
I0822 23:34:25.895687  2111 net.cpp:106] Creating Layer attention_reweight_dropout
I0822 23:34:25.895689  2111 net.cpp:454] attention_reweight_dropout <- attention_reweight_flatten
I0822 23:34:25.895694  2111 net.cpp:411] attention_reweight_dropout -> attention_reweight_dropout
I0822 23:34:25.895716  2111 net.cpp:150] Setting up attention_reweight_dropout
I0822 23:34:25.895720  2111 net.cpp:157] Top shape: 80 512 (40960)
I0822 23:34:25.895722  2111 net.cpp:165] Memory required for data: 149929920
I0822 23:34:25.895725  2111 layer_factory.hpp:77] Creating layer classification_fc1
I0822 23:34:25.895730  2111 net.cpp:106] Creating Layer classification_fc1
I0822 23:34:25.895732  2111 net.cpp:454] classification_fc1 <- attention_reweight_dropout
I0822 23:34:25.895735  2111 net.cpp:411] classification_fc1 -> classification_fc1
I0822 23:34:25.896558  2111 net.cpp:150] Setting up classification_fc1
I0822 23:34:25.896562  2111 net.cpp:157] Top shape: 80 64 (5120)
I0822 23:34:25.896564  2111 net.cpp:165] Memory required for data: 149950400
I0822 23:34:25.896569  2111 layer_factory.hpp:77] Creating layer classification_dropout
I0822 23:34:25.896574  2111 net.cpp:106] Creating Layer classification_dropout
I0822 23:34:25.896576  2111 net.cpp:454] classification_dropout <- classification_fc1
I0822 23:34:25.896579  2111 net.cpp:411] classification_dropout -> classification_dropout
I0822 23:34:25.896602  2111 net.cpp:150] Setting up classification_dropout
I0822 23:34:25.896605  2111 net.cpp:157] Top shape: 80 64 (5120)
I0822 23:34:25.896607  2111 net.cpp:165] Memory required for data: 149970880
I0822 23:34:25.896610  2111 layer_factory.hpp:77] Creating layer classification_fc2
I0822 23:34:25.896615  2111 net.cpp:106] Creating Layer classification_fc2
I0822 23:34:25.896616  2111 net.cpp:454] classification_fc2 <- classification_dropout
I0822 23:34:25.896620  2111 net.cpp:411] classification_fc2 -> classification_fc2
I0822 23:34:25.896685  2111 net.cpp:150] Setting up classification_fc2
I0822 23:34:25.896689  2111 net.cpp:157] Top shape: 80 4 (320)
I0822 23:34:25.896692  2111 net.cpp:165] Memory required for data: 149972160
I0822 23:34:25.896695  2111 layer_factory.hpp:77] Creating layer classification_fc2_classification_fc2_0_split
I0822 23:34:25.896699  2111 net.cpp:106] Creating Layer classification_fc2_classification_fc2_0_split
I0822 23:34:25.896701  2111 net.cpp:454] classification_fc2_classification_fc2_0_split <- classification_fc2
I0822 23:34:25.896705  2111 net.cpp:411] classification_fc2_classification_fc2_0_split -> classification_fc2_classification_fc2_0_split_0
I0822 23:34:25.896708  2111 net.cpp:411] classification_fc2_classification_fc2_0_split -> classification_fc2_classification_fc2_0_split_1
I0822 23:34:25.896730  2111 net.cpp:150] Setting up classification_fc2_classification_fc2_0_split
I0822 23:34:25.896733  2111 net.cpp:157] Top shape: 80 4 (320)
I0822 23:34:25.896736  2111 net.cpp:157] Top shape: 80 4 (320)
I0822 23:34:25.896739  2111 net.cpp:165] Memory required for data: 149974720
I0822 23:34:25.896740  2111 layer_factory.hpp:77] Creating layer classification_loss
I0822 23:34:25.896744  2111 net.cpp:106] Creating Layer classification_loss
I0822 23:34:25.896747  2111 net.cpp:454] classification_loss <- classification_fc2_classification_fc2_0_split_0
I0822 23:34:25.896750  2111 net.cpp:454] classification_loss <- label_data_1_split_0
I0822 23:34:25.896754  2111 net.cpp:411] classification_loss -> classification_loss
I0822 23:34:25.896759  2111 layer_factory.hpp:77] Creating layer classification_loss
I0822 23:34:25.896811  2111 net.cpp:150] Setting up classification_loss
I0822 23:34:25.896814  2111 net.cpp:157] Top shape: (1)
I0822 23:34:25.896816  2111 net.cpp:160]     with loss weight 1
I0822 23:34:25.896826  2111 net.cpp:165] Memory required for data: 149974724
I0822 23:34:25.896827  2111 layer_factory.hpp:77] Creating layer classification_accuracy
I0822 23:34:25.896831  2111 net.cpp:106] Creating Layer classification_accuracy
I0822 23:34:25.896833  2111 net.cpp:454] classification_accuracy <- classification_fc2_classification_fc2_0_split_1
I0822 23:34:25.896836  2111 net.cpp:454] classification_accuracy <- label_data_1_split_1
I0822 23:34:25.896841  2111 net.cpp:411] classification_accuracy -> classification_accuracy
I0822 23:34:25.896845  2111 net.cpp:150] Setting up classification_accuracy
I0822 23:34:25.896848  2111 net.cpp:157] Top shape: (1)
I0822 23:34:25.896850  2111 net.cpp:165] Memory required for data: 149974728
I0822 23:34:25.896852  2111 net.cpp:228] classification_accuracy does not need backward computation.
I0822 23:34:25.896854  2111 net.cpp:226] classification_loss needs backward computation.
I0822 23:34:25.896857  2111 net.cpp:226] classification_fc2_classification_fc2_0_split needs backward computation.
I0822 23:34:25.896859  2111 net.cpp:226] classification_fc2 needs backward computation.
I0822 23:34:25.896862  2111 net.cpp:226] classification_dropout needs backward computation.
I0822 23:34:25.896864  2111 net.cpp:226] classification_fc1 needs backward computation.
I0822 23:34:25.896867  2111 net.cpp:226] attention_reweight_dropout needs backward computation.
I0822 23:34:25.896869  2111 net.cpp:226] attention_reweight_flatten needs backward computation.
I0822 23:34:25.896872  2111 net.cpp:226] attention_reweight_sum needs backward computation.
I0822 23:34:25.896875  2111 net.cpp:226] attention_reweight7 needs backward computation.
I0822 23:34:25.896878  2111 net.cpp:226] attention_weight_tile7 needs backward computation.
I0822 23:34:25.896880  2111 net.cpp:226] attention_reweight6 needs backward computation.
I0822 23:34:25.896883  2111 net.cpp:226] attention_weight_tile6 needs backward computation.
I0822 23:34:25.896885  2111 net.cpp:226] attention_reweight5 needs backward computation.
I0822 23:34:25.896888  2111 net.cpp:226] attention_weight_tile5 needs backward computation.
I0822 23:34:25.896890  2111 net.cpp:226] attention_reweight4 needs backward computation.
I0822 23:34:25.896893  2111 net.cpp:226] attention_weight_tile4 needs backward computation.
I0822 23:34:25.896895  2111 net.cpp:226] attention_reweight3 needs backward computation.
I0822 23:34:25.896898  2111 net.cpp:226] attention_weight_tile3 needs backward computation.
I0822 23:34:25.896900  2111 net.cpp:226] attention_reweight2 needs backward computation.
I0822 23:34:25.896903  2111 net.cpp:226] attention_weight_tile2 needs backward computation.
I0822 23:34:25.896904  2111 net.cpp:226] attention_reweight1 needs backward computation.
I0822 23:34:25.896908  2111 net.cpp:226] attention_weight_tile1 needs backward computation.
I0822 23:34:25.896909  2111 net.cpp:226] attention_reweight0 needs backward computation.
I0822 23:34:25.896911  2111 net.cpp:226] attention_weight_tile0 needs backward computation.
I0822 23:34:25.896914  2111 net.cpp:226] attention_weight_slice needs backward computation.
I0822 23:34:25.896916  2111 net.cpp:226] attention_weight needs backward computation.
I0822 23:34:25.896919  2111 net.cpp:226] attention_height_concat needs backward computation.
I0822 23:34:25.896922  2111 net.cpp:226] attention_height_reshape7 needs backward computation.
I0822 23:34:25.896924  2111 net.cpp:226] attention_height_fc3_7 needs backward computation.
I0822 23:34:25.896927  2111 net.cpp:226] attention_height_fc2_7 needs backward computation.
I0822 23:34:25.896929  2111 net.cpp:226] attention_height_fc1_7 needs backward computation.
I0822 23:34:25.896931  2111 net.cpp:226] attention_height_reshape6 needs backward computation.
I0822 23:34:25.896934  2111 net.cpp:226] attention_height_fc3_6 needs backward computation.
I0822 23:34:25.896935  2111 net.cpp:226] attention_height_fc2_6 needs backward computation.
I0822 23:34:25.896939  2111 net.cpp:226] attention_height_fc1_6 needs backward computation.
I0822 23:34:25.896940  2111 net.cpp:226] attention_height_reshape5 needs backward computation.
I0822 23:34:25.896942  2111 net.cpp:226] attention_height_fc3_5 needs backward computation.
I0822 23:34:25.896944  2111 net.cpp:226] attention_height_fc2_5 needs backward computation.
I0822 23:34:25.896947  2111 net.cpp:226] attention_height_fc1_5 needs backward computation.
I0822 23:34:25.896950  2111 net.cpp:226] attention_height_reshape4 needs backward computation.
I0822 23:34:25.896951  2111 net.cpp:226] attention_height_fc3_4 needs backward computation.
I0822 23:34:25.896953  2111 net.cpp:226] attention_height_fc2_4 needs backward computation.
I0822 23:34:25.896955  2111 net.cpp:226] attention_height_fc1_4 needs backward computation.
I0822 23:34:25.896958  2111 net.cpp:226] attention_height_reshape3 needs backward computation.
I0822 23:34:25.896960  2111 net.cpp:226] attention_height_fc3_3 needs backward computation.
I0822 23:34:25.896962  2111 net.cpp:226] attention_height_fc2_3 needs backward computation.
I0822 23:34:25.896965  2111 net.cpp:226] attention_height_fc1_3 needs backward computation.
I0822 23:34:25.896966  2111 net.cpp:226] attention_height_reshape2 needs backward computation.
I0822 23:34:25.896968  2111 net.cpp:226] attention_height_fc3_2 needs backward computation.
I0822 23:34:25.896971  2111 net.cpp:226] attention_height_fc2_2 needs backward computation.
I0822 23:34:25.896973  2111 net.cpp:226] attention_height_fc1_2 needs backward computation.
I0822 23:34:25.896975  2111 net.cpp:226] attention_height_reshape1 needs backward computation.
I0822 23:34:25.896977  2111 net.cpp:226] attention_height_fc3_1 needs backward computation.
I0822 23:34:25.896981  2111 net.cpp:226] attention_height_fc2_1 needs backward computation.
I0822 23:34:25.896982  2111 net.cpp:226] attention_height_fc1_1 needs backward computation.
I0822 23:34:25.896984  2111 net.cpp:226] attention_height_reshape0 needs backward computation.
I0822 23:34:25.896986  2111 net.cpp:226] attention_height_fc3_0 needs backward computation.
I0822 23:34:25.896988  2111 net.cpp:226] attention_height_fc2_0 needs backward computation.
I0822 23:34:25.896991  2111 net.cpp:226] attention_height_fc1_0 needs backward computation.
I0822 23:34:25.896993  2111 net.cpp:226] attention_height_slice needs backward computation.
I0822 23:34:25.896996  2111 net.cpp:226] attention_scale_concat needs backward computation.
I0822 23:34:25.896999  2111 net.cpp:226] attention_reduction7 needs backward computation.
I0822 23:34:25.897002  2111 net.cpp:226] attention_permute7 needs backward computation.
I0822 23:34:25.897004  2111 net.cpp:226] attention_reduction6 needs backward computation.
I0822 23:34:25.897007  2111 net.cpp:226] attention_permute6 needs backward computation.
I0822 23:34:25.897011  2111 net.cpp:226] attention_reduction5 needs backward computation.
I0822 23:34:25.897012  2111 net.cpp:226] attention_permute5 needs backward computation.
I0822 23:34:25.897014  2111 net.cpp:226] attention_reduction4 needs backward computation.
I0822 23:34:25.897017  2111 net.cpp:226] attention_permute4 needs backward computation.
I0822 23:34:25.897019  2111 net.cpp:226] attention_reduction3 needs backward computation.
I0822 23:34:25.897022  2111 net.cpp:226] attention_permute3 needs backward computation.
I0822 23:34:25.897024  2111 net.cpp:226] attention_reduction2 needs backward computation.
I0822 23:34:25.897027  2111 net.cpp:226] attention_permute2 needs backward computation.
I0822 23:34:25.897029  2111 net.cpp:226] attention_reduction1 needs backward computation.
I0822 23:34:25.897032  2111 net.cpp:226] attention_permute1 needs backward computation.
I0822 23:34:25.897034  2111 net.cpp:226] attention_reduction0 needs backward computation.
I0822 23:34:25.897037  2111 net.cpp:226] attention_permute0 needs backward computation.
I0822 23:34:25.897038  2111 net.cpp:226] attention_scale7_attention_scale_slice_7_split needs backward computation.
I0822 23:34:25.897042  2111 net.cpp:226] attention_scale6_attention_scale_slice_6_split needs backward computation.
I0822 23:34:25.897044  2111 net.cpp:226] attention_scale5_attention_scale_slice_5_split needs backward computation.
I0822 23:34:25.897047  2111 net.cpp:226] attention_scale4_attention_scale_slice_4_split needs backward computation.
I0822 23:34:25.897048  2111 net.cpp:226] attention_scale3_attention_scale_slice_3_split needs backward computation.
I0822 23:34:25.897052  2111 net.cpp:226] attention_scale2_attention_scale_slice_2_split needs backward computation.
I0822 23:34:25.897053  2111 net.cpp:226] attention_scale1_attention_scale_slice_1_split needs backward computation.
I0822 23:34:25.897055  2111 net.cpp:226] attention_scale0_attention_scale_slice_0_split needs backward computation.
I0822 23:34:25.897058  2111 net.cpp:226] attention_scale_slice needs backward computation.
I0822 23:34:25.897060  2111 net.cpp:226] attention_layer_relu1 needs backward computation.
I0822 23:34:25.897063  2111 net.cpp:226] attention_layer_bn1 needs backward computation.
I0822 23:34:25.897066  2111 net.cpp:226] attention_layer_conv1 needs backward computation.
I0822 23:34:25.897068  2111 net.cpp:226] attention_layer_relu0 needs backward computation.
I0822 23:34:25.897070  2111 net.cpp:226] attention_layer_bn0 needs backward computation.
I0822 23:34:25.897073  2111 net.cpp:226] attention_layer_conv0 needs backward computation.
I0822 23:34:25.897075  2111 net.cpp:226] dense_layer_relu4 needs backward computation.
I0822 23:34:25.897078  2111 net.cpp:226] dense_layer_bn4 needs backward computation.
I0822 23:34:25.897081  2111 net.cpp:226] dense_layer_concat4 needs backward computation.
I0822 23:34:25.897084  2111 net.cpp:226] dense_layer_conv4 needs backward computation.
I0822 23:34:25.897087  2111 net.cpp:226] dense_layer_pool3_dense_layer_pool3_0_split needs backward computation.
I0822 23:34:25.897090  2111 net.cpp:226] dense_layer_pool3 needs backward computation.
I0822 23:34:25.897092  2111 net.cpp:226] dense_layer_relu3 needs backward computation.
I0822 23:34:25.897094  2111 net.cpp:226] dense_layer_bn3 needs backward computation.
I0822 23:34:25.897097  2111 net.cpp:226] dense_layer_concat3 needs backward computation.
I0822 23:34:25.897100  2111 net.cpp:226] dense_layer_conv3 needs backward computation.
I0822 23:34:25.897104  2111 net.cpp:226] dense_layer_pool2_dense_layer_pool2_0_split needs backward computation.
I0822 23:34:25.897105  2111 net.cpp:226] dense_layer_pool2 needs backward computation.
I0822 23:34:25.897109  2111 net.cpp:226] dense_layer_relu2 needs backward computation.
I0822 23:34:25.897110  2111 net.cpp:226] dense_layer_bn2 needs backward computation.
I0822 23:34:25.897114  2111 net.cpp:226] dense_layer_concat2 needs backward computation.
I0822 23:34:25.897116  2111 net.cpp:226] dense_layer_conv2 needs backward computation.
I0822 23:34:25.897119  2111 net.cpp:226] dense_layer_pool1_dense_layer_pool1_0_split needs backward computation.
I0822 23:34:25.897121  2111 net.cpp:226] dense_layer_pool1 needs backward computation.
I0822 23:34:25.897125  2111 net.cpp:226] dense_layer_relu1 needs backward computation.
I0822 23:34:25.897126  2111 net.cpp:226] dense_layer_bn1 needs backward computation.
I0822 23:34:25.897130  2111 net.cpp:226] dense_layer_concat1 needs backward computation.
I0822 23:34:25.897131  2111 net.cpp:226] dense_layer_conv1 needs backward computation.
I0822 23:34:25.897135  2111 net.cpp:226] dense_layer_pool0_dense_layer_pool0_0_split needs backward computation.
I0822 23:34:25.897136  2111 net.cpp:226] dense_layer_pool0 needs backward computation.
I0822 23:34:25.897140  2111 net.cpp:226] dense_layer_relu0 needs backward computation.
I0822 23:34:25.897141  2111 net.cpp:226] dense_layer_bn0 needs backward computation.
I0822 23:34:25.897145  2111 net.cpp:226] dense_layer_concat0 needs backward computation.
I0822 23:34:25.897147  2111 net.cpp:226] dense_layer_conv0 needs backward computation.
I0822 23:34:25.897150  2111 net.cpp:226] trans_layer_relu2 needs backward computation.
I0822 23:34:25.897152  2111 net.cpp:226] trans_layer_bn2 needs backward computation.
I0822 23:34:25.897155  2111 net.cpp:226] trans_layer_conv2_trans_layer_conv2_0_split needs backward computation.
I0822 23:34:25.897157  2111 net.cpp:226] trans_layer_conv2 needs backward computation.
I0822 23:34:25.897161  2111 net.cpp:226] trans_layer_relu1 needs backward computation.
I0822 23:34:25.897162  2111 net.cpp:226] trans_layer_bn1 needs backward computation.
I0822 23:34:25.897168  2111 net.cpp:226] trans_layer_conv1_trans_layer_conv1_0_split needs backward computation.
I0822 23:34:25.897183  2111 net.cpp:226] trans_layer_conv1 needs backward computation.
I0822 23:34:25.897186  2111 net.cpp:226] trans_layer_relu0 needs backward computation.
I0822 23:34:25.897188  2111 net.cpp:226] trans_layer_bn0 needs backward computation.
I0822 23:34:25.897191  2111 net.cpp:226] trans_layer_conv0_trans_layer_conv0_0_split needs backward computation.
I0822 23:34:25.897193  2111 net.cpp:226] trans_layer_conv0 needs backward computation.
I0822 23:34:25.897197  2111 net.cpp:228] label_data_1_split does not need backward computation.
I0822 23:34:25.897199  2111 net.cpp:228] data does not need backward computation.
I0822 23:34:25.897202  2111 net.cpp:270] This network produces output classification_accuracy
I0822 23:34:25.897204  2111 net.cpp:270] This network produces output classification_loss
I0822 23:34:25.897272  2111 net.cpp:283] Network initialization done.
I0822 23:34:25.897598  2111 solver.cpp:60] Solver scaffolding done.
I0822 23:34:25.916357  2111 parallel.cpp:391] GPUs pairs 0:1, 2:3, 0:2
I0822 23:34:26.224704  2111 net.cpp:99] Sharing layer data from root net
I0822 23:34:26.225872  2111 net.cpp:143] Created top blob 0 (shape: 64 1 100 300 (1920000)) for shared layer data
I0822 23:34:26.225924  2111 net.cpp:143] Created top blob 1 (shape: 64 (64)) for shared layer data
I0822 23:34:26.555023  2111 net.cpp:99] Sharing layer data from root net
I0822 23:34:26.556262  2111 net.cpp:143] Created top blob 0 (shape: 64 1 100 300 (1920000)) for shared layer data
I0822 23:34:26.556322  2111 net.cpp:143] Created top blob 1 (shape: 64 (64)) for shared layer data
I0822 23:34:26.950647  2111 net.cpp:99] Sharing layer data from root net
I0822 23:34:26.952335  2111 net.cpp:143] Created top blob 0 (shape: 64 1 100 300 (1920000)) for shared layer data
I0822 23:34:26.952425  2111 net.cpp:143] Created top blob 1 (shape: 64 (64)) for shared layer data
I0822 23:34:27.000610  2111 parallel.cpp:419] Starting Optimization
I0822 23:34:27.000676  2111 solver.cpp:280] Solving DenseAttentionNet
I0822 23:34:27.000682  2111 solver.cpp:281] Learning Rate Policy: multistep
I0822 23:34:27.932312  2111 solver.cpp:229] Iteration 0, loss = 1.38632
I0822 23:34:27.932337  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.234375
I0822 23:34:27.932346  2111 solver.cpp:245]     Train net output #1: classification_loss = 1.38632 (* 1 = 1.38632 loss)
I0822 23:34:27.949662  2111 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0822 23:34:27.964038  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0822 23:34:45.563416  2111 solver.cpp:229] Iteration 100, loss = 1.38421
I0822 23:34:45.563437  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.234375
I0822 23:34:45.563446  2111 solver.cpp:245]     Train net output #1: classification_loss = 1.38421 (* 1 = 1.38421 loss)
I0822 23:34:45.572407  2111 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I0822 23:35:03.035877  2111 solver.cpp:229] Iteration 200, loss = 0.70061
I0822 23:35:03.035902  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.734375
I0822 23:35:03.035908  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.70061 (* 1 = 0.70061 loss)
I0822 23:35:03.048383  2111 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I0822 23:35:20.669283  2111 solver.cpp:229] Iteration 300, loss = 0.727213
I0822 23:35:20.669309  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0822 23:35:20.669317  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.727213 (* 1 = 0.727213 loss)
I0822 23:35:20.677178  2111 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I0822 23:35:40.114542  2111 solver.cpp:229] Iteration 400, loss = 0.410458
I0822 23:35:40.114564  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:35:40.114572  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.410458 (* 1 = 0.410458 loss)
I0822 23:35:40.125380  2111 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I0822 23:35:59.630631  2111 solver.cpp:229] Iteration 500, loss = 0.363491
I0822 23:35:59.630657  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:35:59.630666  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.363491 (* 1 = 0.363491 loss)
I0822 23:35:59.664711  2111 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I0822 23:36:21.226284  2111 solver.cpp:229] Iteration 600, loss = 0.401244
I0822 23:36:21.226307  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:36:21.226313  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.401244 (* 1 = 0.401244 loss)
I0822 23:36:21.226335  2111 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I0822 23:36:40.457463  2111 solver.cpp:229] Iteration 700, loss = 0.341294
I0822 23:36:40.457487  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0822 23:36:40.457495  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.341294 (* 1 = 0.341294 loss)
I0822 23:36:40.495846  2111 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I0822 23:36:58.628813  2111 solver.cpp:229] Iteration 800, loss = 0.399694
I0822 23:36:58.628837  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.859375
I0822 23:36:58.628849  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.399694 (* 1 = 0.399694 loss)
I0822 23:36:58.644426  2111 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I0822 23:37:16.911662  2111 solver.cpp:229] Iteration 900, loss = 0.401244
I0822 23:37:16.911687  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.84375
I0822 23:37:16.911697  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.401244 (* 1 = 0.401244 loss)
I0822 23:37:16.923071  2111 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I0822 23:37:38.243077  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_1000.caffemodel
I0822 23:37:38.255399  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_1000.solverstate
I0822 23:37:38.257733  2111 solver.cpp:338] Iteration 1000, Testing net (#0)
I0822 23:37:49.424110  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.901184
I0822 23:37:49.424144  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.294986 (* 1 = 0.294986 loss)
I0822 23:37:49.572417  2111 solver.cpp:229] Iteration 1000, loss = 0.290099
I0822 23:37:49.572446  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0822 23:37:49.572458  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.290099 (* 1 = 0.290099 loss)
I0822 23:37:49.656564  2111 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I0822 23:37:49.660665  2180 blocking_queue.cpp:50] Data layer prefetch queue empty
I0822 23:38:08.618804  2111 solver.cpp:229] Iteration 1100, loss = 0.312646
I0822 23:38:08.618834  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:38:08.618846  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.312646 (* 1 = 0.312646 loss)
I0822 23:38:08.674391  2111 sgd_solver.cpp:106] Iteration 1100, lr = 0.01
I0822 23:38:28.425696  2111 solver.cpp:229] Iteration 1200, loss = 0.351489
I0822 23:38:28.425734  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0822 23:38:28.425743  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.351489 (* 1 = 0.351489 loss)
I0822 23:38:28.431279  2111 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I0822 23:38:47.257066  2111 solver.cpp:229] Iteration 1300, loss = 0.207126
I0822 23:38:47.257091  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:38:47.257097  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.207126 (* 1 = 0.207126 loss)
I0822 23:38:47.269193  2111 sgd_solver.cpp:106] Iteration 1300, lr = 0.01
I0822 23:39:05.800329  2111 solver.cpp:229] Iteration 1400, loss = 0.377453
I0822 23:39:05.800360  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0822 23:39:05.800367  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.377453 (* 1 = 0.377453 loss)
I0822 23:39:05.871238  2111 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
I0822 23:39:25.699934  2111 solver.cpp:229] Iteration 1500, loss = 0.399485
I0822 23:39:25.699957  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.84375
I0822 23:39:25.699965  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.399485 (* 1 = 0.399485 loss)
I0822 23:39:25.710470  2111 sgd_solver.cpp:106] Iteration 1500, lr = 0.01
I0822 23:39:43.387924  2111 solver.cpp:229] Iteration 1600, loss = 0.224727
I0822 23:39:43.387953  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0822 23:39:43.387962  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.224727 (* 1 = 0.224727 loss)
I0822 23:39:43.434350  2111 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I0822 23:40:00.964465  2111 solver.cpp:229] Iteration 1700, loss = 0.333648
I0822 23:40:00.964490  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:40:00.964498  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.333648 (* 1 = 0.333648 loss)
I0822 23:40:00.975445  2111 sgd_solver.cpp:106] Iteration 1700, lr = 0.01
I0822 23:40:22.367962  2111 solver.cpp:229] Iteration 1800, loss = 0.256358
I0822 23:40:22.367985  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:40:22.367991  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.256358 (* 1 = 0.256358 loss)
I0822 23:40:22.368010  2111 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
I0822 23:40:45.232188  2111 solver.cpp:229] Iteration 1900, loss = 0.259545
I0822 23:40:45.232211  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0822 23:40:45.232218  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.259544 (* 1 = 0.259544 loss)
I0822 23:40:45.232246  2111 sgd_solver.cpp:106] Iteration 1900, lr = 0.01
I0822 23:41:06.150820  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_2000.caffemodel
I0822 23:41:06.159296  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_2000.solverstate
I0822 23:41:06.164172  2111 solver.cpp:338] Iteration 2000, Testing net (#0)
I0822 23:41:17.365532  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.864737
I0822 23:41:17.365563  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.371279 (* 1 = 0.371279 loss)
I0822 23:41:17.509763  2111 solver.cpp:229] Iteration 2000, loss = 0.256224
I0822 23:41:17.509788  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:41:17.509795  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.256223 (* 1 = 0.256223 loss)
I0822 23:41:17.635679  2111 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I0822 23:41:17.639129  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0822 23:41:39.482482  2111 solver.cpp:229] Iteration 2100, loss = 0.383445
I0822 23:41:39.482511  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:41:39.482520  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.383445 (* 1 = 0.383445 loss)
I0822 23:41:39.526798  2111 sgd_solver.cpp:106] Iteration 2100, lr = 0.01
I0822 23:41:58.764251  2111 solver.cpp:229] Iteration 2200, loss = 0.297334
I0822 23:41:58.764277  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0822 23:41:58.764286  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.297334 (* 1 = 0.297334 loss)
I0822 23:41:58.787858  2111 sgd_solver.cpp:106] Iteration 2200, lr = 0.01
I0822 23:42:20.649801  2111 solver.cpp:229] Iteration 2300, loss = 0.369652
I0822 23:42:20.649827  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0822 23:42:20.649834  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.369652 (* 1 = 0.369652 loss)
I0822 23:42:20.675998  2111 sgd_solver.cpp:106] Iteration 2300, lr = 0.01
I0822 23:42:42.313685  2111 solver.cpp:229] Iteration 2400, loss = 0.344485
I0822 23:42:42.313711  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0822 23:42:42.313719  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.344485 (* 1 = 0.344485 loss)
I0822 23:42:42.330130  2111 sgd_solver.cpp:106] Iteration 2400, lr = 0.01
I0822 23:43:00.346487  2111 solver.cpp:229] Iteration 2500, loss = 0.440369
I0822 23:43:00.346509  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.84375
I0822 23:43:00.346516  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.440368 (* 1 = 0.440368 loss)
I0822 23:43:00.357852  2111 sgd_solver.cpp:106] Iteration 2500, lr = 0.01
I0822 23:43:18.603564  2111 solver.cpp:229] Iteration 2600, loss = 0.204386
I0822 23:43:18.603590  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0822 23:43:18.603598  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.204386 (* 1 = 0.204386 loss)
I0822 23:43:18.639760  2111 sgd_solver.cpp:106] Iteration 2600, lr = 0.01
I0822 23:43:38.267882  2111 solver.cpp:229] Iteration 2700, loss = 0.169964
I0822 23:43:38.267906  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0822 23:43:38.267913  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.169963 (* 1 = 0.169963 loss)
I0822 23:43:38.267943  2111 sgd_solver.cpp:106] Iteration 2700, lr = 0.01
I0822 23:43:58.537832  2111 solver.cpp:229] Iteration 2800, loss = 0.303901
I0822 23:43:58.537858  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0822 23:43:58.537865  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.303901 (* 1 = 0.303901 loss)
I0822 23:43:58.551908  2111 sgd_solver.cpp:106] Iteration 2800, lr = 0.01
I0822 23:44:16.284802  2111 solver.cpp:229] Iteration 2900, loss = 0.307491
I0822 23:44:16.284826  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.859375
I0822 23:44:16.284833  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.307491 (* 1 = 0.307491 loss)
I0822 23:44:16.296463  2111 sgd_solver.cpp:106] Iteration 2900, lr = 0.01
I0822 23:44:34.104393  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_3000.caffemodel
I0822 23:44:34.109877  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_3000.solverstate
I0822 23:44:34.112246  2111 solver.cpp:338] Iteration 3000, Testing net (#0)
I0822 23:44:45.275446  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.904079
I0822 23:44:45.275476  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.2825 (* 1 = 0.2825 loss)
I0822 23:44:45.502283  2111 solver.cpp:229] Iteration 3000, loss = 0.116482
I0822 23:44:45.502310  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0822 23:44:45.502319  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.116482 (* 1 = 0.116482 loss)
I0822 23:44:45.534211  2111 sgd_solver.cpp:106] Iteration 3000, lr = 0.01
I0822 23:44:45.538007  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0822 23:45:03.710682  2111 solver.cpp:229] Iteration 3100, loss = 0.369273
I0822 23:45:03.710708  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0822 23:45:03.710718  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.369273 (* 1 = 0.369273 loss)
I0822 23:45:03.722419  2111 sgd_solver.cpp:106] Iteration 3100, lr = 0.01
I0822 23:45:21.301571  2111 solver.cpp:229] Iteration 3200, loss = 0.158336
I0822 23:45:21.301596  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0822 23:45:21.301606  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.158335 (* 1 = 0.158335 loss)
I0822 23:45:21.313827  2111 sgd_solver.cpp:106] Iteration 3200, lr = 0.01
I0822 23:45:40.071169  2111 solver.cpp:229] Iteration 3300, loss = 0.226444
I0822 23:45:40.071194  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:45:40.071204  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.226444 (* 1 = 0.226444 loss)
I0822 23:45:40.082108  2111 sgd_solver.cpp:106] Iteration 3300, lr = 0.01
I0822 23:46:01.213012  2111 solver.cpp:229] Iteration 3400, loss = 0.286813
I0822 23:46:01.213043  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:46:01.213052  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.286813 (* 1 = 0.286813 loss)
I0822 23:46:01.254523  2111 sgd_solver.cpp:106] Iteration 3400, lr = 0.01
I0822 23:46:20.570252  2111 solver.cpp:229] Iteration 3500, loss = 0.393747
I0822 23:46:20.570276  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0822 23:46:20.570283  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.393747 (* 1 = 0.393747 loss)
I0822 23:46:20.581827  2111 sgd_solver.cpp:106] Iteration 3500, lr = 0.01
I0822 23:46:38.004040  2111 solver.cpp:229] Iteration 3600, loss = 0.176323
I0822 23:46:38.004063  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:46:38.004070  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.176322 (* 1 = 0.176322 loss)
I0822 23:46:38.016530  2111 sgd_solver.cpp:106] Iteration 3600, lr = 0.01
I0822 23:46:56.964251  2111 solver.cpp:229] Iteration 3700, loss = 0.343515
I0822 23:46:56.964274  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0822 23:46:56.964282  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.343515 (* 1 = 0.343515 loss)
I0822 23:46:56.976156  2111 sgd_solver.cpp:106] Iteration 3700, lr = 0.01
I0822 23:47:14.814224  2111 solver.cpp:229] Iteration 3800, loss = 0.261839
I0822 23:47:14.814247  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0822 23:47:14.814254  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.261839 (* 1 = 0.261839 loss)
I0822 23:47:14.824156  2111 sgd_solver.cpp:106] Iteration 3800, lr = 0.01
I0822 23:47:32.756314  2111 solver.cpp:229] Iteration 3900, loss = 0.15433
I0822 23:47:32.756337  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.984375
I0822 23:47:32.756345  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.15433 (* 1 = 0.15433 loss)
I0822 23:47:32.767105  2111 sgd_solver.cpp:106] Iteration 3900, lr = 0.01
I0822 23:47:52.214756  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_4000.caffemodel
I0822 23:47:52.223323  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_4000.solverstate
I0822 23:47:52.228194  2111 solver.cpp:338] Iteration 4000, Testing net (#0)
I0822 23:48:03.496796  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.907237
I0822 23:48:03.496827  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.279225 (* 1 = 0.279225 loss)
I0822 23:48:03.722337  2111 solver.cpp:229] Iteration 4000, loss = 0.207906
I0822 23:48:03.722368  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0822 23:48:03.722378  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.207906 (* 1 = 0.207906 loss)
I0822 23:48:03.750233  2111 sgd_solver.cpp:106] Iteration 4000, lr = 0.01
I0822 23:48:03.754504  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0822 23:48:21.714118  2111 solver.cpp:229] Iteration 4100, loss = 0.32765
I0822 23:48:21.714143  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0822 23:48:21.714149  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.32765 (* 1 = 0.32765 loss)
I0822 23:48:21.724659  2111 sgd_solver.cpp:106] Iteration 4100, lr = 0.01
I0822 23:48:39.436813  2111 solver.cpp:229] Iteration 4200, loss = 0.133359
I0822 23:48:39.436836  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0822 23:48:39.436844  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.133359 (* 1 = 0.133359 loss)
I0822 23:48:39.451447  2111 sgd_solver.cpp:106] Iteration 4200, lr = 0.01
I0822 23:48:57.232506  2111 solver.cpp:229] Iteration 4300, loss = 0.335151
I0822 23:48:57.232532  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0822 23:48:57.232538  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.335151 (* 1 = 0.335151 loss)
I0822 23:48:57.245486  2111 sgd_solver.cpp:106] Iteration 4300, lr = 0.01
I0822 23:49:14.950207  2111 solver.cpp:229] Iteration 4400, loss = 0.160062
I0822 23:49:14.950233  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0822 23:49:14.950240  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.160062 (* 1 = 0.160062 loss)
I0822 23:49:14.960542  2111 sgd_solver.cpp:106] Iteration 4400, lr = 0.01
I0822 23:49:32.977577  2111 solver.cpp:229] Iteration 4500, loss = 0.238986
I0822 23:49:32.977602  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0822 23:49:32.977609  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.238986 (* 1 = 0.238986 loss)
I0822 23:49:32.988039  2111 sgd_solver.cpp:106] Iteration 4500, lr = 0.01
I0822 23:49:52.560461  2111 solver.cpp:229] Iteration 4600, loss = 0.26866
I0822 23:49:52.560485  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:49:52.560492  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.26866 (* 1 = 0.26866 loss)
I0822 23:49:52.570842  2111 sgd_solver.cpp:106] Iteration 4600, lr = 0.01
I0822 23:50:10.939159  2111 solver.cpp:229] Iteration 4700, loss = 0.333175
I0822 23:50:10.939188  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:50:10.939200  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.333175 (* 1 = 0.333175 loss)
I0822 23:50:10.970655  2111 sgd_solver.cpp:106] Iteration 4700, lr = 0.01
I0822 23:50:30.565582  2111 solver.cpp:229] Iteration 4800, loss = 0.166863
I0822 23:50:30.565606  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0822 23:50:30.565616  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.166863 (* 1 = 0.166863 loss)
I0822 23:50:30.576905  2111 sgd_solver.cpp:106] Iteration 4800, lr = 0.01
I0822 23:50:48.544637  2111 solver.cpp:229] Iteration 4900, loss = 0.149109
I0822 23:50:48.544662  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0822 23:50:48.544669  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.149109 (* 1 = 0.149109 loss)
I0822 23:50:48.555313  2111 sgd_solver.cpp:106] Iteration 4900, lr = 0.01
I0822 23:51:06.316591  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_5000.caffemodel
I0822 23:51:06.322142  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_5000.solverstate
I0822 23:51:06.324515  2111 solver.cpp:338] Iteration 5000, Testing net (#0)
I0822 23:51:17.478811  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.889342
I0822 23:51:17.478839  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.319254 (* 1 = 0.319254 loss)
I0822 23:51:17.627966  2111 solver.cpp:229] Iteration 5000, loss = 0.272206
I0822 23:51:17.627995  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:51:17.628001  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.272206 (* 1 = 0.272206 loss)
I0822 23:51:17.699265  2111 sgd_solver.cpp:106] Iteration 5000, lr = 0.01
I0822 23:51:17.702841  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0822 23:51:36.976414  2111 solver.cpp:229] Iteration 5100, loss = 0.169818
I0822 23:51:36.976440  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0822 23:51:36.976447  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.169818 (* 1 = 0.169818 loss)
I0822 23:51:36.988827  2111 sgd_solver.cpp:106] Iteration 5100, lr = 0.01
I0822 23:51:55.774060  2111 solver.cpp:229] Iteration 5200, loss = 0.34491
I0822 23:51:55.774086  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0822 23:51:55.774094  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.34491 (* 1 = 0.34491 loss)
I0822 23:51:55.786281  2111 sgd_solver.cpp:106] Iteration 5200, lr = 0.01
I0822 23:52:13.616973  2111 solver.cpp:229] Iteration 5300, loss = 0.340708
I0822 23:52:13.616999  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:52:13.617007  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.340708 (* 1 = 0.340708 loss)
I0822 23:52:13.682178  2111 sgd_solver.cpp:106] Iteration 5300, lr = 0.01
I0822 23:52:35.471613  2111 solver.cpp:229] Iteration 5400, loss = 0.250483
I0822 23:52:35.471637  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0822 23:52:35.471644  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.250483 (* 1 = 0.250483 loss)
I0822 23:52:35.510774  2111 sgd_solver.cpp:106] Iteration 5400, lr = 0.01
I0822 23:52:54.364866  2111 solver.cpp:229] Iteration 5500, loss = 0.19046
I0822 23:52:54.364889  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0822 23:52:54.364897  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.19046 (* 1 = 0.19046 loss)
I0822 23:52:54.375385  2111 sgd_solver.cpp:106] Iteration 5500, lr = 0.01
I0822 23:53:12.186319  2111 solver.cpp:229] Iteration 5600, loss = 0.424467
I0822 23:53:12.186342  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:53:12.186349  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.424467 (* 1 = 0.424467 loss)
I0822 23:53:12.196996  2111 sgd_solver.cpp:106] Iteration 5600, lr = 0.01
I0822 23:53:29.865921  2111 solver.cpp:229] Iteration 5700, loss = 0.207951
I0822 23:53:29.865945  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:53:29.865952  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.20795 (* 1 = 0.20795 loss)
I0822 23:53:29.877003  2111 sgd_solver.cpp:106] Iteration 5700, lr = 0.01
I0822 23:53:47.459372  2111 solver.cpp:229] Iteration 5800, loss = 0.330396
I0822 23:53:47.459394  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0822 23:53:47.459401  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.330396 (* 1 = 0.330396 loss)
I0822 23:53:47.470022  2111 sgd_solver.cpp:106] Iteration 5800, lr = 0.01
I0822 23:54:05.106374  2111 solver.cpp:229] Iteration 5900, loss = 0.210206
I0822 23:54:05.106398  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0822 23:54:05.106405  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.210206 (* 1 = 0.210206 loss)
I0822 23:54:05.117131  2111 sgd_solver.cpp:106] Iteration 5900, lr = 0.01
I0822 23:54:23.679759  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_6000.caffemodel
I0822 23:54:23.685155  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_6000.solverstate
I0822 23:54:23.687490  2111 solver.cpp:338] Iteration 6000, Testing net (#0)
I0822 23:54:34.847327  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.911579
I0822 23:54:34.847368  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.269232 (* 1 = 0.269232 loss)
I0822 23:54:34.996425  2111 solver.cpp:229] Iteration 6000, loss = 0.17427
I0822 23:54:34.996454  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0822 23:54:34.996461  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.17427 (* 1 = 0.17427 loss)
I0822 23:54:35.069172  2111 sgd_solver.cpp:106] Iteration 6000, lr = 0.01
I0822 23:54:35.073171  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0822 23:54:53.858621  2111 solver.cpp:229] Iteration 6100, loss = 0.265374
I0822 23:54:53.858654  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0822 23:54:53.858661  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.265374 (* 1 = 0.265374 loss)
I0822 23:54:53.892679  2111 sgd_solver.cpp:106] Iteration 6100, lr = 0.01
I0822 23:55:12.777303  2111 solver.cpp:229] Iteration 6200, loss = 0.725663
I0822 23:55:12.777328  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.828125
I0822 23:55:12.777334  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.725663 (* 1 = 0.725663 loss)
I0822 23:55:12.787582  2111 sgd_solver.cpp:106] Iteration 6200, lr = 0.01
I0822 23:55:30.435884  2111 solver.cpp:229] Iteration 6300, loss = 0.235098
I0822 23:55:30.435906  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:55:30.435914  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.235098 (* 1 = 0.235098 loss)
I0822 23:55:30.448998  2111 sgd_solver.cpp:106] Iteration 6300, lr = 0.01
I0822 23:55:47.983613  2111 solver.cpp:229] Iteration 6400, loss = 0.405385
I0822 23:55:47.983636  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0822 23:55:47.983644  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.405385 (* 1 = 0.405385 loss)
I0822 23:55:47.995842  2111 sgd_solver.cpp:106] Iteration 6400, lr = 0.01
I0822 23:56:05.610110  2111 solver.cpp:229] Iteration 6500, loss = 0.589787
I0822 23:56:05.610132  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.828125
I0822 23:56:05.610139  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.589787 (* 1 = 0.589787 loss)
I0822 23:56:05.621789  2111 sgd_solver.cpp:106] Iteration 6500, lr = 0.01
I0822 23:56:23.572571  2111 solver.cpp:229] Iteration 6600, loss = 0.155392
I0822 23:56:23.572594  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0822 23:56:23.572602  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.155392 (* 1 = 0.155392 loss)
I0822 23:56:23.589709  2111 sgd_solver.cpp:106] Iteration 6600, lr = 0.01
I0822 23:56:43.657196  2111 solver.cpp:229] Iteration 6700, loss = 0.179108
I0822 23:56:43.657219  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0822 23:56:43.657227  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.179108 (* 1 = 0.179108 loss)
I0822 23:56:43.670148  2111 sgd_solver.cpp:106] Iteration 6700, lr = 0.01
I0822 23:57:01.530850  2111 solver.cpp:229] Iteration 6800, loss = 0.227302
I0822 23:57:01.530874  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0822 23:57:01.530880  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.227302 (* 1 = 0.227302 loss)
I0822 23:57:01.541420  2111 sgd_solver.cpp:106] Iteration 6800, lr = 0.01
I0822 23:57:19.620878  2111 solver.cpp:229] Iteration 6900, loss = 0.25374
I0822 23:57:19.620906  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:57:19.620915  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.25374 (* 1 = 0.25374 loss)
I0822 23:57:19.638384  2111 sgd_solver.cpp:106] Iteration 6900, lr = 0.01
I0822 23:57:40.518946  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_7000.caffemodel
I0822 23:57:40.524456  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_7000.solverstate
I0822 23:57:40.526805  2111 solver.cpp:338] Iteration 7000, Testing net (#0)
I0822 23:57:51.695870  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.907368
I0822 23:57:51.695901  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.279326 (* 1 = 0.279326 loss)
I0822 23:57:51.845396  2111 solver.cpp:229] Iteration 7000, loss = 0.389262
I0822 23:57:51.845423  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:57:51.845430  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.389262 (* 1 = 0.389262 loss)
I0822 23:57:51.918354  2111 sgd_solver.cpp:106] Iteration 7000, lr = 0.01
I0822 23:57:51.921782  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0822 23:58:09.720285  2111 solver.cpp:229] Iteration 7100, loss = 0.142972
I0822 23:58:09.720311  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0822 23:58:09.720319  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.142971 (* 1 = 0.142971 loss)
I0822 23:58:09.733275  2111 sgd_solver.cpp:106] Iteration 7100, lr = 0.01
I0822 23:58:27.783943  2111 solver.cpp:229] Iteration 7200, loss = 0.34767
I0822 23:58:27.783972  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:58:27.783978  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.34767 (* 1 = 0.34767 loss)
I0822 23:58:27.793653  2111 sgd_solver.cpp:106] Iteration 7200, lr = 0.01
I0822 23:58:45.711560  2111 solver.cpp:229] Iteration 7300, loss = 0.153629
I0822 23:58:45.711585  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0822 23:58:45.711591  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.153629 (* 1 = 0.153629 loss)
I0822 23:58:45.722601  2111 sgd_solver.cpp:106] Iteration 7300, lr = 0.01
I0822 23:59:03.879549  2111 solver.cpp:229] Iteration 7400, loss = 0.454769
I0822 23:59:03.879571  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.859375
I0822 23:59:03.879578  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.454769 (* 1 = 0.454769 loss)
I0822 23:59:03.890576  2111 sgd_solver.cpp:106] Iteration 7400, lr = 0.01
I0822 23:59:23.404726  2111 solver.cpp:229] Iteration 7500, loss = 0.215026
I0822 23:59:23.404748  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0822 23:59:23.404755  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.215026 (* 1 = 0.215026 loss)
I0822 23:59:23.412413  2111 sgd_solver.cpp:106] Iteration 7500, lr = 0.01
I0822 23:59:41.195580  2111 solver.cpp:229] Iteration 7600, loss = 0.23056
I0822 23:59:41.195605  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0822 23:59:41.195611  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.23056 (* 1 = 0.23056 loss)
I0822 23:59:41.205287  2111 sgd_solver.cpp:106] Iteration 7600, lr = 0.01
I0823 00:00:01.882391  2111 solver.cpp:229] Iteration 7700, loss = 0.38033
I0823 00:00:01.882413  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.859375
I0823 00:00:01.882421  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.38033 (* 1 = 0.38033 loss)
I0823 00:00:01.925206  2111 sgd_solver.cpp:106] Iteration 7700, lr = 0.01
I0823 00:00:24.634487  2111 solver.cpp:229] Iteration 7800, loss = 0.147659
I0823 00:00:24.634510  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:00:24.634517  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.147659 (* 1 = 0.147659 loss)
I0823 00:00:24.634541  2111 sgd_solver.cpp:106] Iteration 7800, lr = 0.01
I0823 00:00:46.760140  2111 solver.cpp:229] Iteration 7900, loss = 0.29308
I0823 00:00:46.760171  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:00:46.760180  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.293079 (* 1 = 0.293079 loss)
I0823 00:00:46.792227  2111 sgd_solver.cpp:106] Iteration 7900, lr = 0.01
I0823 00:01:08.326476  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_8000.caffemodel
I0823 00:01:08.333700  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_8000.solverstate
I0823 00:01:08.337811  2111 solver.cpp:338] Iteration 8000, Testing net (#0)
I0823 00:01:19.496258  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.900131
I0823 00:01:19.496299  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.309312 (* 1 = 0.309312 loss)
I0823 00:01:19.643225  2111 solver.cpp:229] Iteration 8000, loss = 0.209062
I0823 00:01:19.643254  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:01:19.643260  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.209061 (* 1 = 0.209061 loss)
I0823 00:01:19.777602  2111 sgd_solver.cpp:106] Iteration 8000, lr = 0.01
I0823 00:01:19.781255  2180 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:01:42.549494  2111 solver.cpp:229] Iteration 8100, loss = 0.360042
I0823 00:01:42.549525  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.828125
I0823 00:01:42.549535  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.360042 (* 1 = 0.360042 loss)
I0823 00:01:42.553974  2111 sgd_solver.cpp:106] Iteration 8100, lr = 0.01
I0823 00:02:05.299567  2111 solver.cpp:229] Iteration 8200, loss = 0.167926
I0823 00:02:05.299597  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:02:05.299607  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.167926 (* 1 = 0.167926 loss)
I0823 00:02:05.304888  2111 sgd_solver.cpp:106] Iteration 8200, lr = 0.01
I0823 00:02:24.978780  2111 solver.cpp:229] Iteration 8300, loss = 0.299624
I0823 00:02:24.978804  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0823 00:02:24.978811  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.299624 (* 1 = 0.299624 loss)
I0823 00:02:24.998067  2111 sgd_solver.cpp:106] Iteration 8300, lr = 0.01
I0823 00:02:43.304667  2111 solver.cpp:229] Iteration 8400, loss = 0.20333
I0823 00:02:43.304690  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:02:43.304698  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.20333 (* 1 = 0.20333 loss)
I0823 00:02:43.316005  2111 sgd_solver.cpp:106] Iteration 8400, lr = 0.01
I0823 00:03:03.005314  2111 solver.cpp:229] Iteration 8500, loss = 0.500679
I0823 00:03:03.005338  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.84375
I0823 00:03:03.005345  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.500679 (* 1 = 0.500679 loss)
I0823 00:03:03.015658  2111 sgd_solver.cpp:106] Iteration 8500, lr = 0.01
I0823 00:03:22.523905  2111 solver.cpp:229] Iteration 8600, loss = 0.245452
I0823 00:03:22.523931  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:03:22.523937  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.245452 (* 1 = 0.245452 loss)
I0823 00:03:22.523957  2111 sgd_solver.cpp:106] Iteration 8600, lr = 0.01
I0823 00:03:40.224613  2111 solver.cpp:229] Iteration 8700, loss = 0.265111
I0823 00:03:40.224635  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:03:40.224642  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.265111 (* 1 = 0.265111 loss)
I0823 00:03:40.236171  2111 sgd_solver.cpp:106] Iteration 8700, lr = 0.01
I0823 00:03:57.984331  2111 solver.cpp:229] Iteration 8800, loss = 0.236584
I0823 00:03:57.984355  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:03:57.984362  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.236583 (* 1 = 0.236583 loss)
I0823 00:03:57.995713  2111 sgd_solver.cpp:106] Iteration 8800, lr = 0.01
I0823 00:04:15.954640  2111 solver.cpp:229] Iteration 8900, loss = 0.290787
I0823 00:04:15.954664  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:04:15.954671  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.290787 (* 1 = 0.290787 loss)
I0823 00:04:15.965692  2111 sgd_solver.cpp:106] Iteration 8900, lr = 0.01
I0823 00:04:33.313501  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_9000.caffemodel
I0823 00:04:33.318923  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_9000.solverstate
I0823 00:04:33.321247  2111 solver.cpp:338] Iteration 9000, Testing net (#0)
I0823 00:04:44.511433  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.909474
I0823 00:04:44.511462  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.275844 (* 1 = 0.275844 loss)
I0823 00:04:44.660261  2111 solver.cpp:229] Iteration 9000, loss = 0.214344
I0823 00:04:44.660290  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:04:44.660298  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.214343 (* 1 = 0.214343 loss)
I0823 00:04:44.733717  2111 sgd_solver.cpp:106] Iteration 9000, lr = 0.01
I0823 00:04:44.738026  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:05:02.568271  2111 solver.cpp:229] Iteration 9100, loss = 0.156295
I0823 00:05:02.568296  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:05:02.568303  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.156294 (* 1 = 0.156294 loss)
I0823 00:05:02.580497  2111 sgd_solver.cpp:106] Iteration 9100, lr = 0.01
I0823 00:05:22.543015  2111 solver.cpp:229] Iteration 9200, loss = 0.287693
I0823 00:05:22.543041  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:05:22.543053  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.287693 (* 1 = 0.287693 loss)
I0823 00:05:22.577211  2111 sgd_solver.cpp:106] Iteration 9200, lr = 0.01
I0823 00:05:41.577494  2111 solver.cpp:229] Iteration 9300, loss = 0.0758109
I0823 00:05:41.577520  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 1
I0823 00:05:41.577530  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0758105 (* 1 = 0.0758105 loss)
I0823 00:05:41.589517  2111 sgd_solver.cpp:106] Iteration 9300, lr = 0.01
I0823 00:06:00.225513  2111 solver.cpp:229] Iteration 9400, loss = 0.210717
I0823 00:06:00.225535  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:06:00.225543  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.210717 (* 1 = 0.210717 loss)
I0823 00:06:00.261057  2111 sgd_solver.cpp:106] Iteration 9400, lr = 0.01
I0823 00:06:17.994551  2111 solver.cpp:229] Iteration 9500, loss = 0.219929
I0823 00:06:17.994577  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:06:17.994585  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.219929 (* 1 = 0.219929 loss)
I0823 00:06:18.007488  2111 sgd_solver.cpp:106] Iteration 9500, lr = 0.01
I0823 00:06:35.738888  2111 solver.cpp:229] Iteration 9600, loss = 0.137764
I0823 00:06:35.738910  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:06:35.738917  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.137763 (* 1 = 0.137763 loss)
I0823 00:06:35.751430  2111 sgd_solver.cpp:106] Iteration 9600, lr = 0.01
I0823 00:06:53.677917  2111 solver.cpp:229] Iteration 9700, loss = 0.206645
I0823 00:06:53.677942  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:06:53.677949  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.206645 (* 1 = 0.206645 loss)
I0823 00:06:53.691022  2111 sgd_solver.cpp:106] Iteration 9700, lr = 0.01
I0823 00:07:12.517254  2111 solver.cpp:229] Iteration 9800, loss = 0.387212
I0823 00:07:12.517287  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:07:12.517294  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.387211 (* 1 = 0.387211 loss)
I0823 00:07:12.551127  2111 sgd_solver.cpp:106] Iteration 9800, lr = 0.01
I0823 00:07:33.353505  2111 solver.cpp:229] Iteration 9900, loss = 0.257687
I0823 00:07:33.353529  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:07:33.353536  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.257687 (* 1 = 0.257687 loss)
I0823 00:07:33.368127  2111 sgd_solver.cpp:106] Iteration 9900, lr = 0.01
I0823 00:07:51.261576  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_10000.caffemodel
I0823 00:07:51.267007  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_10000.solverstate
I0823 00:07:51.269337  2111 solver.cpp:338] Iteration 10000, Testing net (#0)
I0823 00:08:02.421023  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.893026
I0823 00:08:02.421063  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.328779 (* 1 = 0.328779 loss)
I0823 00:08:02.571776  2111 solver.cpp:229] Iteration 10000, loss = 0.266853
I0823 00:08:02.571805  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 00:08:02.571812  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.266852 (* 1 = 0.266852 loss)
I0823 00:08:02.643945  2111 sgd_solver.cpp:106] Iteration 10000, lr = 0.01
I0823 00:08:02.647557  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:08:20.512202  2111 solver.cpp:229] Iteration 10100, loss = 0.268923
I0823 00:08:20.512228  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:08:20.512234  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.268922 (* 1 = 0.268922 loss)
I0823 00:08:20.523547  2111 sgd_solver.cpp:106] Iteration 10100, lr = 0.01
I0823 00:08:38.348098  2111 solver.cpp:229] Iteration 10200, loss = 0.14011
I0823 00:08:38.348119  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 00:08:38.348125  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.14011 (* 1 = 0.14011 loss)
I0823 00:08:38.358443  2111 sgd_solver.cpp:106] Iteration 10200, lr = 0.01
I0823 00:08:56.029568  2111 solver.cpp:229] Iteration 10300, loss = 0.217884
I0823 00:08:56.029592  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:08:56.029598  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.217884 (* 1 = 0.217884 loss)
I0823 00:08:56.040175  2111 sgd_solver.cpp:106] Iteration 10300, lr = 0.01
I0823 00:09:13.879730  2111 solver.cpp:229] Iteration 10400, loss = 0.390751
I0823 00:09:13.879755  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0823 00:09:13.879762  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.390751 (* 1 = 0.390751 loss)
I0823 00:09:13.892393  2111 sgd_solver.cpp:106] Iteration 10400, lr = 0.01
I0823 00:09:32.369676  2111 solver.cpp:229] Iteration 10500, loss = 0.256636
I0823 00:09:32.369699  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:09:32.369705  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.256636 (* 1 = 0.256636 loss)
I0823 00:09:32.381355  2111 sgd_solver.cpp:106] Iteration 10500, lr = 0.01
I0823 00:09:50.152005  2111 solver.cpp:229] Iteration 10600, loss = 0.201073
I0823 00:09:50.152029  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:09:50.152035  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.201073 (* 1 = 0.201073 loss)
I0823 00:09:50.162477  2111 sgd_solver.cpp:106] Iteration 10600, lr = 0.01
I0823 00:10:09.099311  2111 solver.cpp:229] Iteration 10700, loss = 0.276325
I0823 00:10:09.099340  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:10:09.099349  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.276324 (* 1 = 0.276324 loss)
I0823 00:10:09.168105  2111 sgd_solver.cpp:106] Iteration 10700, lr = 0.01
I0823 00:10:29.971807  2111 solver.cpp:229] Iteration 10800, loss = 0.213287
I0823 00:10:29.971830  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:10:29.971837  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.213286 (* 1 = 0.213286 loss)
I0823 00:10:29.971974  2111 sgd_solver.cpp:106] Iteration 10800, lr = 0.01
I0823 00:10:49.941385  2111 solver.cpp:229] Iteration 10900, loss = 0.208738
I0823 00:10:49.941409  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:10:49.941417  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.208737 (* 1 = 0.208737 loss)
I0823 00:10:49.956182  2111 sgd_solver.cpp:106] Iteration 10900, lr = 0.01
I0823 00:11:07.661942  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_11000.caffemodel
I0823 00:11:07.667356  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_11000.solverstate
I0823 00:11:07.669682  2111 solver.cpp:338] Iteration 11000, Testing net (#0)
I0823 00:11:18.824146  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.887369
I0823 00:11:18.824173  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.322953 (* 1 = 0.322953 loss)
I0823 00:11:18.972412  2111 solver.cpp:229] Iteration 11000, loss = 0.134948
I0823 00:11:18.972442  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:11:18.972450  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.134947 (* 1 = 0.134947 loss)
I0823 00:11:19.052713  2111 sgd_solver.cpp:106] Iteration 11000, lr = 0.01
I0823 00:11:19.056779  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:11:36.799824  2111 solver.cpp:229] Iteration 11100, loss = 0.120121
I0823 00:11:36.799849  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 00:11:36.799855  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.12012 (* 1 = 0.12012 loss)
I0823 00:11:36.810629  2111 sgd_solver.cpp:106] Iteration 11100, lr = 0.01
I0823 00:11:58.109828  2111 solver.cpp:229] Iteration 11200, loss = 0.293192
I0823 00:11:58.109853  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:11:58.109859  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.293191 (* 1 = 0.293191 loss)
I0823 00:11:58.123188  2111 sgd_solver.cpp:106] Iteration 11200, lr = 0.01
I0823 00:12:15.707757  2111 solver.cpp:229] Iteration 11300, loss = 0.239437
I0823 00:12:15.707782  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:12:15.707789  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.239436 (* 1 = 0.239436 loss)
I0823 00:12:15.719314  2111 sgd_solver.cpp:106] Iteration 11300, lr = 0.01
I0823 00:12:33.600819  2111 solver.cpp:229] Iteration 11400, loss = 0.188025
I0823 00:12:33.600844  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:12:33.600852  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.188025 (* 1 = 0.188025 loss)
I0823 00:12:33.611220  2111 sgd_solver.cpp:106] Iteration 11400, lr = 0.01
I0823 00:12:51.380383  2111 solver.cpp:229] Iteration 11500, loss = 0.282874
I0823 00:12:51.380411  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.84375
I0823 00:12:51.380419  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.282873 (* 1 = 0.282873 loss)
I0823 00:12:51.412245  2111 sgd_solver.cpp:106] Iteration 11500, lr = 0.01
I0823 00:13:09.345201  2111 solver.cpp:229] Iteration 11600, loss = 0.335512
I0823 00:13:09.345226  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.84375
I0823 00:13:09.345232  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.335511 (* 1 = 0.335511 loss)
I0823 00:13:09.355911  2111 sgd_solver.cpp:106] Iteration 11600, lr = 0.01
I0823 00:13:27.268988  2111 solver.cpp:229] Iteration 11700, loss = 0.338784
I0823 00:13:27.269013  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.828125
I0823 00:13:27.269021  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.338783 (* 1 = 0.338783 loss)
I0823 00:13:27.281841  2111 sgd_solver.cpp:106] Iteration 11700, lr = 0.01
I0823 00:13:44.978375  2111 solver.cpp:229] Iteration 11800, loss = 0.17249
I0823 00:13:44.978399  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:13:44.978406  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.172489 (* 1 = 0.172489 loss)
I0823 00:13:44.989307  2111 sgd_solver.cpp:106] Iteration 11800, lr = 0.01
I0823 00:14:04.338856  2111 solver.cpp:229] Iteration 11900, loss = 0.218385
I0823 00:14:04.338881  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:14:04.338887  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.218385 (* 1 = 0.218385 loss)
I0823 00:14:04.350697  2111 sgd_solver.cpp:106] Iteration 11900, lr = 0.01
I0823 00:14:21.919534  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_12000.caffemodel
I0823 00:14:21.924983  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_12000.solverstate
I0823 00:14:21.927364  2111 solver.cpp:338] Iteration 12000, Testing net (#0)
I0823 00:14:33.001755  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.915395
I0823 00:14:33.001785  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.264264 (* 1 = 0.264264 loss)
I0823 00:14:33.150554  2111 solver.cpp:229] Iteration 12000, loss = 0.246708
I0823 00:14:33.150584  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0823 00:14:33.150593  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.246707 (* 1 = 0.246707 loss)
I0823 00:14:33.242725  2111 sgd_solver.cpp:106] Iteration 12000, lr = 0.01
I0823 00:14:33.246325  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:14:54.227687  2111 solver.cpp:229] Iteration 12100, loss = 0.538819
I0823 00:14:54.227710  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.859375
I0823 00:14:54.227717  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.538819 (* 1 = 0.538819 loss)
I0823 00:14:54.240257  2111 sgd_solver.cpp:106] Iteration 12100, lr = 0.01
I0823 00:15:11.873145  2111 solver.cpp:229] Iteration 12200, loss = 0.192592
I0823 00:15:11.873172  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:15:11.873179  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.192592 (* 1 = 0.192592 loss)
I0823 00:15:11.886023  2111 sgd_solver.cpp:106] Iteration 12200, lr = 0.01
I0823 00:15:29.492071  2111 solver.cpp:229] Iteration 12300, loss = 0.215135
I0823 00:15:29.492092  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:15:29.492100  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.215134 (* 1 = 0.215134 loss)
I0823 00:15:29.504415  2111 sgd_solver.cpp:106] Iteration 12300, lr = 0.01
I0823 00:15:47.144224  2111 solver.cpp:229] Iteration 12400, loss = 0.334885
I0823 00:15:47.144248  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0823 00:15:47.144254  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.334885 (* 1 = 0.334885 loss)
I0823 00:15:47.155581  2111 sgd_solver.cpp:106] Iteration 12400, lr = 0.01
I0823 00:16:05.213822  2111 solver.cpp:229] Iteration 12500, loss = 0.255616
I0823 00:16:05.213845  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:16:05.213852  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.255616 (* 1 = 0.255616 loss)
I0823 00:16:05.224557  2111 sgd_solver.cpp:106] Iteration 12500, lr = 0.01
I0823 00:16:22.724558  2111 solver.cpp:229] Iteration 12600, loss = 0.178579
I0823 00:16:22.724582  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:16:22.724591  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.178579 (* 1 = 0.178579 loss)
I0823 00:16:22.735508  2111 sgd_solver.cpp:106] Iteration 12600, lr = 0.01
I0823 00:16:40.256515  2111 solver.cpp:229] Iteration 12700, loss = 0.13791
I0823 00:16:40.256536  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:16:40.256543  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.13791 (* 1 = 0.13791 loss)
I0823 00:16:40.266348  2111 sgd_solver.cpp:106] Iteration 12700, lr = 0.01
I0823 00:16:58.051280  2111 solver.cpp:229] Iteration 12800, loss = 0.584366
I0823 00:16:58.051309  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.84375
I0823 00:16:58.051316  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.584366 (* 1 = 0.584366 loss)
I0823 00:16:58.064344  2111 sgd_solver.cpp:106] Iteration 12800, lr = 0.01
I0823 00:17:15.802640  2111 solver.cpp:229] Iteration 12900, loss = 0.248663
I0823 00:17:15.802662  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:17:15.802669  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.248663 (* 1 = 0.248663 loss)
I0823 00:17:15.812639  2111 sgd_solver.cpp:106] Iteration 12900, lr = 0.01
I0823 00:17:33.364174  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_13000.caffemodel
I0823 00:17:33.369565  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_13000.solverstate
I0823 00:17:33.371917  2111 solver.cpp:338] Iteration 13000, Testing net (#0)
I0823 00:17:44.460934  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.898158
I0823 00:17:44.460963  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.302434 (* 1 = 0.302434 loss)
I0823 00:17:44.610633  2111 solver.cpp:229] Iteration 13000, loss = 0.23943
I0823 00:17:44.610661  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:17:44.610668  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.239429 (* 1 = 0.239429 loss)
I0823 00:17:44.680873  2111 sgd_solver.cpp:106] Iteration 13000, lr = 0.01
I0823 00:17:44.685204  2180 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:18:03.826526  2111 solver.cpp:229] Iteration 13100, loss = 0.235204
I0823 00:18:03.826552  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:18:03.826560  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.235203 (* 1 = 0.235203 loss)
I0823 00:18:03.858000  2111 sgd_solver.cpp:106] Iteration 13100, lr = 0.01
I0823 00:18:25.296617  2111 solver.cpp:229] Iteration 13200, loss = 0.175535
I0823 00:18:25.296641  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:18:25.296648  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.175535 (* 1 = 0.175535 loss)
I0823 00:18:25.307636  2111 sgd_solver.cpp:106] Iteration 13200, lr = 0.01
I0823 00:18:42.689791  2111 solver.cpp:229] Iteration 13300, loss = 0.0865661
I0823 00:18:42.689815  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 00:18:42.689822  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0865656 (* 1 = 0.0865656 loss)
I0823 00:18:42.702579  2111 sgd_solver.cpp:106] Iteration 13300, lr = 0.01
I0823 00:19:00.051648  2111 solver.cpp:229] Iteration 13400, loss = 0.103231
I0823 00:19:00.051672  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.984375
I0823 00:19:00.051679  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.10323 (* 1 = 0.10323 loss)
I0823 00:19:00.086374  2111 sgd_solver.cpp:106] Iteration 13400, lr = 0.01
I0823 00:19:17.606837  2111 solver.cpp:229] Iteration 13500, loss = 0.29515
I0823 00:19:17.606863  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:19:17.606869  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.29515 (* 1 = 0.29515 loss)
I0823 00:19:17.618211  2111 sgd_solver.cpp:106] Iteration 13500, lr = 0.01
I0823 00:19:36.523488  2111 solver.cpp:229] Iteration 13600, loss = 0.180846
I0823 00:19:36.523511  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:19:36.523519  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.180846 (* 1 = 0.180846 loss)
I0823 00:19:36.545732  2111 sgd_solver.cpp:106] Iteration 13600, lr = 0.01
I0823 00:19:55.542594  2111 solver.cpp:229] Iteration 13700, loss = 0.145388
I0823 00:19:55.542616  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 00:19:55.542623  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.145388 (* 1 = 0.145388 loss)
I0823 00:19:55.542644  2111 sgd_solver.cpp:106] Iteration 13700, lr = 0.01
I0823 00:20:14.647691  2111 solver.cpp:229] Iteration 13800, loss = 0.214244
I0823 00:20:14.647716  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:20:14.647722  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.214243 (* 1 = 0.214243 loss)
I0823 00:20:14.659091  2111 sgd_solver.cpp:106] Iteration 13800, lr = 0.01
I0823 00:20:37.378406  2111 solver.cpp:229] Iteration 13900, loss = 0.285874
I0823 00:20:37.378430  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0823 00:20:37.378437  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.285873 (* 1 = 0.285873 loss)
I0823 00:20:37.390228  2111 sgd_solver.cpp:106] Iteration 13900, lr = 0.01
I0823 00:20:54.890785  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_14000.caffemodel
I0823 00:20:54.896157  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_14000.solverstate
I0823 00:20:54.898478  2111 solver.cpp:338] Iteration 14000, Testing net (#0)
I0823 00:21:06.541136  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.916053
I0823 00:21:06.541587  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.249242 (* 1 = 0.249242 loss)
I0823 00:21:06.767654  2111 solver.cpp:229] Iteration 14000, loss = 0.220709
I0823 00:21:06.767686  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:21:06.767700  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.220709 (* 1 = 0.220709 loss)
I0823 00:21:06.776979  2111 sgd_solver.cpp:106] Iteration 14000, lr = 0.01
I0823 00:21:06.780755  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:21:29.348081  2111 solver.cpp:229] Iteration 14100, loss = 0.196368
I0823 00:21:29.348110  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 00:21:29.348119  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.196368 (* 1 = 0.196368 loss)
I0823 00:21:29.366843  2111 sgd_solver.cpp:106] Iteration 14100, lr = 0.01
I0823 00:21:47.774101  2111 solver.cpp:229] Iteration 14200, loss = 0.16442
I0823 00:21:47.774125  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:21:47.774132  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.164419 (* 1 = 0.164419 loss)
I0823 00:21:47.786733  2111 sgd_solver.cpp:106] Iteration 14200, lr = 0.01
I0823 00:22:05.370937  2111 solver.cpp:229] Iteration 14300, loss = 0.291944
I0823 00:22:05.370961  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:22:05.370967  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.291943 (* 1 = 0.291943 loss)
I0823 00:22:05.382747  2111 sgd_solver.cpp:106] Iteration 14300, lr = 0.01
I0823 00:22:24.919065  2111 solver.cpp:229] Iteration 14400, loss = 0.269222
I0823 00:22:24.919090  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:22:24.919096  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.269222 (* 1 = 0.269222 loss)
I0823 00:22:24.930390  2111 sgd_solver.cpp:106] Iteration 14400, lr = 0.01
I0823 00:22:45.273986  2111 solver.cpp:229] Iteration 14500, loss = 0.262229
I0823 00:22:45.274016  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0823 00:22:45.274026  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.262229 (* 1 = 0.262229 loss)
I0823 00:22:45.278750  2111 sgd_solver.cpp:106] Iteration 14500, lr = 0.01
I0823 00:23:07.463621  2111 solver.cpp:229] Iteration 14600, loss = 0.191082
I0823 00:23:07.463647  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 00:23:07.463659  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.191082 (* 1 = 0.191082 loss)
I0823 00:23:07.468255  2111 sgd_solver.cpp:106] Iteration 14600, lr = 0.01
I0823 00:23:29.619571  2111 solver.cpp:229] Iteration 14700, loss = 0.156168
I0823 00:23:29.619611  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:23:29.619621  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.156167 (* 1 = 0.156167 loss)
I0823 00:23:29.625171  2111 sgd_solver.cpp:106] Iteration 14700, lr = 0.01
I0823 00:23:48.904760  2111 solver.cpp:229] Iteration 14800, loss = 0.388127
I0823 00:23:48.904781  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:23:48.904788  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.388127 (* 1 = 0.388127 loss)
I0823 00:23:48.915699  2111 sgd_solver.cpp:106] Iteration 14800, lr = 0.01
I0823 00:24:08.750782  2111 solver.cpp:229] Iteration 14900, loss = 0.544223
I0823 00:24:08.750809  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0823 00:24:08.750816  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.544223 (* 1 = 0.544223 loss)
I0823 00:24:08.782027  2111 sgd_solver.cpp:106] Iteration 14900, lr = 0.01
I0823 00:24:26.483330  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_15000.caffemodel
I0823 00:24:26.491047  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_15000.solverstate
I0823 00:24:26.495180  2111 solver.cpp:338] Iteration 15000, Testing net (#0)
I0823 00:24:37.603780  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.901974
I0823 00:24:37.603821  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.284684 (* 1 = 0.284684 loss)
I0823 00:24:37.752014  2111 solver.cpp:229] Iteration 15000, loss = 0.228322
I0823 00:24:37.752043  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:24:37.752050  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.228322 (* 1 = 0.228322 loss)
I0823 00:24:37.846734  2111 sgd_solver.cpp:106] Iteration 15000, lr = 0.01
I0823 00:24:37.850522  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:24:56.051318  2111 solver.cpp:229] Iteration 15100, loss = 0.163139
I0823 00:24:56.051342  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:24:56.051349  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.163138 (* 1 = 0.163138 loss)
I0823 00:24:56.062825  2111 sgd_solver.cpp:106] Iteration 15100, lr = 0.01
I0823 00:25:17.203982  2111 solver.cpp:229] Iteration 15200, loss = 0.16553
I0823 00:25:17.204008  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:25:17.204016  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.165529 (* 1 = 0.165529 loss)
I0823 00:25:17.230289  2111 sgd_solver.cpp:106] Iteration 15200, lr = 0.01
I0823 00:25:39.074507  2111 solver.cpp:229] Iteration 15300, loss = 0.4191
I0823 00:25:39.074534  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:25:39.074546  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.419099 (* 1 = 0.419099 loss)
I0823 00:25:39.108292  2111 sgd_solver.cpp:106] Iteration 15300, lr = 0.01
I0823 00:26:00.974252  2111 solver.cpp:229] Iteration 15400, loss = 0.296988
I0823 00:26:00.974277  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 00:26:00.974283  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.296988 (* 1 = 0.296988 loss)
I0823 00:26:00.985879  2111 sgd_solver.cpp:106] Iteration 15400, lr = 0.01
I0823 00:26:18.982264  2111 solver.cpp:229] Iteration 15500, loss = 0.20905
I0823 00:26:18.982287  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:26:18.982295  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.209049 (* 1 = 0.209049 loss)
I0823 00:26:19.011735  2111 sgd_solver.cpp:106] Iteration 15500, lr = 0.01
I0823 00:26:40.534807  2111 solver.cpp:229] Iteration 15600, loss = 0.404516
I0823 00:26:40.534831  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:26:40.534848  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.404516 (* 1 = 0.404516 loss)
I0823 00:26:40.545594  2111 sgd_solver.cpp:106] Iteration 15600, lr = 0.01
I0823 00:27:01.572124  2111 solver.cpp:229] Iteration 15700, loss = 0.330606
I0823 00:27:01.572150  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0823 00:27:01.572161  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.330605 (* 1 = 0.330605 loss)
I0823 00:27:01.583371  2111 sgd_solver.cpp:106] Iteration 15700, lr = 0.01
I0823 00:27:19.813648  2111 solver.cpp:229] Iteration 15800, loss = 0.28176
I0823 00:27:19.813673  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:27:19.813678  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.28176 (* 1 = 0.28176 loss)
I0823 00:27:19.824239  2111 sgd_solver.cpp:106] Iteration 15800, lr = 0.01
I0823 00:27:37.571316  2111 solver.cpp:229] Iteration 15900, loss = 0.198292
I0823 00:27:37.571354  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:27:37.571369  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.198291 (* 1 = 0.198291 loss)
I0823 00:27:37.571409  2111 sgd_solver.cpp:106] Iteration 15900, lr = 0.01
I0823 00:27:55.283018  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_16000.caffemodel
I0823 00:27:55.288375  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_16000.solverstate
I0823 00:27:55.290688  2111 solver.cpp:338] Iteration 16000, Testing net (#0)
I0823 00:28:06.367765  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.905263
I0823 00:28:06.367795  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.283126 (* 1 = 0.283126 loss)
I0823 00:28:06.595682  2111 solver.cpp:229] Iteration 16000, loss = 0.114756
I0823 00:28:06.595712  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:28:06.595726  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.114756 (* 1 = 0.114756 loss)
I0823 00:28:06.611719  2111 sgd_solver.cpp:106] Iteration 16000, lr = 0.01
I0823 00:28:06.615432  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:28:25.621280  2111 solver.cpp:229] Iteration 16100, loss = 0.264165
I0823 00:28:25.621304  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:28:25.621311  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.264164 (* 1 = 0.264164 loss)
I0823 00:28:25.633774  2111 sgd_solver.cpp:106] Iteration 16100, lr = 0.01
I0823 00:28:43.334635  2111 solver.cpp:229] Iteration 16200, loss = 0.454258
I0823 00:28:43.334661  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0823 00:28:43.334667  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.454257 (* 1 = 0.454257 loss)
I0823 00:28:43.345345  2111 sgd_solver.cpp:106] Iteration 16200, lr = 0.01
I0823 00:29:01.179543  2111 solver.cpp:229] Iteration 16300, loss = 0.207093
I0823 00:29:01.179563  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:29:01.179570  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.207092 (* 1 = 0.207092 loss)
I0823 00:29:01.191073  2111 sgd_solver.cpp:106] Iteration 16300, lr = 0.01
I0823 00:29:18.895212  2111 solver.cpp:229] Iteration 16400, loss = 0.472835
I0823 00:29:18.895234  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.859375
I0823 00:29:18.895241  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.472834 (* 1 = 0.472834 loss)
I0823 00:29:18.905218  2111 sgd_solver.cpp:106] Iteration 16400, lr = 0.01
I0823 00:29:38.723575  2111 solver.cpp:229] Iteration 16500, loss = 0.209856
I0823 00:29:38.723603  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:29:38.723609  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.209855 (* 1 = 0.209855 loss)
I0823 00:29:38.786497  2111 sgd_solver.cpp:106] Iteration 16500, lr = 0.01
I0823 00:30:01.462891  2111 solver.cpp:229] Iteration 16600, loss = 0.333551
I0823 00:30:01.462911  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:30:01.462918  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.333551 (* 1 = 0.333551 loss)
I0823 00:30:01.473762  2111 sgd_solver.cpp:106] Iteration 16600, lr = 0.01
I0823 00:30:24.467833  2111 solver.cpp:229] Iteration 16700, loss = 0.322879
I0823 00:30:24.467854  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0823 00:30:24.467860  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.322879 (* 1 = 0.322879 loss)
I0823 00:30:24.467882  2111 sgd_solver.cpp:106] Iteration 16700, lr = 0.01
I0823 00:30:47.760403  2111 solver.cpp:229] Iteration 16800, loss = 0.0984703
I0823 00:30:47.760432  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.984375
I0823 00:30:47.760442  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0984697 (* 1 = 0.0984697 loss)
I0823 00:30:47.765918  2111 sgd_solver.cpp:106] Iteration 16800, lr = 0.01
I0823 00:31:06.882243  2111 solver.cpp:229] Iteration 16900, loss = 0.259598
I0823 00:31:06.882268  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0823 00:31:06.882274  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.259597 (* 1 = 0.259597 loss)
I0823 00:31:06.897004  2111 sgd_solver.cpp:106] Iteration 16900, lr = 0.01
I0823 00:31:25.407197  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_17000.caffemodel
I0823 00:31:25.417691  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_17000.solverstate
I0823 00:31:25.422578  2111 solver.cpp:338] Iteration 17000, Testing net (#0)
I0823 00:31:36.556687  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.901974
I0823 00:31:36.556717  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.285372 (* 1 = 0.285372 loss)
I0823 00:31:36.704385  2111 solver.cpp:229] Iteration 17000, loss = 0.29751
I0823 00:31:36.704411  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:31:36.704417  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.297509 (* 1 = 0.297509 loss)
I0823 00:31:36.777395  2111 sgd_solver.cpp:106] Iteration 17000, lr = 0.01
I0823 00:31:36.781078  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:31:54.831302  2111 solver.cpp:229] Iteration 17100, loss = 0.236122
I0823 00:31:54.831327  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0823 00:31:54.831334  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.236122 (* 1 = 0.236122 loss)
I0823 00:31:54.843889  2111 sgd_solver.cpp:106] Iteration 17100, lr = 0.01
I0823 00:32:13.478966  2111 solver.cpp:229] Iteration 17200, loss = 0.18567
I0823 00:32:13.478994  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:32:13.479008  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.18567 (* 1 = 0.18567 loss)
I0823 00:32:13.484532  2111 sgd_solver.cpp:106] Iteration 17200, lr = 0.01
I0823 00:32:36.553059  2111 solver.cpp:229] Iteration 17300, loss = 0.103529
I0823 00:32:36.553083  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 00:32:36.553091  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.103528 (* 1 = 0.103528 loss)
I0823 00:32:36.564961  2111 sgd_solver.cpp:106] Iteration 17300, lr = 0.01
I0823 00:32:56.264899  2111 solver.cpp:229] Iteration 17400, loss = 0.216043
I0823 00:32:56.264930  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:32:56.264941  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.216043 (* 1 = 0.216043 loss)
I0823 00:32:56.310277  2111 sgd_solver.cpp:106] Iteration 17400, lr = 0.01
I0823 00:33:16.230747  2111 solver.cpp:229] Iteration 17500, loss = 0.257335
I0823 00:33:16.230773  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:33:16.230784  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.257335 (* 1 = 0.257335 loss)
I0823 00:33:16.290526  2111 sgd_solver.cpp:106] Iteration 17500, lr = 0.01
I0823 00:33:37.950953  2111 solver.cpp:229] Iteration 17600, loss = 0.156478
I0823 00:33:37.950984  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:33:37.950995  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.156478 (* 1 = 0.156478 loss)
I0823 00:33:37.983934  2111 sgd_solver.cpp:106] Iteration 17600, lr = 0.01
I0823 00:33:58.688136  2111 solver.cpp:229] Iteration 17700, loss = 0.526031
I0823 00:33:58.688161  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.859375
I0823 00:33:58.688171  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.52603 (* 1 = 0.52603 loss)
I0823 00:33:58.698554  2111 sgd_solver.cpp:106] Iteration 17700, lr = 0.01
I0823 00:34:16.437871  2111 solver.cpp:229] Iteration 17800, loss = 0.127858
I0823 00:34:16.437893  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:34:16.437902  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.127858 (* 1 = 0.127858 loss)
I0823 00:34:16.437950  2111 sgd_solver.cpp:106] Iteration 17800, lr = 0.01
I0823 00:34:35.853029  2111 solver.cpp:229] Iteration 17900, loss = 0.42419
I0823 00:34:35.853054  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0823 00:34:35.853063  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.424189 (* 1 = 0.424189 loss)
I0823 00:34:35.863775  2111 sgd_solver.cpp:106] Iteration 17900, lr = 0.01
I0823 00:34:57.123023  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_18000.caffemodel
I0823 00:34:57.128372  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_18000.solverstate
I0823 00:34:57.130674  2111 solver.cpp:338] Iteration 18000, Testing net (#0)
I0823 00:35:08.354904  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.914474
I0823 00:35:08.354933  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.283125 (* 1 = 0.283125 loss)
I0823 00:35:08.504439  2111 solver.cpp:229] Iteration 18000, loss = 0.281201
I0823 00:35:08.504467  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0823 00:35:08.504474  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.281201 (* 1 = 0.281201 loss)
I0823 00:35:08.575851  2111 sgd_solver.cpp:106] Iteration 18000, lr = 0.01
I0823 00:35:08.580217  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:35:26.472950  2111 solver.cpp:229] Iteration 18100, loss = 0.319552
I0823 00:35:26.472973  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0823 00:35:26.472980  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.319551 (* 1 = 0.319551 loss)
I0823 00:35:26.483311  2111 sgd_solver.cpp:106] Iteration 18100, lr = 0.01
I0823 00:35:45.235378  2111 solver.cpp:229] Iteration 18200, loss = 0.196906
I0823 00:35:45.235402  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:35:45.235409  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.196905 (* 1 = 0.196905 loss)
I0823 00:35:45.235435  2111 sgd_solver.cpp:106] Iteration 18200, lr = 0.01
I0823 00:36:03.621449  2111 solver.cpp:229] Iteration 18300, loss = 0.216624
I0823 00:36:03.621474  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:36:03.621480  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.216623 (* 1 = 0.216623 loss)
I0823 00:36:03.631739  2111 sgd_solver.cpp:106] Iteration 18300, lr = 0.01
I0823 00:36:22.446403  2111 solver.cpp:229] Iteration 18400, loss = 0.28621
I0823 00:36:22.446425  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0823 00:36:22.446432  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.286209 (* 1 = 0.286209 loss)
I0823 00:36:22.457855  2111 sgd_solver.cpp:106] Iteration 18400, lr = 0.01
I0823 00:36:42.837301  2111 solver.cpp:229] Iteration 18500, loss = 0.135063
I0823 00:36:42.837324  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:36:42.837332  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.135063 (* 1 = 0.135063 loss)
I0823 00:36:42.837352  2111 sgd_solver.cpp:106] Iteration 18500, lr = 0.01
I0823 00:37:02.652235  2111 solver.cpp:229] Iteration 18600, loss = 0.232131
I0823 00:37:02.652259  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:37:02.652266  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.23213 (* 1 = 0.23213 loss)
I0823 00:37:02.675882  2111 sgd_solver.cpp:106] Iteration 18600, lr = 0.01
I0823 00:37:20.513056  2111 solver.cpp:229] Iteration 18700, loss = 0.277578
I0823 00:37:20.513079  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0823 00:37:20.513087  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.277578 (* 1 = 0.277578 loss)
I0823 00:37:20.523789  2111 sgd_solver.cpp:106] Iteration 18700, lr = 0.01
I0823 00:37:38.223059  2111 solver.cpp:229] Iteration 18800, loss = 0.136119
I0823 00:37:38.223083  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:37:38.223090  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.136119 (* 1 = 0.136119 loss)
I0823 00:37:38.234712  2111 sgd_solver.cpp:106] Iteration 18800, lr = 0.01
I0823 00:37:57.036288  2111 solver.cpp:229] Iteration 18900, loss = 0.145091
I0823 00:37:57.036314  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:37:57.036320  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.14509 (* 1 = 0.14509 loss)
I0823 00:37:57.049542  2111 sgd_solver.cpp:106] Iteration 18900, lr = 0.01
I0823 00:38:14.574829  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_19000.caffemodel
I0823 00:38:14.580221  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_19000.solverstate
I0823 00:38:14.582562  2111 solver.cpp:338] Iteration 19000, Testing net (#0)
I0823 00:38:25.666364  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.902237
I0823 00:38:25.666406  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.342708 (* 1 = 0.342708 loss)
I0823 00:38:25.815117  2111 solver.cpp:229] Iteration 19000, loss = 0.297566
I0823 00:38:25.815145  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0823 00:38:25.815151  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.297565 (* 1 = 0.297565 loss)
I0823 00:38:25.888553  2111 sgd_solver.cpp:106] Iteration 19000, lr = 0.01
I0823 00:38:25.892244  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:38:47.174423  2111 solver.cpp:229] Iteration 19100, loss = 0.183015
I0823 00:38:47.174450  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:38:47.174458  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.183015 (* 1 = 0.183015 loss)
I0823 00:38:47.203603  2111 sgd_solver.cpp:106] Iteration 19100, lr = 0.01
I0823 00:39:05.237566  2111 solver.cpp:229] Iteration 19200, loss = 0.206875
I0823 00:39:05.237589  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:39:05.237596  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.206874 (* 1 = 0.206874 loss)
I0823 00:39:05.248953  2111 sgd_solver.cpp:106] Iteration 19200, lr = 0.01
I0823 00:39:22.883779  2111 solver.cpp:229] Iteration 19300, loss = 0.221417
I0823 00:39:22.883803  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:39:22.883810  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.221416 (* 1 = 0.221416 loss)
I0823 00:39:22.894670  2111 sgd_solver.cpp:106] Iteration 19300, lr = 0.01
I0823 00:39:41.952307  2111 solver.cpp:229] Iteration 19400, loss = 0.123856
I0823 00:39:41.952337  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:39:41.952347  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.123856 (* 1 = 0.123856 loss)
I0823 00:39:41.957650  2111 sgd_solver.cpp:106] Iteration 19400, lr = 0.01
I0823 00:40:02.676105  2111 solver.cpp:229] Iteration 19500, loss = 0.379872
I0823 00:40:02.676128  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0823 00:40:02.676136  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.379871 (* 1 = 0.379871 loss)
I0823 00:40:02.687758  2111 sgd_solver.cpp:106] Iteration 19500, lr = 0.01
I0823 00:40:20.589445  2111 solver.cpp:229] Iteration 19600, loss = 0.189375
I0823 00:40:20.589468  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:40:20.589474  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.189375 (* 1 = 0.189375 loss)
I0823 00:40:20.599977  2111 sgd_solver.cpp:106] Iteration 19600, lr = 0.01
I0823 00:40:38.317782  2111 solver.cpp:229] Iteration 19700, loss = 0.259495
I0823 00:40:38.317806  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:40:38.317813  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.259495 (* 1 = 0.259495 loss)
I0823 00:40:38.328145  2111 sgd_solver.cpp:106] Iteration 19700, lr = 0.01
I0823 00:40:55.969895  2111 solver.cpp:229] Iteration 19800, loss = 0.310782
I0823 00:40:55.969919  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:40:55.969926  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.310781 (* 1 = 0.310781 loss)
I0823 00:40:55.984381  2111 sgd_solver.cpp:106] Iteration 19800, lr = 0.01
I0823 00:41:13.845211  2111 solver.cpp:229] Iteration 19900, loss = 0.199368
I0823 00:41:13.845237  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:41:13.845244  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.199367 (* 1 = 0.199367 loss)
I0823 00:41:13.860863  2111 sgd_solver.cpp:106] Iteration 19900, lr = 0.01
I0823 00:41:33.442878  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_20000.caffemodel
I0823 00:41:33.448297  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_20000.solverstate
I0823 00:41:33.450650  2111 solver.cpp:338] Iteration 20000, Testing net (#0)
I0823 00:41:44.532754  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.915263
I0823 00:41:44.532784  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.331544 (* 1 = 0.331544 loss)
I0823 00:41:44.768301  2111 solver.cpp:229] Iteration 20000, loss = 0.253521
I0823 00:41:44.768332  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:41:44.768340  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.25352 (* 1 = 0.25352 loss)
I0823 00:41:44.790735  2111 sgd_solver.cpp:106] Iteration 20000, lr = 0.01
I0823 00:41:44.797329  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:42:04.226581  2111 solver.cpp:229] Iteration 20100, loss = 0.329036
I0823 00:42:04.226614  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0823 00:42:04.226626  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.329036 (* 1 = 0.329036 loss)
I0823 00:42:04.266106  2111 sgd_solver.cpp:106] Iteration 20100, lr = 0.01
I0823 00:42:24.960495  2111 solver.cpp:229] Iteration 20200, loss = 0.157475
I0823 00:42:24.960520  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:42:24.960527  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.157475 (* 1 = 0.157475 loss)
I0823 00:42:24.971081  2111 sgd_solver.cpp:106] Iteration 20200, lr = 0.01
I0823 00:42:42.842265  2111 solver.cpp:229] Iteration 20300, loss = 0.186934
I0823 00:42:42.842288  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:42:42.842295  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.186934 (* 1 = 0.186934 loss)
I0823 00:42:42.854220  2111 sgd_solver.cpp:106] Iteration 20300, lr = 0.01
I0823 00:43:00.674648  2111 solver.cpp:229] Iteration 20400, loss = 0.26386
I0823 00:43:00.674671  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:43:00.674679  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.263859 (* 1 = 0.263859 loss)
I0823 00:43:00.684512  2111 sgd_solver.cpp:106] Iteration 20400, lr = 0.01
I0823 00:43:19.447340  2111 solver.cpp:229] Iteration 20500, loss = 0.12638
I0823 00:43:19.447361  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:43:19.447371  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.126379 (* 1 = 0.126379 loss)
I0823 00:43:19.460346  2111 sgd_solver.cpp:106] Iteration 20500, lr = 0.01
I0823 00:43:37.222685  2111 solver.cpp:229] Iteration 20600, loss = 0.104455
I0823 00:43:37.222709  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:43:37.222719  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.104455 (* 1 = 0.104455 loss)
I0823 00:43:37.233321  2111 sgd_solver.cpp:106] Iteration 20600, lr = 0.01
I0823 00:43:57.563647  2111 solver.cpp:229] Iteration 20700, loss = 0.204573
I0823 00:43:57.563668  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:43:57.563674  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.204572 (* 1 = 0.204572 loss)
I0823 00:43:57.563694  2111 sgd_solver.cpp:106] Iteration 20700, lr = 0.01
I0823 00:44:20.443540  2111 solver.cpp:229] Iteration 20800, loss = 0.239139
I0823 00:44:20.443567  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:44:20.443578  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.239138 (* 1 = 0.239138 loss)
I0823 00:44:20.448405  2111 sgd_solver.cpp:106] Iteration 20800, lr = 0.01
I0823 00:44:43.395637  2111 solver.cpp:229] Iteration 20900, loss = 0.304012
I0823 00:44:43.395665  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:44:43.395675  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.304011 (* 1 = 0.304011 loss)
I0823 00:44:43.400708  2111 sgd_solver.cpp:106] Iteration 20900, lr = 0.01
I0823 00:45:06.162855  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_21000.caffemodel
I0823 00:45:06.168210  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_21000.solverstate
I0823 00:45:06.170524  2111 solver.cpp:338] Iteration 21000, Testing net (#0)
I0823 00:45:17.268740  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.896053
I0823 00:45:17.268771  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.292557 (* 1 = 0.292557 loss)
I0823 00:45:17.496258  2111 solver.cpp:229] Iteration 21000, loss = 0.17433
I0823 00:45:17.496285  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 00:45:17.496299  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.174329 (* 1 = 0.174329 loss)
I0823 00:45:17.512516  2111 sgd_solver.cpp:106] Iteration 21000, lr = 0.01
I0823 00:45:17.516626  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:45:36.592217  2111 solver.cpp:229] Iteration 21100, loss = 0.218098
I0823 00:45:36.592242  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:45:36.592247  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.218097 (* 1 = 0.218097 loss)
I0823 00:45:36.603022  2111 sgd_solver.cpp:106] Iteration 21100, lr = 0.01
I0823 00:45:54.232515  2111 solver.cpp:229] Iteration 21200, loss = 0.228532
I0823 00:45:54.232539  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:45:54.232547  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.228531 (* 1 = 0.228531 loss)
I0823 00:45:54.242723  2111 sgd_solver.cpp:106] Iteration 21200, lr = 0.01
I0823 00:46:12.158763  2111 solver.cpp:229] Iteration 21300, loss = 0.248703
I0823 00:46:12.158790  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:46:12.158798  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.248702 (* 1 = 0.248702 loss)
I0823 00:46:12.170222  2111 sgd_solver.cpp:106] Iteration 21300, lr = 0.01
I0823 00:46:30.911290  2111 solver.cpp:229] Iteration 21400, loss = 0.249154
I0823 00:46:30.911315  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:46:30.911322  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.249153 (* 1 = 0.249153 loss)
I0823 00:46:30.950779  2111 sgd_solver.cpp:106] Iteration 21400, lr = 0.01
I0823 00:46:52.012507  2111 solver.cpp:229] Iteration 21500, loss = 0.179231
I0823 00:46:52.012537  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:46:52.012547  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.17923 (* 1 = 0.17923 loss)
I0823 00:46:52.019502  2111 sgd_solver.cpp:106] Iteration 21500, lr = 0.01
I0823 00:47:11.246867  2111 solver.cpp:229] Iteration 21600, loss = 0.186025
I0823 00:47:11.246896  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:47:11.246903  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.186024 (* 1 = 0.186024 loss)
I0823 00:47:11.316516  2111 sgd_solver.cpp:106] Iteration 21600, lr = 0.01
I0823 00:47:30.771638  2111 solver.cpp:229] Iteration 21700, loss = 0.16579
I0823 00:47:30.771662  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0823 00:47:30.771669  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.165789 (* 1 = 0.165789 loss)
I0823 00:47:30.781781  2111 sgd_solver.cpp:106] Iteration 21700, lr = 0.01
I0823 00:47:48.923426  2111 solver.cpp:229] Iteration 21800, loss = 0.284381
I0823 00:47:48.923452  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0823 00:47:48.923472  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.28438 (* 1 = 0.28438 loss)
I0823 00:47:48.929200  2111 sgd_solver.cpp:106] Iteration 21800, lr = 0.01
I0823 00:48:07.440001  2111 solver.cpp:229] Iteration 21900, loss = 0.0879152
I0823 00:48:07.440029  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 1
I0823 00:48:07.440037  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0879143 (* 1 = 0.0879143 loss)
I0823 00:48:07.449852  2111 sgd_solver.cpp:106] Iteration 21900, lr = 0.01
I0823 00:48:24.871014  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_22000.caffemodel
I0823 00:48:24.876353  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_22000.solverstate
I0823 00:48:24.878657  2111 solver.cpp:338] Iteration 22000, Testing net (#0)
I0823 00:48:35.952167  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.91329
I0823 00:48:35.952208  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.2924 (* 1 = 0.2924 loss)
I0823 00:48:36.104313  2111 solver.cpp:229] Iteration 22000, loss = 0.329528
I0823 00:48:36.104341  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:48:36.104347  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.329527 (* 1 = 0.329527 loss)
I0823 00:48:36.173123  2111 sgd_solver.cpp:106] Iteration 22000, lr = 0.01
I0823 00:48:36.177289  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:48:54.130007  2111 solver.cpp:229] Iteration 22100, loss = 0.153352
I0823 00:48:54.130035  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:48:54.130043  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.153351 (* 1 = 0.153351 loss)
I0823 00:48:54.154965  2111 sgd_solver.cpp:106] Iteration 22100, lr = 0.01
I0823 00:49:11.911830  2111 solver.cpp:229] Iteration 22200, loss = 0.258152
I0823 00:49:11.911854  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0823 00:49:11.911861  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.258151 (* 1 = 0.258151 loss)
I0823 00:49:11.922485  2111 sgd_solver.cpp:106] Iteration 22200, lr = 0.01
I0823 00:49:29.754817  2111 solver.cpp:229] Iteration 22300, loss = 0.464603
I0823 00:49:29.754843  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0823 00:49:29.754850  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.464602 (* 1 = 0.464602 loss)
I0823 00:49:29.766082  2111 sgd_solver.cpp:106] Iteration 22300, lr = 0.01
I0823 00:49:47.261442  2111 solver.cpp:229] Iteration 22400, loss = 0.277106
I0823 00:49:47.261467  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:49:47.261474  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.277105 (* 1 = 0.277105 loss)
I0823 00:49:47.272737  2111 sgd_solver.cpp:106] Iteration 22400, lr = 0.01
I0823 00:50:04.957123  2111 solver.cpp:229] Iteration 22500, loss = 0.211575
I0823 00:50:04.957145  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:50:04.957151  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.211574 (* 1 = 0.211574 loss)
I0823 00:50:04.970317  2111 sgd_solver.cpp:106] Iteration 22500, lr = 0.01
I0823 00:50:22.981554  2111 solver.cpp:229] Iteration 22600, loss = 0.154794
I0823 00:50:22.981578  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:50:22.981585  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.154794 (* 1 = 0.154794 loss)
I0823 00:50:22.992265  2111 sgd_solver.cpp:106] Iteration 22600, lr = 0.01
I0823 00:50:40.766255  2111 solver.cpp:229] Iteration 22700, loss = 0.248094
I0823 00:50:40.766278  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:50:40.766285  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.248093 (* 1 = 0.248093 loss)
I0823 00:50:40.777379  2111 sgd_solver.cpp:106] Iteration 22700, lr = 0.01
I0823 00:50:58.630921  2111 solver.cpp:229] Iteration 22800, loss = 0.331528
I0823 00:50:58.630946  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0823 00:50:58.630954  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.331527 (* 1 = 0.331527 loss)
I0823 00:50:58.655196  2111 sgd_solver.cpp:106] Iteration 22800, lr = 0.01
I0823 00:51:17.427999  2111 solver.cpp:229] Iteration 22900, loss = 0.13552
I0823 00:51:17.428022  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:51:17.428030  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.135519 (* 1 = 0.135519 loss)
I0823 00:51:17.440155  2111 sgd_solver.cpp:106] Iteration 22900, lr = 0.01
I0823 00:51:36.479049  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_23000.caffemodel
I0823 00:51:36.484704  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_23000.solverstate
I0823 00:51:36.487924  2111 solver.cpp:338] Iteration 23000, Testing net (#0)
I0823 00:51:47.739349  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.873421
I0823 00:51:47.739380  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.345705 (* 1 = 0.345705 loss)
I0823 00:51:47.962903  2111 solver.cpp:229] Iteration 23000, loss = 0.155011
I0823 00:51:47.962934  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:51:47.962945  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.15501 (* 1 = 0.15501 loss)
I0823 00:51:47.975862  2111 sgd_solver.cpp:106] Iteration 23000, lr = 0.01
I0823 00:51:47.979691  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:52:06.731636  2111 solver.cpp:229] Iteration 23100, loss = 0.15244
I0823 00:52:06.731662  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:52:06.731669  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.152439 (* 1 = 0.152439 loss)
I0823 00:52:06.744858  2111 sgd_solver.cpp:106] Iteration 23100, lr = 0.01
I0823 00:52:24.720335  2111 solver.cpp:229] Iteration 23200, loss = 0.0940513
I0823 00:52:24.720360  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.984375
I0823 00:52:24.720366  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0940505 (* 1 = 0.0940505 loss)
I0823 00:52:24.731163  2111 sgd_solver.cpp:106] Iteration 23200, lr = 0.01
I0823 00:52:42.833544  2111 solver.cpp:229] Iteration 23300, loss = 0.178042
I0823 00:52:42.833567  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:52:42.833573  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.178041 (* 1 = 0.178041 loss)
I0823 00:52:42.845477  2111 sgd_solver.cpp:106] Iteration 23300, lr = 0.01
I0823 00:53:00.431602  2111 solver.cpp:229] Iteration 23400, loss = 0.308945
I0823 00:53:00.431623  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:53:00.431630  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.308944 (* 1 = 0.308944 loss)
I0823 00:53:00.443506  2111 sgd_solver.cpp:106] Iteration 23400, lr = 0.01
I0823 00:53:18.021395  2111 solver.cpp:229] Iteration 23500, loss = 0.135949
I0823 00:53:18.021420  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:53:18.021427  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.135948 (* 1 = 0.135948 loss)
I0823 00:53:18.033833  2111 sgd_solver.cpp:106] Iteration 23500, lr = 0.01
I0823 00:53:37.962898  2111 solver.cpp:229] Iteration 23600, loss = 0.152689
I0823 00:53:37.962921  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:53:37.962929  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.152688 (* 1 = 0.152688 loss)
I0823 00:53:37.974242  2111 sgd_solver.cpp:106] Iteration 23600, lr = 0.01
I0823 00:53:55.632288  2111 solver.cpp:229] Iteration 23700, loss = 0.198618
I0823 00:53:55.632314  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:53:55.632324  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.198617 (* 1 = 0.198617 loss)
I0823 00:53:55.642596  2111 sgd_solver.cpp:106] Iteration 23700, lr = 0.01
I0823 00:54:13.176445  2111 solver.cpp:229] Iteration 23800, loss = 0.342669
I0823 00:54:13.176470  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:54:13.176481  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.342668 (* 1 = 0.342668 loss)
I0823 00:54:13.189322  2111 sgd_solver.cpp:106] Iteration 23800, lr = 0.01
I0823 00:54:31.345382  2111 solver.cpp:229] Iteration 23900, loss = 0.290649
I0823 00:54:31.345412  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:54:31.345419  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.290648 (* 1 = 0.290648 loss)
I0823 00:54:31.386651  2111 sgd_solver.cpp:106] Iteration 23900, lr = 0.01
I0823 00:54:48.944721  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_24000.caffemodel
I0823 00:54:48.950079  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_24000.solverstate
I0823 00:54:48.952416  2111 solver.cpp:338] Iteration 24000, Testing net (#0)
I0823 00:55:00.040695  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.918158
I0823 00:55:00.040725  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.246378 (* 1 = 0.246378 loss)
I0823 00:55:00.189334  2111 solver.cpp:229] Iteration 24000, loss = 0.176975
I0823 00:55:00.189363  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:55:00.189369  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.176974 (* 1 = 0.176974 loss)
I0823 00:55:00.264072  2111 sgd_solver.cpp:106] Iteration 24000, lr = 0.01
I0823 00:55:00.267606  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:55:19.485929  2111 solver.cpp:229] Iteration 24100, loss = 0.181446
I0823 00:55:19.485952  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:55:19.485960  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.181445 (* 1 = 0.181445 loss)
I0823 00:55:19.496448  2111 sgd_solver.cpp:106] Iteration 24100, lr = 0.01
I0823 00:55:37.078935  2111 solver.cpp:229] Iteration 24200, loss = 0.211811
I0823 00:55:37.078959  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:55:37.078966  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.21181 (* 1 = 0.21181 loss)
I0823 00:55:37.089663  2111 sgd_solver.cpp:106] Iteration 24200, lr = 0.01
I0823 00:55:57.373492  2111 solver.cpp:229] Iteration 24300, loss = 0.0822034
I0823 00:55:57.373517  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.984375
I0823 00:55:57.373523  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0822027 (* 1 = 0.0822027 loss)
I0823 00:55:57.373543  2111 sgd_solver.cpp:106] Iteration 24300, lr = 0.01
I0823 00:56:16.266089  2111 solver.cpp:229] Iteration 24400, loss = 0.195347
I0823 00:56:16.266118  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:56:16.266125  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.195346 (* 1 = 0.195346 loss)
I0823 00:56:16.279000  2111 sgd_solver.cpp:106] Iteration 24400, lr = 0.01
I0823 00:56:34.355948  2111 solver.cpp:229] Iteration 24500, loss = 0.314908
I0823 00:56:34.355973  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0823 00:56:34.355979  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.314907 (* 1 = 0.314907 loss)
I0823 00:56:34.372629  2111 sgd_solver.cpp:106] Iteration 24500, lr = 0.01
I0823 00:56:52.964038  2111 solver.cpp:229] Iteration 24600, loss = 0.286981
I0823 00:56:52.964063  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:56:52.964071  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.28698 (* 1 = 0.28698 loss)
I0823 00:56:52.974679  2111 sgd_solver.cpp:106] Iteration 24600, lr = 0.01
I0823 00:57:10.398744  2111 solver.cpp:229] Iteration 24700, loss = 0.217326
I0823 00:57:10.398766  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:57:10.398772  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.217326 (* 1 = 0.217326 loss)
I0823 00:57:10.410009  2111 sgd_solver.cpp:106] Iteration 24700, lr = 0.01
I0823 00:57:29.289191  2111 solver.cpp:229] Iteration 24800, loss = 0.228085
I0823 00:57:29.289217  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:57:29.289225  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.228084 (* 1 = 0.228084 loss)
I0823 00:57:29.316926  2111 sgd_solver.cpp:106] Iteration 24800, lr = 0.01
I0823 00:57:51.148736  2111 solver.cpp:229] Iteration 24900, loss = 0.232031
I0823 00:57:51.148762  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.890625
I0823 00:57:51.148772  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.232031 (* 1 = 0.232031 loss)
I0823 00:57:51.177731  2111 sgd_solver.cpp:106] Iteration 24900, lr = 0.01
I0823 00:58:12.739038  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_25000.caffemodel
I0823 00:58:12.744501  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_25000.solverstate
I0823 00:58:12.746879  2111 solver.cpp:338] Iteration 25000, Testing net (#0)
I0823 00:58:23.867748  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.902368
I0823 00:58:23.867779  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.28446 (* 1 = 0.28446 loss)
I0823 00:58:24.015161  2111 solver.cpp:229] Iteration 25000, loss = 0.266606
I0823 00:58:24.015187  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:58:24.015193  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.266605 (* 1 = 0.266605 loss)
I0823 00:58:24.158260  2111 sgd_solver.cpp:46] MultiStep Status: Iteration 25000, step = 1
I0823 00:58:24.158273  2111 sgd_solver.cpp:106] Iteration 25000, lr = 0.001
I0823 00:58:24.161660  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 00:58:43.981782  2111 solver.cpp:229] Iteration 25100, loss = 0.165959
I0823 00:58:43.981807  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 00:58:43.981814  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.165958 (* 1 = 0.165958 loss)
I0823 00:58:43.992360  2111 sgd_solver.cpp:106] Iteration 25100, lr = 0.001
I0823 00:59:01.407351  2111 solver.cpp:229] Iteration 25200, loss = 0.219278
I0823 00:59:01.407374  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 00:59:01.407382  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.219278 (* 1 = 0.219278 loss)
I0823 00:59:01.420640  2111 sgd_solver.cpp:106] Iteration 25200, lr = 0.001
I0823 00:59:18.810168  2111 solver.cpp:229] Iteration 25300, loss = 0.261183
I0823 00:59:18.810190  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 00:59:18.810197  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.261183 (* 1 = 0.261183 loss)
I0823 00:59:18.820650  2111 sgd_solver.cpp:106] Iteration 25300, lr = 0.001
I0823 00:59:36.347509  2111 solver.cpp:229] Iteration 25400, loss = 0.0802347
I0823 00:59:36.347532  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 00:59:36.347538  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0802341 (* 1 = 0.0802341 loss)
I0823 00:59:36.361354  2111 sgd_solver.cpp:106] Iteration 25400, lr = 0.001
I0823 00:59:54.171524  2111 solver.cpp:229] Iteration 25500, loss = 0.127183
I0823 00:59:54.171547  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 00:59:54.171555  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.127182 (* 1 = 0.127182 loss)
I0823 00:59:54.182340  2111 sgd_solver.cpp:106] Iteration 25500, lr = 0.001
I0823 01:00:11.873757  2111 solver.cpp:229] Iteration 25600, loss = 0.12326
I0823 01:00:11.873781  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 01:00:11.873788  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.123259 (* 1 = 0.123259 loss)
I0823 01:00:11.884130  2111 sgd_solver.cpp:106] Iteration 25600, lr = 0.001
I0823 01:00:29.366776  2111 solver.cpp:229] Iteration 25700, loss = 0.15868
I0823 01:00:29.366801  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 01:00:29.366807  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.15868 (* 1 = 0.15868 loss)
I0823 01:00:29.378859  2111 sgd_solver.cpp:106] Iteration 25700, lr = 0.001
I0823 01:00:46.988829  2111 solver.cpp:229] Iteration 25800, loss = 0.348768
I0823 01:00:46.988850  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.875
I0823 01:00:46.988857  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.348768 (* 1 = 0.348768 loss)
I0823 01:00:46.999755  2111 sgd_solver.cpp:106] Iteration 25800, lr = 0.001
I0823 01:01:05.033448  2111 solver.cpp:229] Iteration 25900, loss = 0.113577
I0823 01:01:05.033473  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 01:01:05.033479  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.113576 (* 1 = 0.113576 loss)
I0823 01:01:05.044559  2111 sgd_solver.cpp:106] Iteration 25900, lr = 0.001
I0823 01:01:22.630548  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_26000.caffemodel
I0823 01:01:22.635918  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_26000.solverstate
I0823 01:01:22.638262  2111 solver.cpp:338] Iteration 26000, Testing net (#0)
I0823 01:01:33.799326  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.934605
I0823 01:01:33.799358  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.207824 (* 1 = 0.207824 loss)
I0823 01:01:33.951063  2111 solver.cpp:229] Iteration 26000, loss = 0.142773
I0823 01:01:33.951092  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 01:01:33.951099  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.142773 (* 1 = 0.142773 loss)
I0823 01:01:34.020606  2111 sgd_solver.cpp:106] Iteration 26000, lr = 0.001
I0823 01:01:34.024158  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 01:01:51.653276  2111 solver.cpp:229] Iteration 26100, loss = 0.0696817
I0823 01:01:51.653301  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 01:01:51.653308  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0696812 (* 1 = 0.0696812 loss)
I0823 01:01:51.664999  2111 sgd_solver.cpp:106] Iteration 26100, lr = 0.001
I0823 01:02:10.203572  2111 solver.cpp:229] Iteration 26200, loss = 0.0902444
I0823 01:02:10.203593  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.984375
I0823 01:02:10.203601  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0902438 (* 1 = 0.0902438 loss)
I0823 01:02:10.203621  2111 sgd_solver.cpp:106] Iteration 26200, lr = 0.001
I0823 01:02:30.224926  2111 solver.cpp:229] Iteration 26300, loss = 0.0911815
I0823 01:02:30.224951  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 01:02:30.224958  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.091181 (* 1 = 0.091181 loss)
I0823 01:02:30.236464  2111 sgd_solver.cpp:106] Iteration 26300, lr = 0.001
I0823 01:02:47.759577  2111 solver.cpp:229] Iteration 26400, loss = 0.141583
I0823 01:02:47.759603  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 01:02:47.759610  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.141582 (* 1 = 0.141582 loss)
I0823 01:02:47.769699  2111 sgd_solver.cpp:106] Iteration 26400, lr = 0.001
I0823 01:03:07.409987  2111 solver.cpp:229] Iteration 26500, loss = 0.102838
I0823 01:03:07.410022  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 01:03:07.410029  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.102838 (* 1 = 0.102838 loss)
I0823 01:03:07.410054  2111 sgd_solver.cpp:106] Iteration 26500, lr = 0.001
I0823 01:03:25.829063  2111 solver.cpp:229] Iteration 26600, loss = 0.14485
I0823 01:03:25.829087  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 01:03:25.829094  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.14485 (* 1 = 0.14485 loss)
I0823 01:03:25.840507  2111 sgd_solver.cpp:106] Iteration 26600, lr = 0.001
I0823 01:03:43.567586  2111 solver.cpp:229] Iteration 26700, loss = 0.170915
I0823 01:03:43.567611  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 01:03:43.567617  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.170914 (* 1 = 0.170914 loss)
I0823 01:03:43.578914  2111 sgd_solver.cpp:106] Iteration 26700, lr = 0.001
I0823 01:04:01.615747  2111 solver.cpp:229] Iteration 26800, loss = 0.0740193
I0823 01:04:01.615770  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 1
I0823 01:04:01.615777  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0740188 (* 1 = 0.0740188 loss)
I0823 01:04:01.629703  2111 sgd_solver.cpp:106] Iteration 26800, lr = 0.001
I0823 01:04:22.770467  2111 solver.cpp:229] Iteration 26900, loss = 0.199493
I0823 01:04:22.770496  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 01:04:22.770510  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.199492 (* 1 = 0.199492 loss)
I0823 01:04:22.777007  2111 sgd_solver.cpp:106] Iteration 26900, lr = 0.001
I0823 01:04:43.689090  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_27000.caffemodel
I0823 01:04:43.694839  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_27000.solverstate
I0823 01:04:43.697348  2111 solver.cpp:338] Iteration 27000, Testing net (#0)
I0823 01:04:54.923177  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.932895
I0823 01:04:54.923208  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.223672 (* 1 = 0.223672 loss)
I0823 01:04:55.071909  2111 solver.cpp:229] Iteration 27000, loss = 0.187401
I0823 01:04:55.071940  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 01:04:55.071947  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.1874 (* 1 = 0.1874 loss)
I0823 01:04:55.140625  2111 sgd_solver.cpp:106] Iteration 27000, lr = 0.001
I0823 01:04:55.144987  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 01:05:14.779768  2111 solver.cpp:229] Iteration 27100, loss = 0.151029
I0823 01:05:14.779793  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 01:05:14.779800  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.151029 (* 1 = 0.151029 loss)
I0823 01:05:14.824754  2111 sgd_solver.cpp:106] Iteration 27100, lr = 0.001
I0823 01:05:34.331502  2111 solver.cpp:229] Iteration 27200, loss = 0.251377
I0823 01:05:34.331524  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 01:05:34.331532  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.251376 (* 1 = 0.251376 loss)
I0823 01:05:34.346415  2111 sgd_solver.cpp:106] Iteration 27200, lr = 0.001
I0823 01:05:51.871139  2111 solver.cpp:229] Iteration 27300, loss = 0.129018
I0823 01:05:51.871162  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 01:05:51.871170  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.129017 (* 1 = 0.129017 loss)
I0823 01:05:51.883002  2111 sgd_solver.cpp:106] Iteration 27300, lr = 0.001
I0823 01:06:10.206265  2111 solver.cpp:229] Iteration 27400, loss = 0.197067
I0823 01:06:10.206291  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 01:06:10.206300  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.197066 (* 1 = 0.197066 loss)
I0823 01:06:10.241766  2111 sgd_solver.cpp:106] Iteration 27400, lr = 0.001
I0823 01:06:28.635813  2111 solver.cpp:229] Iteration 27500, loss = 0.0991416
I0823 01:06:28.635843  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 01:06:28.635851  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0991411 (* 1 = 0.0991411 loss)
I0823 01:06:28.697105  2111 sgd_solver.cpp:106] Iteration 27500, lr = 0.001
I0823 01:06:50.526921  2111 solver.cpp:229] Iteration 27600, loss = 0.170846
I0823 01:06:50.526948  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.984375
I0823 01:06:50.526955  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.170846 (* 1 = 0.170846 loss)
I0823 01:06:50.593933  2111 sgd_solver.cpp:106] Iteration 27600, lr = 0.001
I0823 01:07:10.979671  2111 solver.cpp:229] Iteration 27700, loss = 0.189313
I0823 01:07:10.979694  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 01:07:10.979701  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.189313 (* 1 = 0.189313 loss)
I0823 01:07:10.997843  2111 sgd_solver.cpp:106] Iteration 27700, lr = 0.001
I0823 01:07:28.611496  2111 solver.cpp:229] Iteration 27800, loss = 0.0452078
I0823 01:07:28.611521  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 1
I0823 01:07:28.611528  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0452073 (* 1 = 0.0452073 loss)
I0823 01:07:28.623356  2111 sgd_solver.cpp:106] Iteration 27800, lr = 0.001
I0823 01:07:46.071290  2111 solver.cpp:229] Iteration 27900, loss = 0.120282
I0823 01:07:46.071316  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 01:07:46.071322  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.120282 (* 1 = 0.120282 loss)
I0823 01:07:46.081818  2111 sgd_solver.cpp:106] Iteration 27900, lr = 0.001
I0823 01:08:06.533162  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_28000.caffemodel
I0823 01:08:06.538511  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_28000.solverstate
I0823 01:08:06.540829  2111 solver.cpp:338] Iteration 28000, Testing net (#0)
I0823 01:08:17.635727  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.930789
I0823 01:08:17.635768  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.239383 (* 1 = 0.239383 loss)
I0823 01:08:17.784071  2111 solver.cpp:229] Iteration 28000, loss = 0.0632951
I0823 01:08:17.784101  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.984375
I0823 01:08:17.784107  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0632946 (* 1 = 0.0632946 loss)
I0823 01:08:17.856263  2111 sgd_solver.cpp:46] MultiStep Status: Iteration 28000, step = 2
I0823 01:08:17.856277  2111 sgd_solver.cpp:106] Iteration 28000, lr = 0.0001
I0823 01:08:17.859982  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 01:08:35.445730  2111 solver.cpp:229] Iteration 28100, loss = 0.135337
I0823 01:08:35.445752  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 01:08:35.445760  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.135336 (* 1 = 0.135336 loss)
I0823 01:08:35.457799  2111 sgd_solver.cpp:106] Iteration 28100, lr = 0.0001
I0823 01:08:52.999927  2111 solver.cpp:229] Iteration 28200, loss = 0.0962424
I0823 01:08:52.999949  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 01:08:52.999956  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0962419 (* 1 = 0.0962419 loss)
I0823 01:08:53.014179  2111 sgd_solver.cpp:106] Iteration 28200, lr = 0.0001
I0823 01:09:10.605804  2111 solver.cpp:229] Iteration 28300, loss = 0.195838
I0823 01:09:10.605829  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 01:09:10.605835  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.195838 (* 1 = 0.195838 loss)
I0823 01:09:10.616398  2111 sgd_solver.cpp:106] Iteration 28300, lr = 0.0001
I0823 01:09:28.118027  2111 solver.cpp:229] Iteration 28400, loss = 0.0962359
I0823 01:09:28.118052  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 01:09:28.118057  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0962354 (* 1 = 0.0962354 loss)
I0823 01:09:28.130555  2111 sgd_solver.cpp:106] Iteration 28400, lr = 0.0001
I0823 01:09:45.580747  2111 solver.cpp:229] Iteration 28500, loss = 0.0600849
I0823 01:09:45.580771  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 01:09:45.580780  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0600844 (* 1 = 0.0600844 loss)
I0823 01:09:45.591576  2111 sgd_solver.cpp:106] Iteration 28500, lr = 0.0001
I0823 01:10:05.190372  2111 solver.cpp:229] Iteration 28600, loss = 0.161215
I0823 01:10:05.190395  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 01:10:05.190402  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.161214 (* 1 = 0.161214 loss)
I0823 01:10:05.190428  2111 sgd_solver.cpp:106] Iteration 28600, lr = 0.0001
I0823 01:10:25.069619  2111 solver.cpp:229] Iteration 28700, loss = 0.106104
I0823 01:10:25.069643  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 01:10:25.069650  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.106103 (* 1 = 0.106103 loss)
I0823 01:10:25.081125  2111 sgd_solver.cpp:106] Iteration 28700, lr = 0.0001
I0823 01:10:42.891731  2111 solver.cpp:229] Iteration 28800, loss = 0.0736742
I0823 01:10:42.891752  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.96875
I0823 01:10:42.891759  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0736738 (* 1 = 0.0736738 loss)
I0823 01:10:42.903251  2111 sgd_solver.cpp:106] Iteration 28800, lr = 0.0001
I0823 01:11:01.233314  2111 solver.cpp:229] Iteration 28900, loss = 0.054815
I0823 01:11:01.233338  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.984375
I0823 01:11:01.233345  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0548146 (* 1 = 0.0548146 loss)
I0823 01:11:01.244933  2111 sgd_solver.cpp:106] Iteration 28900, lr = 0.0001
I0823 01:11:21.661046  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_29000.caffemodel
I0823 01:11:21.666834  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_29000.solverstate
I0823 01:11:21.669405  2111 solver.cpp:338] Iteration 29000, Testing net (#0)
I0823 01:11:32.772222  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.935658
I0823 01:11:32.772253  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.230417 (* 1 = 0.230417 loss)
I0823 01:11:32.921459  2111 solver.cpp:229] Iteration 29000, loss = 0.0622386
I0823 01:11:32.921489  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.984375
I0823 01:11:32.921499  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0622381 (* 1 = 0.0622381 loss)
I0823 01:11:33.011462  2111 sgd_solver.cpp:106] Iteration 29000, lr = 0.0001
I0823 01:11:33.015043  2181 blocking_queue.cpp:50] Data layer prefetch queue empty
I0823 01:11:50.840008  2111 solver.cpp:229] Iteration 29100, loss = 0.17104
I0823 01:11:50.840032  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 01:11:50.840039  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.17104 (* 1 = 0.17104 loss)
I0823 01:11:50.850688  2111 sgd_solver.cpp:106] Iteration 29100, lr = 0.0001
I0823 01:12:12.521950  2111 solver.cpp:229] Iteration 29200, loss = 0.16813
I0823 01:12:12.521973  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.90625
I0823 01:12:12.521981  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.168129 (* 1 = 0.168129 loss)
I0823 01:12:12.533035  2111 sgd_solver.cpp:106] Iteration 29200, lr = 0.0001
I0823 01:12:31.496891  2111 solver.cpp:229] Iteration 29300, loss = 0.0908962
I0823 01:12:31.496912  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 01:12:31.496918  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0908957 (* 1 = 0.0908957 loss)
I0823 01:12:31.506772  2111 sgd_solver.cpp:106] Iteration 29300, lr = 0.0001
I0823 01:12:48.957962  2111 solver.cpp:229] Iteration 29400, loss = 0.0744702
I0823 01:12:48.957988  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.984375
I0823 01:12:48.957994  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0744697 (* 1 = 0.0744697 loss)
I0823 01:12:48.969349  2111 sgd_solver.cpp:106] Iteration 29400, lr = 0.0001
I0823 01:13:10.650578  2111 solver.cpp:229] Iteration 29500, loss = 0.0612604
I0823 01:13:10.650601  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.984375
I0823 01:13:10.650607  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.0612599 (* 1 = 0.0612599 loss)
I0823 01:13:10.650629  2111 sgd_solver.cpp:106] Iteration 29500, lr = 0.0001
I0823 01:13:29.810555  2111 solver.cpp:229] Iteration 29600, loss = 0.14028
I0823 01:13:29.810581  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.953125
I0823 01:13:29.810588  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.140279 (* 1 = 0.140279 loss)
I0823 01:13:29.821967  2111 sgd_solver.cpp:106] Iteration 29600, lr = 0.0001
I0823 01:13:50.182329  2111 solver.cpp:229] Iteration 29700, loss = 0.168441
I0823 01:13:50.182353  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.921875
I0823 01:13:50.182359  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.16844 (* 1 = 0.16844 loss)
I0823 01:13:50.182380  2111 sgd_solver.cpp:106] Iteration 29700, lr = 0.0001
I0823 01:14:08.593231  2111 solver.cpp:229] Iteration 29800, loss = 0.190299
I0823 01:14:08.593255  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 01:14:08.593262  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.190299 (* 1 = 0.190299 loss)
I0823 01:14:08.608680  2111 sgd_solver.cpp:106] Iteration 29800, lr = 0.0001
I0823 01:14:26.215087  2111 solver.cpp:229] Iteration 29900, loss = 0.129341
I0823 01:14:26.215111  2111 solver.cpp:245]     Train net output #0: classification_accuracy = 0.9375
I0823 01:14:26.215117  2111 solver.cpp:245]     Train net output #1: classification_loss = 0.12934 (* 1 = 0.12934 loss)
I0823 01:14:26.225555  2111 sgd_solver.cpp:106] Iteration 29900, lr = 0.0001
I0823 01:14:43.810647  2111 solver.cpp:456] Snapshotting to binary proto file model_iter_30000.caffemodel
I0823 01:14:43.818277  2111 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model_iter_30000.solverstate
I0823 01:14:43.925437  2111 solver.cpp:318] Iteration 30000, loss = 0.107223
I0823 01:14:43.925462  2111 solver.cpp:338] Iteration 30000, Testing net (#0)
I0823 01:14:54.968833  2111 solver.cpp:406]     Test net output #0: classification_accuracy = 0.935921
I0823 01:14:54.968874  2111 solver.cpp:406]     Test net output #1: classification_loss = 0.232446 (* 1 = 0.232446 loss)
I0823 01:14:54.968879  2111 solver.cpp:323] Optimization Done.
I0823 01:14:55.104977  2111 caffe.cpp:222] Optimization Done.
